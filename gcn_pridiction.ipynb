{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_pridiction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XkvkckV6Lc5VVRFDhXiPCTrNYzg7vQ0W",
      "authorship_tag": "ABX9TyOuRvvWsJXEiqpEJyDlNm/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevejobws/Colaboratory/blob/master/gcn_pridiction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA0EeTTq6Gi-"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install torchvision\n",
        "!pip install torch_sparse -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_scatter -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_geometric # 下载安装pytorch_geometric\n",
        "!pip install networkx # 画图\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric \n",
        "from torch_geometric.nn import GCNConv, ChebConv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5TCDF488n7"
      },
      "source": [
        "! python -c \"import torch_geometric; print(torch_geometric.__version__)\" # 检查是否安装成功"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57sAyKd7EgIc",
        "outputId": "3faba9a1-84bd-4ff2-95d8-d5cd39cb8b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "! uname -a  # 查看系统  \n",
        "! python --version  # 查看python版本 \n",
        "! python -c 'import torch; print(torch.version.cuda)' # 查看cuda的版本，检查是否和cuda的一致\n",
        "! nvcc --version # 查看nvcc版本 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux e4e606ebf6e4 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Python 3.6.9\n",
            "10.1\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki24HI_7E2mL",
        "outputId": "fa4e9f0d-0cea-4594-b605-ce32e4a136e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(torch.version.cuda) # torch的cuda版本"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZMKOaKvfRmz",
        "outputId": "80138fe3-64c8-4634-fa4d-ca872eac2431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "print(\"hello torch{}\".format(torch.__version__))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello torch1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLtfimHc64d"
      },
      "source": [
        "# 1. 预处理数据集的格式，转化为GCN所需要的格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8QW2jP7JV2",
        "outputId": "5ed54fa9-5bb8-4878-8386-2192b2a52ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf # Orange 1.14.0\n",
        "print(tf.__version__)\n",
        "import keras # 2.2.5\n",
        "print(keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TkV_5eTmCFU",
        "outputId": "0e31d38f-24f1-47aa-b58b-b99b8619b432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "!pip install keras==2.2.5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 87kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 33.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.10.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.35.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqo1Ho-djcrH"
      },
      "source": [
        "### 1.1 import file is  attribute of node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScqvErTlc5iO",
        "outputId": "b053c14e-2ecd-49db-cf93-b6e7fd06a5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "node_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNodeAttribute18416.csv',header = None) \n",
        "num = node_features.shape[0] # Number of nodes\n",
        "node_features  \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.016331</td>\n",
              "      <td>0.146668</td>\n",
              "      <td>0.725543</td>\n",
              "      <td>0.174163</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.089384</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.711078</td>\n",
              "      <td>0.535862</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.070424</td>\n",
              "      <td>0.459120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.451255</td>\n",
              "      <td>0.134811</td>\n",
              "      <td>0.338473</td>\n",
              "      <td>0.122156</td>\n",
              "      <td>0.509349</td>\n",
              "      <td>0.282891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.367115</td>\n",
              "      <td>0.452088</td>\n",
              "      <td>0.251513</td>\n",
              "      <td>0.886946</td>\n",
              "      <td>0.101663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065272</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.440651</td>\n",
              "      <td>0.401696</td>\n",
              "      <td>0.095689</td>\n",
              "      <td>0.765287</td>\n",
              "      <td>0.526771</td>\n",
              "      <td>1.057028</td>\n",
              "      <td>0.098744</td>\n",
              "      <td>0.023218</td>\n",
              "      <td>0.342406</td>\n",
              "      <td>0.566687</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.696072</td>\n",
              "      <td>0.586652</td>\n",
              "      <td>0.262387</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077090</td>\n",
              "      <td>0.423339</td>\n",
              "      <td>0.185215</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.281270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.184586</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.407317</td>\n",
              "      <td>0.605318</td>\n",
              "      <td>0.327069</td>\n",
              "      <td>0.909140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.788762</td>\n",
              "      <td>0.312990</td>\n",
              "      <td>0.078151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.783508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.435840</td>\n",
              "      <td>1.204385</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.441521</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.839651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.316772</td>\n",
              "      <td>1.094028</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400978</td>\n",
              "      <td>0.308016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.221955</td>\n",
              "      <td>0.485380</td>\n",
              "      <td>1.558614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.246036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.682492</td>\n",
              "      <td>0.366790</td>\n",
              "      <td>0.751529</td>\n",
              "      <td>0.526286</td>\n",
              "      <td>1.971504</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.339621</td>\n",
              "      <td>1.621154</td>\n",
              "      <td>0.883370</td>\n",
              "      <td>0.523618</td>\n",
              "      <td>1.788122</td>\n",
              "      <td>1.439940</td>\n",
              "      <td>0.159284</td>\n",
              "      <td>0.284751</td>\n",
              "      <td>0.669727</td>\n",
              "      <td>1.761021</td>\n",
              "      <td>2.606565</td>\n",
              "      <td>0.139260</td>\n",
              "      <td>0.794215</td>\n",
              "      <td>0.608262</td>\n",
              "      <td>1.132268</td>\n",
              "      <td>0.994848</td>\n",
              "      <td>0.020278</td>\n",
              "      <td>0.017182</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.019694</td>\n",
              "      <td>0.077861</td>\n",
              "      <td>0.057064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.567412</td>\n",
              "      <td>0.270772</td>\n",
              "      <td>1.786874</td>\n",
              "      <td>0.516166</td>\n",
              "      <td>0.085904</td>\n",
              "      <td>0.931137</td>\n",
              "      <td>0.525576</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.032238</td>\n",
              "      <td>1.385969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.259270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.324180</td>\n",
              "      <td>0.309331</td>\n",
              "      <td>0.014996</td>\n",
              "      <td>0.460173</td>\n",
              "      <td>0.708755</td>\n",
              "      <td>0.751080</td>\n",
              "      <td>0.338239</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.104922</td>\n",
              "      <td>0.051036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.848546</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.795172</td>\n",
              "      <td>0.203561</td>\n",
              "      <td>0.433606</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.197207</td>\n",
              "      <td>0.354380</td>\n",
              "      <td>0.020565</td>\n",
              "      <td>0.197066</td>\n",
              "      <td>0.209536</td>\n",
              "      <td>1.022902</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.840677</td>\n",
              "      <td>0.292150</td>\n",
              "      <td>0.025301</td>\n",
              "      <td>0.117673</td>\n",
              "      <td>0.248926</td>\n",
              "      <td>0.918377</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.186953</td>\n",
              "      <td>0.638557</td>\n",
              "      <td>1.504499</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.845216</td>\n",
              "      <td>0.869688</td>\n",
              "      <td>0.231062</td>\n",
              "      <td>0.741330</td>\n",
              "      <td>0.118901</td>\n",
              "      <td>0.233369</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.793322</td>\n",
              "      <td>0.430846</td>\n",
              "      <td>0.668513</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.165300</td>\n",
              "      <td>0.591209</td>\n",
              "      <td>0.688688</td>\n",
              "      <td>1.253160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045387</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.779330</td>\n",
              "      <td>0.629813</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.082397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.165230</td>\n",
              "      <td>0.978996</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.744489</td>\n",
              "      <td>0.258909</td>\n",
              "      <td>0.060296</td>\n",
              "      <td>0.168233</td>\n",
              "      <td>0.199796</td>\n",
              "      <td>1.103272</td>\n",
              "      <td>0.251078</td>\n",
              "      <td>0.098119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.238727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.169436</td>\n",
              "      <td>0.174725</td>\n",
              "      <td>0.665714</td>\n",
              "      <td>0.704781</td>\n",
              "      <td>1.123834</td>\n",
              "      <td>0.503610</td>\n",
              "      <td>0.065120</td>\n",
              "      <td>0.332690</td>\n",
              "      <td>0.198439</td>\n",
              "      <td>0.766891</td>\n",
              "      <td>1.167586</td>\n",
              "      <td>0.296143</td>\n",
              "      <td>0.298238</td>\n",
              "      <td>1.088058</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081464</td>\n",
              "      <td>0.504218</td>\n",
              "      <td>0.610977</td>\n",
              "      <td>0.868126</td>\n",
              "      <td>0.849582</td>\n",
              "      <td>0.107903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.588164</td>\n",
              "      <td>0.545574</td>\n",
              "      <td>0.497186</td>\n",
              "      <td>1.005129</td>\n",
              "      <td>0.722074</td>\n",
              "      <td>0.089798</td>\n",
              "      <td>0.783461</td>\n",
              "      <td>0.030348</td>\n",
              "      <td>0.506543</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.249757</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.302376</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250245</td>\n",
              "      <td>0.990561</td>\n",
              "      <td>0.558670</td>\n",
              "      <td>1.217978</td>\n",
              "      <td>0.378977</td>\n",
              "      <td>0.257189</td>\n",
              "      <td>0.548955</td>\n",
              "      <td>1.136081</td>\n",
              "      <td>0.168006</td>\n",
              "      <td>0.032234</td>\n",
              "      <td>0.747449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.268873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.454440</td>\n",
              "      <td>0.153748</td>\n",
              "      <td>0.429066</td>\n",
              "      <td>0.313568</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.163547</td>\n",
              "      <td>1.020495</td>\n",
              "      <td>0.254495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097542</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.487418</td>\n",
              "      <td>0.404117</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.618405</td>\n",
              "      <td>0.683403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.113998</td>\n",
              "      <td>0.010242</td>\n",
              "      <td>0.214518</td>\n",
              "      <td>0.271182</td>\n",
              "      <td>0.454820</td>\n",
              "      <td>0.277028</td>\n",
              "      <td>0.014495</td>\n",
              "      <td>0.539655</td>\n",
              "      <td>0.332845</td>\n",
              "      <td>0.143752</td>\n",
              "      <td>0.132466</td>\n",
              "      <td>0.399873</td>\n",
              "      <td>0.112376</td>\n",
              "      <td>0.291646</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.716976</td>\n",
              "      <td>0.134282</td>\n",
              "      <td>0.205520</td>\n",
              "      <td>0.492683</td>\n",
              "      <td>0.741167</td>\n",
              "      <td>0.038630</td>\n",
              "      <td>0.232192</td>\n",
              "      <td>0.230327</td>\n",
              "      <td>0.278061</td>\n",
              "      <td>0.603764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.605231</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.676816</td>\n",
              "      <td>0.057315</td>\n",
              "      <td>0.464129</td>\n",
              "      <td>0.181568</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149361</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150121</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166518</td>\n",
              "      <td>0.509469</td>\n",
              "      <td>0.370294</td>\n",
              "      <td>0.536197</td>\n",
              "      <td>0.033468</td>\n",
              "      <td>0.491073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>862</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056832</td>\n",
              "      <td>0.084803</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066325</td>\n",
              "      <td>0.099736</td>\n",
              "      <td>0.012418</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087421</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099734</td>\n",
              "      <td>0.036357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047761</td>\n",
              "      <td>0.033273</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095286</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005404</td>\n",
              "      <td>0.016310</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056599</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.056369</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043184</td>\n",
              "      <td>0.011914</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.060415</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045033</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003504</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023373</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037773</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005106</td>\n",
              "      <td>0.147170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>863</th>\n",
              "      <td>863</td>\n",
              "      <td>0.057513</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.132687</td>\n",
              "      <td>0.071455</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003410</td>\n",
              "      <td>0.032789</td>\n",
              "      <td>0.054981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.072750</td>\n",
              "      <td>0.051715</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022584</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076077</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045040</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084927</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030305</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051135</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014930</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063040</td>\n",
              "      <td>0.035233</td>\n",
              "      <td>0.036601</td>\n",
              "      <td>0.013912</td>\n",
              "      <td>0.067421</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>864</th>\n",
              "      <td>864</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027228</td>\n",
              "      <td>0.202966</td>\n",
              "      <td>0.052150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.105366</td>\n",
              "      <td>0.135931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010894</td>\n",
              "      <td>0.104228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.101856</td>\n",
              "      <td>0.082611</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.072593</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051932</td>\n",
              "      <td>0.042124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049414</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087628</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028326</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.104760</td>\n",
              "      <td>0.086567</td>\n",
              "      <td>0.023353</td>\n",
              "      <td>0.029847</td>\n",
              "      <td>0.079753</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>865</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000984</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.261730</td>\n",
              "      <td>0.030116</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093965</td>\n",
              "      <td>0.343396</td>\n",
              "      <td>0.503311</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>0.300570</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.249718</td>\n",
              "      <td>0.097306</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.478518</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024793</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017304</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097856</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.068886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.573325</td>\n",
              "      <td>0.047867</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016300</td>\n",
              "      <td>0.173708</td>\n",
              "      <td>0.095977</td>\n",
              "      <td>0.053227</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073491</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>867 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0         1         2         3   ...        61        62        63        64\n",
              "0      0  0.016331  0.146668  0.725543  ...  0.312990  0.078151  0.000000  0.783508\n",
              "1      1  0.435840  1.204385  0.000000  ...  0.525576  0.000000  1.032238  1.385969\n",
              "2      2  0.000000  0.259270  0.000000  ...  0.779330  0.629813  0.000000  1.082397\n",
              "3      3  0.165230  0.978996  0.000000  ...  1.136081  0.168006  0.032234  0.747449\n",
              "4      4  0.268873  0.000000  0.454440  ...  0.370294  0.536197  0.033468  0.491073\n",
              "..   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
              "862  862  0.000000  0.000000  0.056832  ...  0.147170  0.000000  0.000000  0.000000\n",
              "863  863  0.057513  0.000000  0.132687  ...  0.067421  0.000000  0.000000  0.000000\n",
              "864  864  0.000000  0.027228  0.202966  ...  0.079753  0.000000  0.000000  0.000000\n",
              "865  865  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000\n",
              "866  866  0.000000  0.261730  0.030116  ...  0.000000  0.000000  0.073491  0.000000\n",
              "\n",
              "[867 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ozu5hHltHR",
        "outputId": "2fcbcb6b-3978-4ca5-ee18-3f1b71cff0d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 将词向量提取为特征,第二列到倒数第一列\n",
        "features =node_features.iloc[:,1:]\n",
        " # 检查特征：共64个特征，837个样本点\n",
        "print(features.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(867, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gypPFc0lb_c",
        "outputId": "69a90b8a-0c1c-40fb-b4b9-a9bc03f4f5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# 提取节点标签\n",
        "node_label = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNode_label18416.csv',header = None)\n",
        "labels = node_label[1] # 提取节点标签列\n",
        "labels[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yiryWH9Npol",
        "outputId": "5a0a9b44-a62e-4dc1-9a42-bf45ec7e31de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "filename1 = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum18416.csv'\n",
        "def load_file_as_Adj_matrix(filename):\n",
        "  '''\n",
        "  Read .rating file and Return dok matrix.\n",
        "  The first line of .rating file is: num_users\\t num_items\n",
        "  '''\n",
        "  # Get number of users and items\n",
        "  num_users, num_items = 0, 0\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      u, i = int(arr[0]), int(arr[1])\n",
        "      num_users = max(num_users, u)\n",
        "      num_items = max(num_items, i)\n",
        "      line = f.readline()\n",
        "  # Construct matrix\n",
        "  print(num_users)\n",
        "  print(num_items)\n",
        "  relation_matrix = np.zeros((num_items+1,num_items+1))\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      # user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "      # if (rating > 0):\n",
        "      user, item = int(arr[0]), int(arr[1])\n",
        "      relation_matrix[user, item] = 1\n",
        "      line = f.readline()    \n",
        "  return relation_matrix\n",
        "Adj = load_file_as_Adj_matrix(filename1)\n",
        "import scipy.sparse as sp\n",
        "Adj = sp.csr_matrix(Adj, dtype=np.float32)\n",
        "Adj.todense()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "268\n",
            "866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUCPEuDYdh4"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "def load_data(adj,node_features,node_labels):\n",
        "  features = sp.csr_matrix(node_features, dtype=np.float32)  # 储存为csr型稀疏矩阵\n",
        "  # build symmetric adjacency matrix   论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
        "  # adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "  # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
        "  features = normalize(features)\n",
        "  adj = normalize(adj + sp.eye(adj.shape[0]))   # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
        "  # 对应公式A~=A+IN\n",
        "  # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
        "  idx_train = range(500)\n",
        "  idx_val = range(500, 660)\n",
        "  idx_test = range(660, int(adj.shape[0]))  \n",
        "  features = torch.FloatTensor(np.array(features.todense()))  # tensor为pytorch常用的数据结构\n",
        "  labels = torch.LongTensor(np.array(node_labels))\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj)   # 邻接矩阵转为tensor处理\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "  idx_val = torch.LongTensor(idx_val)\n",
        "  idx_test = torch.LongTensor(idx_test)\n",
        "  return adj, features, labels, idx_train, idx_val, idx_test  \n",
        "def normalize(mx):\n",
        "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))  # 对每一行求和\n",
        "  r_inv = np.power(rowsum, -1).flatten()  # 求倒数\n",
        "  r_inv[np.isinf(r_inv)] = 0.  # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
        "  r_mat_inv = sp.diags(r_inv)  # 构建对角元素为r_inv的对角矩阵\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
        "  return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels) # 使用type_as(tesnor)将张量转换为给定类型的张量。\n",
        "  correct = preds.eq(labels).double()  # 记录等于preds的label eq:equal\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):    # 把一个sparse matrix转为torch稀疏张量\n",
        "  \"\"\"\n",
        "  numpy中的ndarray转化成pytorch中的tensor : torch.from_numpy()\n",
        "  pytorch中的tensor转化成numpy中的ndarray : numpy()\n",
        "  \"\"\"\n",
        "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "  indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "  # 不懂的可以去看看COO性稀疏矩阵的结构\n",
        "  values = torch.from_numpy(sparse_mx.data)\n",
        "  shape = torch.Size(sparse_mx.shape)\n",
        "  return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEF3l9vGxWhX"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "\n",
        "    # 初始化层：输入feature，输出feature，权重，偏移\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # FloatTensor建立tensor\n",
        "        # 常见用法self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))：\n",
        "        # 首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter\n",
        "        # 绑定到这个module里面，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。\n",
        "        # 使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "            # Parameters与register_parameter都会向parameters写入参数，但是后者可以支持字符串命名\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # 初始化权重\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        # size()函数主要是用来统计矩阵元素个数，或矩阵某一维上的元素个数的函数  size（1）为行\n",
        "        self.weight.data.uniform_(-stdv, stdv)  # uniform() 方法将随机生成下一个实数，它在 [x, y] 范围内\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    '''\n",
        "    前馈运算 即计算A~ X W(0)\n",
        "    input X与权重W相乘，然后adj矩阵与他们的积稀疏乘\n",
        "    直接输入与权重之间进行torch.mm操作，得到support，即XW\n",
        "    support与adj进行torch.spmm操作，得到output，即AXW选择是否加bias\n",
        "    '''\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # torch.mm(a, b)是矩阵a和b矩阵相乘，torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias \n",
        "        else:\n",
        "            return output\n",
        "#通过设置断点，可以看出output的形式是0.01，0.01，0.01，0.01，0.01，#0.01，0.94]，里面的值代表该x对应标签不同的概率，故此值可转换为#[0,0,0,0,0,0,1]，对应我们之前把标签onthot后的第七种标签\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NWtCnVPB4nb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "    # 底层节点的参数，feature的个数；隐层节点个数；最终的分类数\n",
        "    super(GCN, self).__init__()  #  super()._init_()在利用父类里的对象构造函数\n",
        "    self.gc1 = GraphConvolution(nfeat, nhid)   # gc1输入尺寸nfeat，输出尺寸nhid\n",
        "    self.gc2 = GraphConvolution(nhid, nclass)  # gc2输入尺寸nhid，输出尺寸ncalss\n",
        "    self.dropout = dropout\n",
        "    self.weight = Parameter(torch.FloatTensor(nfeat, nhid))  # FloatTensor建立tensor\n",
        "    # 输入分别是特征和邻接矩阵。最后输出为输出层做log_softmax变换的结果\n",
        "  def forward(self, x, adj):\n",
        "    x = F.relu(self.gc1(x, adj))    # adj即公式Z=softmax(A~Relu(A~XW(0))W(1))中的A~\n",
        "    x1 = F.dropout(x, self.dropout, training = self.training)  # x要dropout\n",
        "    x2 = self.gc2(x1, adj)\n",
        "    return F.log_softmax(x2, dim = 1), x   #, x  # 参数dim=1表示对每一行求softmax，那么每一行的值加起来都等于1。"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9JVSKDhxw00",
        "outputId": "386bff3b-328e-4a29-95fd-5ace745ecd1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import argparse  # argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Training settings\n",
        "learning_rate = 0.01\n",
        "weight_decay = 5e-4\n",
        "epoch_num = 200\n",
        "dropout = 0.02\n",
        "#in_size = node_features  #设置输入层的维数\n",
        "hi_size = 64 # 16 #设置隐藏层的维数\n",
        "#out_size = node_label #设置输入层的维数\n",
        "\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(Adj,features,labels)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=hi_size,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# 数据写入cuda，便于后续加速\n",
        "\n",
        "# if args.cuda:\n",
        "#     model.cuda()   # . cuda()会分配到显存里（如果gpu可用）\n",
        "#     features = features.cuda()\n",
        "#     adj = adj.cuda()\n",
        "#     labels = labels.cuda()\n",
        "#     idx_train = idx_train.cuda()\n",
        "#     idx_val = idx_val.cuda()\n",
        "#     idx_test = idx_test.cuda()\n",
        "#global node_vec\n",
        "train_loss = []\n",
        "def train(epoch_num):\n",
        "  t = time.time()  # 返回当前时间\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  # optimizer.zero_grad()意思是把梯度置零，也就是把loss关于weight的导数变成0.\n",
        "  # pytorch中每一轮batch需要设置optimizer.zero_gra\n",
        "  global Emdebding_train, output\n",
        "  output, Emdebding_train = model(features, adj)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  train_loss.append(loss_train)\n",
        "  # 由于在算output时已经使用了log_softmax，这里使用的损失函数就是NLLloss，如果前面没有加log运算，\n",
        "  # 这里就要使用CrossEntropyLoss了\n",
        "  # 损失函数NLLLoss() 的输入是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率，\n",
        "  # 适合最后一层是log_softmax()的网络. 损失函数 CrossEntropyLoss() 与 NLLLoss() 类似,\n",
        "  # 唯一的不同是它为我们去做 softmax.可以理解为：CrossEntropyLoss()=log_softmax() + NLLLoss()\n",
        "  # https://blog.csdn.net/hao5335156/article/details/80607732\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])  #计算准确率\n",
        "  loss_train.backward()  # 反向求导  Back Propagation\n",
        "  optimizer.step()  # 更新所有的参数  Gradient Descent\n",
        "    \n",
        "  #if not args.fastmode:\n",
        "      # Evaluate validation set performance separately,\n",
        "      # deactivates dropout during validation run.\n",
        "  model.eval()  # eval() 函数用来执行一个字符串表达式，并返回表达式的值\n",
        "  global Emdebding_eval\n",
        "  output, Emdebding_eval = model(features, adj)\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])    # 验证集的损失函数\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),     \n",
        "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "        'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
        "def test():\n",
        "  model.eval()\n",
        "  global Emdebding_test\n",
        "  output, Emdebding_test = model(features, adj)\n",
        "  loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "  acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "  print(\"Test set results:\",\n",
        "        \"loss= {:.4f}\".format(loss_test.item()),\n",
        "        \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "# Train model  逐个epoch进行train，最后test\n",
        "t_total = time.time()\n",
        "for epoch in range(epoch_num):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "test()\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "epochs = len(train_loss)\n",
        "plt.plot(range(0,epochs,1), train_loss, label='train_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "#plt.savefig(\"/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/\"+time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))+\"Unet-过拟合C0.jpg\")\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 0.6928 acc_train: 0.5760 loss_val: 0.8921 acc_val: 0.0000 time: 0.0238s\n",
            "Epoch: 0002 loss_train: 0.6946 acc_train: 0.5380 loss_val: 0.8479 acc_val: 0.0000 time: 0.0097s\n",
            "Epoch: 0003 loss_train: 0.6906 acc_train: 0.5380 loss_val: 0.7633 acc_val: 0.0063 time: 0.0084s\n",
            "Epoch: 0004 loss_train: 0.6880 acc_train: 0.5540 loss_val: 0.7044 acc_val: 0.3875 time: 0.0079s\n",
            "Epoch: 0005 loss_train: 0.6893 acc_train: 0.7080 loss_val: 0.7030 acc_val: 0.4250 time: 0.0074s\n",
            "Epoch: 0006 loss_train: 0.6878 acc_train: 0.7100 loss_val: 0.7367 acc_val: 0.2188 time: 0.0085s\n",
            "Epoch: 0007 loss_train: 0.6850 acc_train: 0.6640 loss_val: 0.7800 acc_val: 0.0813 time: 0.0085s\n",
            "Epoch: 0008 loss_train: 0.6843 acc_train: 0.5780 loss_val: 0.8121 acc_val: 0.0500 time: 0.0085s\n",
            "Epoch: 0009 loss_train: 0.6832 acc_train: 0.5560 loss_val: 0.8169 acc_val: 0.0688 time: 0.0092s\n",
            "Epoch: 0010 loss_train: 0.6833 acc_train: 0.5660 loss_val: 0.7938 acc_val: 0.1562 time: 0.0087s\n",
            "Epoch: 0011 loss_train: 0.6809 acc_train: 0.6100 loss_val: 0.7619 acc_val: 0.2500 time: 0.0087s\n",
            "Epoch: 0012 loss_train: 0.6800 acc_train: 0.6960 loss_val: 0.7419 acc_val: 0.3125 time: 0.0087s\n",
            "Epoch: 0013 loss_train: 0.6795 acc_train: 0.7200 loss_val: 0.7454 acc_val: 0.3187 time: 0.0086s\n",
            "Epoch: 0014 loss_train: 0.6784 acc_train: 0.7140 loss_val: 0.7661 acc_val: 0.2750 time: 0.0081s\n",
            "Epoch: 0015 loss_train: 0.6777 acc_train: 0.7040 loss_val: 0.7892 acc_val: 0.2437 time: 0.0077s\n",
            "Epoch: 0016 loss_train: 0.6767 acc_train: 0.6860 loss_val: 0.8020 acc_val: 0.2313 time: 0.0078s\n",
            "Epoch: 0017 loss_train: 0.6766 acc_train: 0.6660 loss_val: 0.7956 acc_val: 0.2500 time: 0.0077s\n",
            "Epoch: 0018 loss_train: 0.6759 acc_train: 0.6840 loss_val: 0.7784 acc_val: 0.3187 time: 0.0090s\n",
            "Epoch: 0019 loss_train: 0.6747 acc_train: 0.7140 loss_val: 0.7670 acc_val: 0.3500 time: 0.0111s\n",
            "Epoch: 0020 loss_train: 0.6741 acc_train: 0.7240 loss_val: 0.7711 acc_val: 0.3500 time: 0.0102s\n",
            "Epoch: 0021 loss_train: 0.6732 acc_train: 0.7220 loss_val: 0.7884 acc_val: 0.3312 time: 0.0088s\n",
            "Epoch: 0022 loss_train: 0.6727 acc_train: 0.7020 loss_val: 0.8033 acc_val: 0.3000 time: 0.0082s\n",
            "Epoch: 0023 loss_train: 0.6722 acc_train: 0.6940 loss_val: 0.8070 acc_val: 0.3000 time: 0.0105s\n",
            "Epoch: 0024 loss_train: 0.6718 acc_train: 0.6900 loss_val: 0.7994 acc_val: 0.3250 time: 0.0088s\n",
            "Epoch: 0025 loss_train: 0.6698 acc_train: 0.7060 loss_val: 0.7898 acc_val: 0.3375 time: 0.0082s\n",
            "Epoch: 0026 loss_train: 0.6698 acc_train: 0.7200 loss_val: 0.7923 acc_val: 0.3500 time: 0.0079s\n",
            "Epoch: 0027 loss_train: 0.6696 acc_train: 0.7160 loss_val: 0.8040 acc_val: 0.3375 time: 0.0086s\n",
            "Epoch: 0028 loss_train: 0.6695 acc_train: 0.7100 loss_val: 0.8145 acc_val: 0.3125 time: 0.0084s\n",
            "Epoch: 0029 loss_train: 0.6684 acc_train: 0.7180 loss_val: 0.8176 acc_val: 0.3187 time: 0.0089s\n",
            "Epoch: 0030 loss_train: 0.6668 acc_train: 0.7200 loss_val: 0.8141 acc_val: 0.3250 time: 0.0086s\n",
            "Epoch: 0031 loss_train: 0.6675 acc_train: 0.7180 loss_val: 0.8060 acc_val: 0.3563 time: 0.0095s\n",
            "Epoch: 0032 loss_train: 0.6667 acc_train: 0.7340 loss_val: 0.8129 acc_val: 0.3500 time: 0.0096s\n",
            "Epoch: 0033 loss_train: 0.6664 acc_train: 0.7340 loss_val: 0.8303 acc_val: 0.3125 time: 0.0093s\n",
            "Epoch: 0034 loss_train: 0.6653 acc_train: 0.7180 loss_val: 0.8342 acc_val: 0.3125 time: 0.0088s\n",
            "Epoch: 0035 loss_train: 0.6652 acc_train: 0.7180 loss_val: 0.8251 acc_val: 0.3250 time: 0.0094s\n",
            "Epoch: 0036 loss_train: 0.6640 acc_train: 0.7280 loss_val: 0.8210 acc_val: 0.3375 time: 0.0095s\n",
            "Epoch: 0037 loss_train: 0.6640 acc_train: 0.7280 loss_val: 0.8340 acc_val: 0.3187 time: 0.0085s\n",
            "Epoch: 0038 loss_train: 0.6626 acc_train: 0.7280 loss_val: 0.8502 acc_val: 0.3125 time: 0.0089s\n",
            "Epoch: 0039 loss_train: 0.6629 acc_train: 0.7280 loss_val: 0.8461 acc_val: 0.3187 time: 0.0080s\n",
            "Epoch: 0040 loss_train: 0.6606 acc_train: 0.7320 loss_val: 0.8400 acc_val: 0.3375 time: 0.0086s\n",
            "Epoch: 0041 loss_train: 0.6616 acc_train: 0.7360 loss_val: 0.8444 acc_val: 0.3438 time: 0.0086s\n",
            "Epoch: 0042 loss_train: 0.6615 acc_train: 0.7300 loss_val: 0.8589 acc_val: 0.3375 time: 0.0080s\n",
            "Epoch: 0043 loss_train: 0.6608 acc_train: 0.7240 loss_val: 0.8547 acc_val: 0.3500 time: 0.0081s\n",
            "Epoch: 0044 loss_train: 0.6596 acc_train: 0.7360 loss_val: 0.8532 acc_val: 0.3500 time: 0.0094s\n",
            "Epoch: 0045 loss_train: 0.6588 acc_train: 0.7420 loss_val: 0.8641 acc_val: 0.3500 time: 0.0111s\n",
            "Epoch: 0046 loss_train: 0.6585 acc_train: 0.7320 loss_val: 0.8669 acc_val: 0.3438 time: 0.0082s\n",
            "Epoch: 0047 loss_train: 0.6586 acc_train: 0.7340 loss_val: 0.8702 acc_val: 0.3500 time: 0.0082s\n",
            "Epoch: 0048 loss_train: 0.6575 acc_train: 0.7320 loss_val: 0.8743 acc_val: 0.3500 time: 0.0079s\n",
            "Epoch: 0049 loss_train: 0.6570 acc_train: 0.7260 loss_val: 0.8743 acc_val: 0.3375 time: 0.0083s\n",
            "Epoch: 0050 loss_train: 0.6556 acc_train: 0.7340 loss_val: 0.8884 acc_val: 0.3250 time: 0.0083s\n",
            "Epoch: 0051 loss_train: 0.6558 acc_train: 0.7400 loss_val: 0.8762 acc_val: 0.3438 time: 0.0081s\n",
            "Epoch: 0052 loss_train: 0.6538 acc_train: 0.7440 loss_val: 0.8978 acc_val: 0.3250 time: 0.0079s\n",
            "Epoch: 0053 loss_train: 0.6554 acc_train: 0.7260 loss_val: 0.8854 acc_val: 0.3375 time: 0.0079s\n",
            "Epoch: 0054 loss_train: 0.6533 acc_train: 0.7380 loss_val: 0.9041 acc_val: 0.3187 time: 0.0079s\n",
            "Epoch: 0055 loss_train: 0.6540 acc_train: 0.7400 loss_val: 0.9087 acc_val: 0.3187 time: 0.0085s\n",
            "Epoch: 0056 loss_train: 0.6527 acc_train: 0.7420 loss_val: 0.9008 acc_val: 0.3250 time: 0.0087s\n",
            "Epoch: 0057 loss_train: 0.6513 acc_train: 0.7480 loss_val: 0.9287 acc_val: 0.3125 time: 0.0082s\n",
            "Epoch: 0058 loss_train: 0.6533 acc_train: 0.7320 loss_val: 0.8959 acc_val: 0.3312 time: 0.0086s\n",
            "Epoch: 0059 loss_train: 0.6510 acc_train: 0.7540 loss_val: 0.9437 acc_val: 0.3063 time: 0.0077s\n",
            "Epoch: 0060 loss_train: 0.6504 acc_train: 0.7380 loss_val: 0.9093 acc_val: 0.3312 time: 0.0084s\n",
            "Epoch: 0061 loss_train: 0.6500 acc_train: 0.7500 loss_val: 0.9467 acc_val: 0.3063 time: 0.0078s\n",
            "Epoch: 0062 loss_train: 0.6488 acc_train: 0.7420 loss_val: 0.9302 acc_val: 0.3250 time: 0.0079s\n",
            "Epoch: 0063 loss_train: 0.6494 acc_train: 0.7400 loss_val: 0.9218 acc_val: 0.3250 time: 0.0085s\n",
            "Epoch: 0064 loss_train: 0.6490 acc_train: 0.7540 loss_val: 0.9821 acc_val: 0.3000 time: 0.0099s\n",
            "Epoch: 0065 loss_train: 0.6490 acc_train: 0.7340 loss_val: 0.9105 acc_val: 0.3812 time: 0.0115s\n",
            "Epoch: 0066 loss_train: 0.6485 acc_train: 0.7600 loss_val: 0.9766 acc_val: 0.3000 time: 0.0121s\n",
            "Epoch: 0067 loss_train: 0.6477 acc_train: 0.7380 loss_val: 0.9616 acc_val: 0.3187 time: 0.0130s\n",
            "Epoch: 0068 loss_train: 0.6475 acc_train: 0.7380 loss_val: 0.9166 acc_val: 0.3812 time: 0.0122s\n",
            "Epoch: 0069 loss_train: 0.6470 acc_train: 0.7560 loss_val: 1.0169 acc_val: 0.2938 time: 0.0109s\n",
            "Epoch: 0070 loss_train: 0.6462 acc_train: 0.7340 loss_val: 0.9492 acc_val: 0.3500 time: 0.0083s\n",
            "Epoch: 0071 loss_train: 0.6460 acc_train: 0.7540 loss_val: 0.9430 acc_val: 0.3625 time: 0.0097s\n",
            "Epoch: 0072 loss_train: 0.6464 acc_train: 0.7520 loss_val: 1.0233 acc_val: 0.2938 time: 0.0114s\n",
            "Epoch: 0073 loss_train: 0.6439 acc_train: 0.7280 loss_val: 0.9682 acc_val: 0.3187 time: 0.0077s\n",
            "Epoch: 0074 loss_train: 0.6449 acc_train: 0.7560 loss_val: 0.9516 acc_val: 0.3625 time: 0.0084s\n",
            "Epoch: 0075 loss_train: 0.6440 acc_train: 0.7700 loss_val: 1.0236 acc_val: 0.3000 time: 0.0078s\n",
            "Epoch: 0076 loss_train: 0.6445 acc_train: 0.7500 loss_val: 0.9812 acc_val: 0.3063 time: 0.0081s\n",
            "Epoch: 0077 loss_train: 0.6442 acc_train: 0.7440 loss_val: 0.9555 acc_val: 0.3438 time: 0.0084s\n",
            "Epoch: 0078 loss_train: 0.6442 acc_train: 0.7540 loss_val: 1.0218 acc_val: 0.3063 time: 0.0087s\n",
            "Epoch: 0079 loss_train: 0.6415 acc_train: 0.7440 loss_val: 0.9907 acc_val: 0.3063 time: 0.0075s\n",
            "Epoch: 0080 loss_train: 0.6435 acc_train: 0.7440 loss_val: 0.9728 acc_val: 0.3375 time: 0.0082s\n",
            "Epoch: 0081 loss_train: 0.6428 acc_train: 0.7560 loss_val: 1.0346 acc_val: 0.3000 time: 0.0088s\n",
            "Epoch: 0082 loss_train: 0.6432 acc_train: 0.7500 loss_val: 0.9914 acc_val: 0.3187 time: 0.0116s\n",
            "Epoch: 0083 loss_train: 0.6424 acc_train: 0.7560 loss_val: 0.9945 acc_val: 0.3250 time: 0.0092s\n",
            "Epoch: 0084 loss_train: 0.6410 acc_train: 0.7600 loss_val: 1.0339 acc_val: 0.2938 time: 0.0091s\n",
            "Epoch: 0085 loss_train: 0.6415 acc_train: 0.7380 loss_val: 0.9967 acc_val: 0.3625 time: 0.0089s\n",
            "Epoch: 0086 loss_train: 0.6405 acc_train: 0.7580 loss_val: 1.0142 acc_val: 0.3125 time: 0.0093s\n",
            "Epoch: 0087 loss_train: 0.6392 acc_train: 0.7500 loss_val: 1.0232 acc_val: 0.3063 time: 0.0099s\n",
            "Epoch: 0088 loss_train: 0.6398 acc_train: 0.7580 loss_val: 1.0035 acc_val: 0.3625 time: 0.0115s\n",
            "Epoch: 0089 loss_train: 0.6404 acc_train: 0.7660 loss_val: 1.0249 acc_val: 0.3125 time: 0.0101s\n",
            "Epoch: 0090 loss_train: 0.6389 acc_train: 0.7560 loss_val: 1.0323 acc_val: 0.3000 time: 0.0078s\n",
            "Epoch: 0091 loss_train: 0.6380 acc_train: 0.7480 loss_val: 1.0184 acc_val: 0.3312 time: 0.0082s\n",
            "Epoch: 0092 loss_train: 0.6383 acc_train: 0.7600 loss_val: 1.0416 acc_val: 0.3000 time: 0.0083s\n",
            "Epoch: 0093 loss_train: 0.6386 acc_train: 0.7440 loss_val: 1.0228 acc_val: 0.3187 time: 0.0083s\n",
            "Epoch: 0094 loss_train: 0.6390 acc_train: 0.7560 loss_val: 1.0509 acc_val: 0.3000 time: 0.0079s\n",
            "Epoch: 0095 loss_train: 0.6383 acc_train: 0.7560 loss_val: 1.0405 acc_val: 0.3000 time: 0.0083s\n",
            "Epoch: 0096 loss_train: 0.6375 acc_train: 0.7480 loss_val: 1.0261 acc_val: 0.3500 time: 0.0085s\n",
            "Epoch: 0097 loss_train: 0.6391 acc_train: 0.7560 loss_val: 1.0828 acc_val: 0.2812 time: 0.0085s\n",
            "Epoch: 0098 loss_train: 0.6393 acc_train: 0.7460 loss_val: 1.0016 acc_val: 0.3812 time: 0.0086s\n",
            "Epoch: 0099 loss_train: 0.6381 acc_train: 0.7680 loss_val: 1.0907 acc_val: 0.2750 time: 0.0086s\n",
            "Epoch: 0100 loss_train: 0.6365 acc_train: 0.7360 loss_val: 1.0359 acc_val: 0.3438 time: 0.0081s\n",
            "Epoch: 0101 loss_train: 0.6367 acc_train: 0.7640 loss_val: 1.0361 acc_val: 0.3563 time: 0.0088s\n",
            "Epoch: 0102 loss_train: 0.6368 acc_train: 0.7700 loss_val: 1.0878 acc_val: 0.2875 time: 0.0079s\n",
            "Epoch: 0103 loss_train: 0.6381 acc_train: 0.7460 loss_val: 1.0251 acc_val: 0.3750 time: 0.0082s\n",
            "Epoch: 0104 loss_train: 0.6375 acc_train: 0.7660 loss_val: 1.1005 acc_val: 0.2812 time: 0.0075s\n",
            "Epoch: 0105 loss_train: 0.6385 acc_train: 0.7500 loss_val: 1.0258 acc_val: 0.3750 time: 0.0076s\n",
            "Epoch: 0106 loss_train: 0.6375 acc_train: 0.7540 loss_val: 1.0610 acc_val: 0.3000 time: 0.0078s\n",
            "Epoch: 0107 loss_train: 0.6356 acc_train: 0.7460 loss_val: 1.0878 acc_val: 0.2875 time: 0.0077s\n",
            "Epoch: 0108 loss_train: 0.6361 acc_train: 0.7500 loss_val: 1.0302 acc_val: 0.3688 time: 0.0080s\n",
            "Epoch: 0109 loss_train: 0.6359 acc_train: 0.7600 loss_val: 1.0723 acc_val: 0.3000 time: 0.0085s\n",
            "Epoch: 0110 loss_train: 0.6357 acc_train: 0.7500 loss_val: 1.0914 acc_val: 0.2875 time: 0.0083s\n",
            "Epoch: 0111 loss_train: 0.6362 acc_train: 0.7500 loss_val: 1.0178 acc_val: 0.3812 time: 0.0112s\n",
            "Epoch: 0112 loss_train: 0.6353 acc_train: 0.7500 loss_val: 1.1334 acc_val: 0.2687 time: 0.0082s\n",
            "Epoch: 0113 loss_train: 0.6352 acc_train: 0.7380 loss_val: 1.0425 acc_val: 0.3688 time: 0.0079s\n",
            "Epoch: 0114 loss_train: 0.6335 acc_train: 0.7600 loss_val: 1.0644 acc_val: 0.3438 time: 0.0079s\n",
            "Epoch: 0115 loss_train: 0.6342 acc_train: 0.7640 loss_val: 1.1376 acc_val: 0.2687 time: 0.0080s\n",
            "Epoch: 0116 loss_train: 0.6361 acc_train: 0.7440 loss_val: 1.0186 acc_val: 0.3812 time: 0.0087s\n",
            "Epoch: 0117 loss_train: 0.6356 acc_train: 0.7560 loss_val: 1.1067 acc_val: 0.2812 time: 0.0079s\n",
            "Epoch: 0118 loss_train: 0.6356 acc_train: 0.7460 loss_val: 1.1161 acc_val: 0.2812 time: 0.0086s\n",
            "Epoch: 0119 loss_train: 0.6344 acc_train: 0.7560 loss_val: 1.0248 acc_val: 0.3812 time: 0.0075s\n",
            "Epoch: 0120 loss_train: 0.6353 acc_train: 0.7700 loss_val: 1.1322 acc_val: 0.2750 time: 0.0079s\n",
            "Epoch: 0121 loss_train: 0.6338 acc_train: 0.7480 loss_val: 1.0837 acc_val: 0.3063 time: 0.0079s\n",
            "Epoch: 0122 loss_train: 0.6319 acc_train: 0.7480 loss_val: 1.0443 acc_val: 0.3750 time: 0.0080s\n",
            "Epoch: 0123 loss_train: 0.6353 acc_train: 0.7520 loss_val: 1.1355 acc_val: 0.2750 time: 0.0086s\n",
            "Epoch: 0124 loss_train: 0.6348 acc_train: 0.7440 loss_val: 1.0693 acc_val: 0.3563 time: 0.0078s\n",
            "Epoch: 0125 loss_train: 0.6330 acc_train: 0.7640 loss_val: 1.0619 acc_val: 0.3750 time: 0.0089s\n",
            "Epoch: 0126 loss_train: 0.6347 acc_train: 0.7580 loss_val: 1.1229 acc_val: 0.2812 time: 0.0112s\n",
            "Epoch: 0127 loss_train: 0.6335 acc_train: 0.7600 loss_val: 1.0908 acc_val: 0.3438 time: 0.0110s\n",
            "Epoch: 0128 loss_train: 0.6327 acc_train: 0.7520 loss_val: 1.0563 acc_val: 0.3812 time: 0.0082s\n",
            "Epoch: 0129 loss_train: 0.6326 acc_train: 0.7540 loss_val: 1.1275 acc_val: 0.2812 time: 0.0086s\n",
            "Epoch: 0130 loss_train: 0.6333 acc_train: 0.7440 loss_val: 1.1049 acc_val: 0.3000 time: 0.0086s\n",
            "Epoch: 0131 loss_train: 0.6342 acc_train: 0.7580 loss_val: 1.0533 acc_val: 0.3812 time: 0.0088s\n",
            "Epoch: 0132 loss_train: 0.6336 acc_train: 0.7640 loss_val: 1.1421 acc_val: 0.2812 time: 0.0087s\n",
            "Epoch: 0133 loss_train: 0.6327 acc_train: 0.7440 loss_val: 1.0974 acc_val: 0.3375 time: 0.0103s\n",
            "Epoch: 0134 loss_train: 0.6326 acc_train: 0.7640 loss_val: 1.0486 acc_val: 0.3812 time: 0.0095s\n",
            "Epoch: 0135 loss_train: 0.6310 acc_train: 0.7680 loss_val: 1.1471 acc_val: 0.2812 time: 0.0078s\n",
            "Epoch: 0136 loss_train: 0.6318 acc_train: 0.7500 loss_val: 1.1145 acc_val: 0.2938 time: 0.0085s\n",
            "Epoch: 0137 loss_train: 0.6319 acc_train: 0.7640 loss_val: 1.0491 acc_val: 0.3875 time: 0.0082s\n",
            "Epoch: 0138 loss_train: 0.6315 acc_train: 0.7580 loss_val: 1.1518 acc_val: 0.2687 time: 0.0083s\n",
            "Epoch: 0139 loss_train: 0.6313 acc_train: 0.7560 loss_val: 1.1198 acc_val: 0.2938 time: 0.0076s\n",
            "Epoch: 0140 loss_train: 0.6324 acc_train: 0.7540 loss_val: 1.0816 acc_val: 0.3563 time: 0.0075s\n",
            "Epoch: 0141 loss_train: 0.6319 acc_train: 0.7640 loss_val: 1.1279 acc_val: 0.2812 time: 0.0086s\n",
            "Epoch: 0142 loss_train: 0.6313 acc_train: 0.7520 loss_val: 1.1297 acc_val: 0.2812 time: 0.0096s\n",
            "Epoch: 0143 loss_train: 0.6304 acc_train: 0.7500 loss_val: 1.0911 acc_val: 0.3375 time: 0.0087s\n",
            "Epoch: 0144 loss_train: 0.6300 acc_train: 0.7660 loss_val: 1.1100 acc_val: 0.3312 time: 0.0079s\n",
            "Epoch: 0145 loss_train: 0.6295 acc_train: 0.7680 loss_val: 1.1357 acc_val: 0.2812 time: 0.0080s\n",
            "Epoch: 0146 loss_train: 0.6316 acc_train: 0.7500 loss_val: 1.0987 acc_val: 0.3500 time: 0.0080s\n",
            "Epoch: 0147 loss_train: 0.6297 acc_train: 0.7620 loss_val: 1.1212 acc_val: 0.3250 time: 0.0077s\n",
            "Epoch: 0148 loss_train: 0.6269 acc_train: 0.7520 loss_val: 1.1318 acc_val: 0.2875 time: 0.0086s\n",
            "Epoch: 0149 loss_train: 0.6300 acc_train: 0.7620 loss_val: 1.1078 acc_val: 0.3438 time: 0.0083s\n",
            "Epoch: 0150 loss_train: 0.6311 acc_train: 0.7660 loss_val: 1.1285 acc_val: 0.3250 time: 0.0086s\n",
            "Epoch: 0151 loss_train: 0.6290 acc_train: 0.7600 loss_val: 1.1305 acc_val: 0.3250 time: 0.0082s\n",
            "Epoch: 0152 loss_train: 0.6299 acc_train: 0.7680 loss_val: 1.1070 acc_val: 0.3500 time: 0.0089s\n",
            "Epoch: 0153 loss_train: 0.6293 acc_train: 0.7700 loss_val: 1.1229 acc_val: 0.3312 time: 0.0080s\n",
            "Epoch: 0154 loss_train: 0.6279 acc_train: 0.7660 loss_val: 1.1582 acc_val: 0.2812 time: 0.0086s\n",
            "Epoch: 0155 loss_train: 0.6289 acc_train: 0.7500 loss_val: 1.1023 acc_val: 0.3500 time: 0.0085s\n",
            "Epoch: 0156 loss_train: 0.6295 acc_train: 0.7620 loss_val: 1.1383 acc_val: 0.3187 time: 0.0108s\n",
            "Epoch: 0157 loss_train: 0.6271 acc_train: 0.7640 loss_val: 1.1396 acc_val: 0.3187 time: 0.0091s\n",
            "Epoch: 0158 loss_train: 0.6290 acc_train: 0.7560 loss_val: 1.0978 acc_val: 0.3625 time: 0.0075s\n",
            "Epoch: 0159 loss_train: 0.6297 acc_train: 0.7660 loss_val: 1.1773 acc_val: 0.2562 time: 0.0087s\n",
            "Epoch: 0160 loss_train: 0.6304 acc_train: 0.7460 loss_val: 1.1057 acc_val: 0.3438 time: 0.0078s\n",
            "Epoch: 0161 loss_train: 0.6296 acc_train: 0.7640 loss_val: 1.1206 acc_val: 0.3375 time: 0.0081s\n",
            "Epoch: 0162 loss_train: 0.6259 acc_train: 0.7780 loss_val: 1.1841 acc_val: 0.2500 time: 0.0080s\n",
            "Epoch: 0163 loss_train: 0.6278 acc_train: 0.7540 loss_val: 1.1042 acc_val: 0.3563 time: 0.0078s\n",
            "Epoch: 0164 loss_train: 0.6277 acc_train: 0.7660 loss_val: 1.1265 acc_val: 0.3375 time: 0.0078s\n",
            "Epoch: 0165 loss_train: 0.6263 acc_train: 0.7620 loss_val: 1.1704 acc_val: 0.2625 time: 0.0084s\n",
            "Epoch: 0166 loss_train: 0.6286 acc_train: 0.7560 loss_val: 1.0979 acc_val: 0.3625 time: 0.0087s\n",
            "Epoch: 0167 loss_train: 0.6272 acc_train: 0.7740 loss_val: 1.1601 acc_val: 0.2750 time: 0.0079s\n",
            "Epoch: 0168 loss_train: 0.6277 acc_train: 0.7580 loss_val: 1.1542 acc_val: 0.2812 time: 0.0089s\n",
            "Epoch: 0169 loss_train: 0.6263 acc_train: 0.7520 loss_val: 1.1279 acc_val: 0.3438 time: 0.0087s\n",
            "Epoch: 0170 loss_train: 0.6273 acc_train: 0.7660 loss_val: 1.1450 acc_val: 0.3312 time: 0.0084s\n",
            "Epoch: 0171 loss_train: 0.6269 acc_train: 0.7640 loss_val: 1.1683 acc_val: 0.2938 time: 0.0088s\n",
            "Epoch: 0172 loss_train: 0.6259 acc_train: 0.7580 loss_val: 1.1233 acc_val: 0.3438 time: 0.0078s\n",
            "Epoch: 0173 loss_train: 0.6260 acc_train: 0.7780 loss_val: 1.1333 acc_val: 0.3438 time: 0.0078s\n",
            "Epoch: 0174 loss_train: 0.6254 acc_train: 0.7720 loss_val: 1.1698 acc_val: 0.3000 time: 0.0085s\n",
            "Epoch: 0175 loss_train: 0.6261 acc_train: 0.7640 loss_val: 1.1455 acc_val: 0.3312 time: 0.0083s\n",
            "Epoch: 0176 loss_train: 0.6254 acc_train: 0.7700 loss_val: 1.1316 acc_val: 0.3500 time: 0.0077s\n",
            "Epoch: 0177 loss_train: 0.6259 acc_train: 0.7700 loss_val: 1.1673 acc_val: 0.3125 time: 0.0079s\n",
            "Epoch: 0178 loss_train: 0.6246 acc_train: 0.7700 loss_val: 1.1696 acc_val: 0.3063 time: 0.0081s\n",
            "Epoch: 0179 loss_train: 0.6249 acc_train: 0.7640 loss_val: 1.1187 acc_val: 0.3563 time: 0.0109s\n",
            "Epoch: 0180 loss_train: 0.6260 acc_train: 0.7660 loss_val: 1.1886 acc_val: 0.2750 time: 0.0084s\n",
            "Epoch: 0181 loss_train: 0.6258 acc_train: 0.7500 loss_val: 1.1559 acc_val: 0.3312 time: 0.0081s\n",
            "Epoch: 0182 loss_train: 0.6247 acc_train: 0.7700 loss_val: 1.1542 acc_val: 0.3375 time: 0.0085s\n",
            "Epoch: 0183 loss_train: 0.6248 acc_train: 0.7640 loss_val: 1.1753 acc_val: 0.3063 time: 0.0085s\n",
            "Epoch: 0184 loss_train: 0.6225 acc_train: 0.7600 loss_val: 1.1574 acc_val: 0.3312 time: 0.0077s\n",
            "Epoch: 0185 loss_train: 0.6257 acc_train: 0.7660 loss_val: 1.1476 acc_val: 0.3438 time: 0.0076s\n",
            "Epoch: 0186 loss_train: 0.6254 acc_train: 0.7760 loss_val: 1.1883 acc_val: 0.3000 time: 0.0087s\n",
            "Epoch: 0187 loss_train: 0.6241 acc_train: 0.7660 loss_val: 1.1417 acc_val: 0.3438 time: 0.0078s\n",
            "Epoch: 0188 loss_train: 0.6225 acc_train: 0.7700 loss_val: 1.1881 acc_val: 0.3063 time: 0.0085s\n",
            "Epoch: 0189 loss_train: 0.6227 acc_train: 0.7640 loss_val: 1.1628 acc_val: 0.3312 time: 0.0079s\n",
            "Epoch: 0190 loss_train: 0.6255 acc_train: 0.7520 loss_val: 1.1725 acc_val: 0.3250 time: 0.0082s\n",
            "Epoch: 0191 loss_train: 0.6221 acc_train: 0.7660 loss_val: 1.2079 acc_val: 0.2687 time: 0.0094s\n",
            "Epoch: 0192 loss_train: 0.6232 acc_train: 0.7560 loss_val: 1.1086 acc_val: 0.3688 time: 0.0086s\n",
            "Epoch: 0193 loss_train: 0.6247 acc_train: 0.7760 loss_val: 1.2313 acc_val: 0.2500 time: 0.0092s\n",
            "Epoch: 0194 loss_train: 0.6235 acc_train: 0.7620 loss_val: 1.1631 acc_val: 0.2938 time: 0.0081s\n",
            "Epoch: 0195 loss_train: 0.6235 acc_train: 0.7480 loss_val: 1.1418 acc_val: 0.3563 time: 0.0080s\n",
            "Epoch: 0196 loss_train: 0.6223 acc_train: 0.7740 loss_val: 1.2344 acc_val: 0.2500 time: 0.0082s\n",
            "Epoch: 0197 loss_train: 0.6244 acc_train: 0.7520 loss_val: 1.1194 acc_val: 0.3625 time: 0.0076s\n",
            "Epoch: 0198 loss_train: 0.6237 acc_train: 0.7700 loss_val: 1.2108 acc_val: 0.3000 time: 0.0097s\n",
            "Epoch: 0199 loss_train: 0.6226 acc_train: 0.7660 loss_val: 1.2010 acc_val: 0.3000 time: 0.0081s\n",
            "Epoch: 0200 loss_train: 0.6225 acc_train: 0.7780 loss_val: 1.1360 acc_val: 0.3563 time: 0.0086s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 1.8877s\n",
            "Test set results: loss= 1.0605 accuracy= 0.3382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqrrHtd7SGT7"
      },
      "source": [
        "建立NeuMF层模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYWxFtfXrg9a",
        "outputId": "554533de-08af-4cb8-8356-99280b36c941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "#from keras import initializations\n",
        "from keras.regularizers import l1, l2#, l1l2\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Dense, Lambda, Activation\n",
        "from keras.layers import Embedding, Input, Dense, merge, Reshape, Flatten, Dropout\n",
        "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
        "from time import time\n",
        "import sys\n",
        "import argparse\n",
        "import scipy.sparse as sp"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kHfN6ORfXap"
      },
      "source": [
        "使用数据生成正负样本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNF9ZTvvoe3N"
      },
      "source": [
        "def load_rating_file_as_matrix(filename):\n",
        "  '''\n",
        "  Read .rating file and Return dok matrix.\n",
        "  The first line of .rating file is: num_users\\t num_items\n",
        "  '''\n",
        "  # Get number of users and items\n",
        "  num_users, num_items = 0, 0\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      u, i = int(arr[0]), int(arr[1])\n",
        "      num_users = max(num_users, u)\n",
        "      num_items = max(num_items, i)\n",
        "      line = f.readline()\n",
        "  # Construct matrix\n",
        "  print(num_users)\n",
        "  print(num_items)\n",
        "  mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      #user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "      #if (rating > 0):\n",
        "      user, item = int(arr[0]), int(arr[1])\n",
        "      mat[user, item] = 1.0\n",
        "      line = f.readline()    \n",
        "  return mat"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCaosFTwqXrM",
        "outputId": "e6b5c362-4a28-4930-c9f1-c5a4a0ee2d48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "filename = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum18416.csv'\n",
        "train_Martrix = load_rating_file_as_matrix(filename)\n",
        "train_Martrix"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "268\n",
            "866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<269x867 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 18416 stored elements in Dictionary Of Keys format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDjZWK9vQ5R6"
      },
      "source": [
        "将vector of each nodes construct a serise of pairs of nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qke2rO5PBnfa",
        "outputId": "2a631f82-a5d6-48d6-976c-4be903bd7a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "# 读取负样本\n",
        "N_filename = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/NegativeSample.txt'\n",
        "negative_data = pd.read_table(N_filename,sep='\\t',header=None)\n",
        "negative_data = negative_data[:17414]\n",
        "negative_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17409</th>\n",
              "      <td>264</td>\n",
              "      <td>748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17410</th>\n",
              "      <td>264</td>\n",
              "      <td>738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17411</th>\n",
              "      <td>264</td>\n",
              "      <td>636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17412</th>\n",
              "      <td>264</td>\n",
              "      <td>523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17413</th>\n",
              "      <td>264</td>\n",
              "      <td>477</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17414 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1\n",
              "0        4  279\n",
              "1        4  557\n",
              "2        4  543\n",
              "3        4  421\n",
              "4        4  666\n",
              "...    ...  ...\n",
              "17409  264  748\n",
              "17410  264  738\n",
              "17411  264  636\n",
              "17412  264  523\n",
              "17413  264  477\n",
              "\n",
              "[17414 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DOd7e4MLCCD",
        "outputId": "1f08afd2-0fa8-446c-9dc5-dfef1841f615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "source": [
        "# 读取Embedding node2vec\n",
        "E_filename = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/Embedding_Node2vec18416.txt'\n",
        "Embedding_Node2vec = pd.read_csv(E_filename,sep=' ',header=None)\n",
        "Embedding_Node2vec = Embedding_Node2vec.sort_values(0,ascending=True)# 建立序号排序\n",
        "Embedding_Node2vec.reset_index(drop=True, inplace=True) \n",
        "Embedding_Node2vec = Embedding_Node2vec.iloc[:,1:]\n",
        "Embedding_Node2vec"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.039781</td>\n",
              "      <td>-0.335233</td>\n",
              "      <td>-0.248732</td>\n",
              "      <td>-0.535646</td>\n",
              "      <td>-0.678693</td>\n",
              "      <td>0.224000</td>\n",
              "      <td>0.083105</td>\n",
              "      <td>-0.087151</td>\n",
              "      <td>0.477437</td>\n",
              "      <td>0.045952</td>\n",
              "      <td>-0.635879</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>-0.556794</td>\n",
              "      <td>0.050988</td>\n",
              "      <td>0.438524</td>\n",
              "      <td>-0.694794</td>\n",
              "      <td>-0.383268</td>\n",
              "      <td>0.322675</td>\n",
              "      <td>-0.449308</td>\n",
              "      <td>-0.179545</td>\n",
              "      <td>0.881999</td>\n",
              "      <td>-0.399369</td>\n",
              "      <td>0.116898</td>\n",
              "      <td>-0.088363</td>\n",
              "      <td>0.264947</td>\n",
              "      <td>-0.097121</td>\n",
              "      <td>0.764488</td>\n",
              "      <td>0.059529</td>\n",
              "      <td>0.123631</td>\n",
              "      <td>0.236388</td>\n",
              "      <td>0.255058</td>\n",
              "      <td>-0.331762</td>\n",
              "      <td>0.039242</td>\n",
              "      <td>0.347407</td>\n",
              "      <td>0.488749</td>\n",
              "      <td>-0.394416</td>\n",
              "      <td>-0.052347</td>\n",
              "      <td>-0.260033</td>\n",
              "      <td>0.088437</td>\n",
              "      <td>0.119286</td>\n",
              "      <td>-0.335932</td>\n",
              "      <td>-0.160803</td>\n",
              "      <td>-0.363354</td>\n",
              "      <td>0.425962</td>\n",
              "      <td>0.477789</td>\n",
              "      <td>-0.178279</td>\n",
              "      <td>-0.172379</td>\n",
              "      <td>-0.143428</td>\n",
              "      <td>0.143136</td>\n",
              "      <td>0.261178</td>\n",
              "      <td>-0.096698</td>\n",
              "      <td>-0.448925</td>\n",
              "      <td>-0.098317</td>\n",
              "      <td>-0.233874</td>\n",
              "      <td>-0.335732</td>\n",
              "      <td>-0.034095</td>\n",
              "      <td>0.107028</td>\n",
              "      <td>0.213188</td>\n",
              "      <td>0.422625</td>\n",
              "      <td>0.377720</td>\n",
              "      <td>0.221889</td>\n",
              "      <td>-0.330601</td>\n",
              "      <td>0.119765</td>\n",
              "      <td>-0.260214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.075479</td>\n",
              "      <td>-0.294005</td>\n",
              "      <td>-0.046772</td>\n",
              "      <td>0.237143</td>\n",
              "      <td>-0.458318</td>\n",
              "      <td>-0.291966</td>\n",
              "      <td>-0.199544</td>\n",
              "      <td>0.108335</td>\n",
              "      <td>-0.454633</td>\n",
              "      <td>0.347237</td>\n",
              "      <td>0.096521</td>\n",
              "      <td>0.127930</td>\n",
              "      <td>-0.080794</td>\n",
              "      <td>0.352098</td>\n",
              "      <td>-0.073850</td>\n",
              "      <td>0.016558</td>\n",
              "      <td>-0.132856</td>\n",
              "      <td>-0.149450</td>\n",
              "      <td>0.268198</td>\n",
              "      <td>-0.257542</td>\n",
              "      <td>0.282167</td>\n",
              "      <td>-0.211521</td>\n",
              "      <td>-0.282441</td>\n",
              "      <td>-0.185783</td>\n",
              "      <td>0.208504</td>\n",
              "      <td>-0.224241</td>\n",
              "      <td>-0.134609</td>\n",
              "      <td>0.235056</td>\n",
              "      <td>-0.285485</td>\n",
              "      <td>-0.104913</td>\n",
              "      <td>-0.235070</td>\n",
              "      <td>0.118800</td>\n",
              "      <td>0.041633</td>\n",
              "      <td>-0.464412</td>\n",
              "      <td>0.042228</td>\n",
              "      <td>0.348361</td>\n",
              "      <td>0.156607</td>\n",
              "      <td>0.067848</td>\n",
              "      <td>-0.379439</td>\n",
              "      <td>-0.375601</td>\n",
              "      <td>0.151587</td>\n",
              "      <td>-0.119523</td>\n",
              "      <td>-0.187936</td>\n",
              "      <td>0.047140</td>\n",
              "      <td>-0.305963</td>\n",
              "      <td>0.178903</td>\n",
              "      <td>-0.247258</td>\n",
              "      <td>0.028649</td>\n",
              "      <td>0.294739</td>\n",
              "      <td>-0.299021</td>\n",
              "      <td>0.015989</td>\n",
              "      <td>0.182624</td>\n",
              "      <td>-0.385533</td>\n",
              "      <td>0.236361</td>\n",
              "      <td>0.112295</td>\n",
              "      <td>-0.389604</td>\n",
              "      <td>-0.204730</td>\n",
              "      <td>-0.252193</td>\n",
              "      <td>0.635018</td>\n",
              "      <td>0.189516</td>\n",
              "      <td>0.101471</td>\n",
              "      <td>0.112080</td>\n",
              "      <td>-0.335977</td>\n",
              "      <td>-0.044735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.157106</td>\n",
              "      <td>-0.175712</td>\n",
              "      <td>0.023253</td>\n",
              "      <td>-0.032772</td>\n",
              "      <td>0.071081</td>\n",
              "      <td>0.116083</td>\n",
              "      <td>-0.617599</td>\n",
              "      <td>-0.335524</td>\n",
              "      <td>-0.046631</td>\n",
              "      <td>0.454473</td>\n",
              "      <td>0.228802</td>\n",
              "      <td>0.172570</td>\n",
              "      <td>0.100656</td>\n",
              "      <td>-0.040965</td>\n",
              "      <td>-0.378550</td>\n",
              "      <td>0.066289</td>\n",
              "      <td>-0.073959</td>\n",
              "      <td>-0.180003</td>\n",
              "      <td>0.321569</td>\n",
              "      <td>0.299371</td>\n",
              "      <td>-0.004012</td>\n",
              "      <td>-0.127096</td>\n",
              "      <td>0.348403</td>\n",
              "      <td>0.145536</td>\n",
              "      <td>-0.070099</td>\n",
              "      <td>-0.223546</td>\n",
              "      <td>-0.306711</td>\n",
              "      <td>-0.193751</td>\n",
              "      <td>0.180603</td>\n",
              "      <td>-0.397566</td>\n",
              "      <td>-0.432159</td>\n",
              "      <td>-0.148621</td>\n",
              "      <td>0.112285</td>\n",
              "      <td>0.084235</td>\n",
              "      <td>0.189770</td>\n",
              "      <td>-0.094351</td>\n",
              "      <td>0.270869</td>\n",
              "      <td>0.005423</td>\n",
              "      <td>0.082824</td>\n",
              "      <td>0.191515</td>\n",
              "      <td>-0.060832</td>\n",
              "      <td>-0.142816</td>\n",
              "      <td>-0.270069</td>\n",
              "      <td>0.193098</td>\n",
              "      <td>-0.030191</td>\n",
              "      <td>0.308404</td>\n",
              "      <td>0.082115</td>\n",
              "      <td>-0.164072</td>\n",
              "      <td>0.469607</td>\n",
              "      <td>-0.005211</td>\n",
              "      <td>-0.097195</td>\n",
              "      <td>0.183946</td>\n",
              "      <td>-0.119373</td>\n",
              "      <td>-0.243924</td>\n",
              "      <td>-0.200405</td>\n",
              "      <td>-0.414571</td>\n",
              "      <td>-0.219225</td>\n",
              "      <td>-0.239065</td>\n",
              "      <td>0.240783</td>\n",
              "      <td>0.034274</td>\n",
              "      <td>0.759402</td>\n",
              "      <td>0.010972</td>\n",
              "      <td>0.234457</td>\n",
              "      <td>0.067845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.013436</td>\n",
              "      <td>-0.148017</td>\n",
              "      <td>0.352701</td>\n",
              "      <td>-0.131061</td>\n",
              "      <td>0.174146</td>\n",
              "      <td>0.129484</td>\n",
              "      <td>-0.413225</td>\n",
              "      <td>-0.111031</td>\n",
              "      <td>-0.146133</td>\n",
              "      <td>0.045019</td>\n",
              "      <td>0.256642</td>\n",
              "      <td>0.171290</td>\n",
              "      <td>0.595571</td>\n",
              "      <td>-0.160549</td>\n",
              "      <td>0.095094</td>\n",
              "      <td>0.430633</td>\n",
              "      <td>-0.201790</td>\n",
              "      <td>0.183508</td>\n",
              "      <td>0.369891</td>\n",
              "      <td>0.080270</td>\n",
              "      <td>0.444618</td>\n",
              "      <td>0.072056</td>\n",
              "      <td>-0.118598</td>\n",
              "      <td>-0.158481</td>\n",
              "      <td>0.058511</td>\n",
              "      <td>-0.252512</td>\n",
              "      <td>-0.064391</td>\n",
              "      <td>0.013366</td>\n",
              "      <td>0.340152</td>\n",
              "      <td>-0.490949</td>\n",
              "      <td>-0.081042</td>\n",
              "      <td>0.185482</td>\n",
              "      <td>0.067920</td>\n",
              "      <td>-0.114612</td>\n",
              "      <td>0.256302</td>\n",
              "      <td>-0.269262</td>\n",
              "      <td>0.266265</td>\n",
              "      <td>0.145428</td>\n",
              "      <td>0.215150</td>\n",
              "      <td>0.202037</td>\n",
              "      <td>-0.288611</td>\n",
              "      <td>0.365572</td>\n",
              "      <td>-0.255401</td>\n",
              "      <td>0.590876</td>\n",
              "      <td>0.340104</td>\n",
              "      <td>0.116361</td>\n",
              "      <td>0.535133</td>\n",
              "      <td>-0.343554</td>\n",
              "      <td>0.062785</td>\n",
              "      <td>0.177860</td>\n",
              "      <td>-0.135284</td>\n",
              "      <td>-0.164490</td>\n",
              "      <td>-0.036298</td>\n",
              "      <td>-0.143003</td>\n",
              "      <td>-0.401418</td>\n",
              "      <td>-0.652725</td>\n",
              "      <td>-0.051549</td>\n",
              "      <td>-0.264892</td>\n",
              "      <td>-0.115718</td>\n",
              "      <td>0.164063</td>\n",
              "      <td>0.369150</td>\n",
              "      <td>-0.573249</td>\n",
              "      <td>-0.459089</td>\n",
              "      <td>0.024804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.371445</td>\n",
              "      <td>0.159327</td>\n",
              "      <td>0.455429</td>\n",
              "      <td>-0.596464</td>\n",
              "      <td>-0.268588</td>\n",
              "      <td>0.126519</td>\n",
              "      <td>0.239619</td>\n",
              "      <td>0.008648</td>\n",
              "      <td>-0.673925</td>\n",
              "      <td>0.023879</td>\n",
              "      <td>-0.273572</td>\n",
              "      <td>0.239186</td>\n",
              "      <td>-0.078532</td>\n",
              "      <td>0.614255</td>\n",
              "      <td>0.102584</td>\n",
              "      <td>-0.337493</td>\n",
              "      <td>-0.354234</td>\n",
              "      <td>0.536146</td>\n",
              "      <td>-0.205617</td>\n",
              "      <td>0.007369</td>\n",
              "      <td>0.040316</td>\n",
              "      <td>-0.336832</td>\n",
              "      <td>-0.228535</td>\n",
              "      <td>0.551412</td>\n",
              "      <td>0.390050</td>\n",
              "      <td>0.113819</td>\n",
              "      <td>0.086606</td>\n",
              "      <td>-0.511803</td>\n",
              "      <td>-0.536867</td>\n",
              "      <td>-0.036259</td>\n",
              "      <td>-0.135167</td>\n",
              "      <td>0.293361</td>\n",
              "      <td>0.187640</td>\n",
              "      <td>0.339917</td>\n",
              "      <td>-0.011865</td>\n",
              "      <td>-0.235137</td>\n",
              "      <td>-0.013774</td>\n",
              "      <td>-0.388511</td>\n",
              "      <td>0.216360</td>\n",
              "      <td>0.091991</td>\n",
              "      <td>-0.059994</td>\n",
              "      <td>-0.355788</td>\n",
              "      <td>0.399938</td>\n",
              "      <td>0.156817</td>\n",
              "      <td>0.219716</td>\n",
              "      <td>-0.477571</td>\n",
              "      <td>-0.198949</td>\n",
              "      <td>-0.100966</td>\n",
              "      <td>0.293873</td>\n",
              "      <td>-0.079380</td>\n",
              "      <td>0.207862</td>\n",
              "      <td>-0.466658</td>\n",
              "      <td>0.251690</td>\n",
              "      <td>-0.148807</td>\n",
              "      <td>-0.335689</td>\n",
              "      <td>-0.358697</td>\n",
              "      <td>-0.215853</td>\n",
              "      <td>-0.360466</td>\n",
              "      <td>0.483892</td>\n",
              "      <td>-0.149062</td>\n",
              "      <td>0.129870</td>\n",
              "      <td>0.217657</td>\n",
              "      <td>-0.609400</td>\n",
              "      <td>-0.017193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>-0.088815</td>\n",
              "      <td>0.022185</td>\n",
              "      <td>0.198475</td>\n",
              "      <td>0.141373</td>\n",
              "      <td>-0.069439</td>\n",
              "      <td>0.441111</td>\n",
              "      <td>-0.174259</td>\n",
              "      <td>0.148802</td>\n",
              "      <td>0.027075</td>\n",
              "      <td>0.080487</td>\n",
              "      <td>-0.263487</td>\n",
              "      <td>0.144089</td>\n",
              "      <td>0.532065</td>\n",
              "      <td>0.487996</td>\n",
              "      <td>-0.040672</td>\n",
              "      <td>-0.807102</td>\n",
              "      <td>0.191869</td>\n",
              "      <td>0.333450</td>\n",
              "      <td>0.038851</td>\n",
              "      <td>-0.298559</td>\n",
              "      <td>0.406521</td>\n",
              "      <td>0.024400</td>\n",
              "      <td>-0.236066</td>\n",
              "      <td>0.563577</td>\n",
              "      <td>-0.145722</td>\n",
              "      <td>0.039838</td>\n",
              "      <td>0.349672</td>\n",
              "      <td>0.326088</td>\n",
              "      <td>-0.157120</td>\n",
              "      <td>0.071829</td>\n",
              "      <td>-0.182436</td>\n",
              "      <td>0.193131</td>\n",
              "      <td>-0.121393</td>\n",
              "      <td>0.063746</td>\n",
              "      <td>0.110731</td>\n",
              "      <td>-0.198281</td>\n",
              "      <td>0.110654</td>\n",
              "      <td>0.051254</td>\n",
              "      <td>-0.250695</td>\n",
              "      <td>-0.237532</td>\n",
              "      <td>-0.163908</td>\n",
              "      <td>-0.082923</td>\n",
              "      <td>-0.165127</td>\n",
              "      <td>-0.193229</td>\n",
              "      <td>-0.194419</td>\n",
              "      <td>-0.209940</td>\n",
              "      <td>0.460403</td>\n",
              "      <td>-0.111722</td>\n",
              "      <td>0.271668</td>\n",
              "      <td>0.035706</td>\n",
              "      <td>-0.363050</td>\n",
              "      <td>0.101376</td>\n",
              "      <td>-0.214439</td>\n",
              "      <td>0.392653</td>\n",
              "      <td>-0.471439</td>\n",
              "      <td>-0.441111</td>\n",
              "      <td>-0.024606</td>\n",
              "      <td>-0.358498</td>\n",
              "      <td>0.207629</td>\n",
              "      <td>0.101739</td>\n",
              "      <td>-0.004727</td>\n",
              "      <td>0.053674</td>\n",
              "      <td>-0.282951</td>\n",
              "      <td>0.118688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>863</th>\n",
              "      <td>0.513575</td>\n",
              "      <td>0.173657</td>\n",
              "      <td>-0.092450</td>\n",
              "      <td>0.306902</td>\n",
              "      <td>-0.251064</td>\n",
              "      <td>0.415721</td>\n",
              "      <td>-0.240512</td>\n",
              "      <td>0.092843</td>\n",
              "      <td>0.273895</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>-0.036605</td>\n",
              "      <td>0.468723</td>\n",
              "      <td>0.118615</td>\n",
              "      <td>0.460603</td>\n",
              "      <td>0.263123</td>\n",
              "      <td>-0.408796</td>\n",
              "      <td>-0.068153</td>\n",
              "      <td>-0.058013</td>\n",
              "      <td>0.220054</td>\n",
              "      <td>-0.384735</td>\n",
              "      <td>0.270615</td>\n",
              "      <td>0.413665</td>\n",
              "      <td>-0.223892</td>\n",
              "      <td>0.287283</td>\n",
              "      <td>-0.178338</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.162769</td>\n",
              "      <td>-0.055739</td>\n",
              "      <td>-0.493531</td>\n",
              "      <td>-0.111475</td>\n",
              "      <td>-0.426543</td>\n",
              "      <td>0.344583</td>\n",
              "      <td>0.004420</td>\n",
              "      <td>0.205909</td>\n",
              "      <td>-0.105806</td>\n",
              "      <td>-0.268764</td>\n",
              "      <td>-0.166235</td>\n",
              "      <td>-0.135018</td>\n",
              "      <td>-0.152727</td>\n",
              "      <td>-0.196303</td>\n",
              "      <td>-0.345665</td>\n",
              "      <td>0.076228</td>\n",
              "      <td>-0.126674</td>\n",
              "      <td>-0.052930</td>\n",
              "      <td>-0.366802</td>\n",
              "      <td>0.201344</td>\n",
              "      <td>-0.105965</td>\n",
              "      <td>-0.156266</td>\n",
              "      <td>0.210755</td>\n",
              "      <td>0.016453</td>\n",
              "      <td>-0.386576</td>\n",
              "      <td>0.096127</td>\n",
              "      <td>-0.144775</td>\n",
              "      <td>0.380052</td>\n",
              "      <td>-0.141334</td>\n",
              "      <td>-0.298545</td>\n",
              "      <td>0.135280</td>\n",
              "      <td>-0.215668</td>\n",
              "      <td>0.052903</td>\n",
              "      <td>0.135094</td>\n",
              "      <td>0.156829</td>\n",
              "      <td>-0.257563</td>\n",
              "      <td>-0.364858</td>\n",
              "      <td>0.593225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>864</th>\n",
              "      <td>0.055499</td>\n",
              "      <td>0.271936</td>\n",
              "      <td>0.131731</td>\n",
              "      <td>0.363282</td>\n",
              "      <td>-0.139166</td>\n",
              "      <td>0.345999</td>\n",
              "      <td>0.013006</td>\n",
              "      <td>0.208592</td>\n",
              "      <td>0.269805</td>\n",
              "      <td>0.300604</td>\n",
              "      <td>-0.118364</td>\n",
              "      <td>0.076399</td>\n",
              "      <td>0.239183</td>\n",
              "      <td>0.359718</td>\n",
              "      <td>0.324052</td>\n",
              "      <td>-0.769147</td>\n",
              "      <td>-0.046822</td>\n",
              "      <td>0.388044</td>\n",
              "      <td>0.081373</td>\n",
              "      <td>-0.411007</td>\n",
              "      <td>0.258937</td>\n",
              "      <td>0.215962</td>\n",
              "      <td>-0.345710</td>\n",
              "      <td>0.334608</td>\n",
              "      <td>-0.062599</td>\n",
              "      <td>-0.169237</td>\n",
              "      <td>0.167745</td>\n",
              "      <td>0.147336</td>\n",
              "      <td>-0.120497</td>\n",
              "      <td>-0.297221</td>\n",
              "      <td>-0.001375</td>\n",
              "      <td>0.239487</td>\n",
              "      <td>0.049928</td>\n",
              "      <td>-0.009399</td>\n",
              "      <td>-0.028516</td>\n",
              "      <td>-0.004482</td>\n",
              "      <td>-0.221651</td>\n",
              "      <td>-0.195385</td>\n",
              "      <td>-0.146860</td>\n",
              "      <td>0.086918</td>\n",
              "      <td>-0.403117</td>\n",
              "      <td>-0.037853</td>\n",
              "      <td>0.211849</td>\n",
              "      <td>-0.000915</td>\n",
              "      <td>-0.140229</td>\n",
              "      <td>0.373146</td>\n",
              "      <td>0.294121</td>\n",
              "      <td>0.051133</td>\n",
              "      <td>0.340852</td>\n",
              "      <td>-0.061040</td>\n",
              "      <td>-0.238875</td>\n",
              "      <td>0.133588</td>\n",
              "      <td>-0.483274</td>\n",
              "      <td>-0.324963</td>\n",
              "      <td>-0.396426</td>\n",
              "      <td>-0.369805</td>\n",
              "      <td>0.470491</td>\n",
              "      <td>0.052288</td>\n",
              "      <td>0.117031</td>\n",
              "      <td>-0.045250</td>\n",
              "      <td>0.064913</td>\n",
              "      <td>0.140784</td>\n",
              "      <td>-0.182053</td>\n",
              "      <td>0.030330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>-0.181779</td>\n",
              "      <td>-0.363329</td>\n",
              "      <td>-0.423186</td>\n",
              "      <td>-0.157311</td>\n",
              "      <td>-0.214929</td>\n",
              "      <td>-0.252519</td>\n",
              "      <td>-0.350931</td>\n",
              "      <td>-0.232254</td>\n",
              "      <td>0.113516</td>\n",
              "      <td>0.063742</td>\n",
              "      <td>0.147492</td>\n",
              "      <td>0.302928</td>\n",
              "      <td>0.106877</td>\n",
              "      <td>-0.290624</td>\n",
              "      <td>-0.308740</td>\n",
              "      <td>-0.658004</td>\n",
              "      <td>0.115639</td>\n",
              "      <td>0.123080</td>\n",
              "      <td>-0.482925</td>\n",
              "      <td>-0.228882</td>\n",
              "      <td>-0.143022</td>\n",
              "      <td>-0.462407</td>\n",
              "      <td>-0.165071</td>\n",
              "      <td>0.396990</td>\n",
              "      <td>-0.030545</td>\n",
              "      <td>-0.468110</td>\n",
              "      <td>0.305775</td>\n",
              "      <td>-0.441464</td>\n",
              "      <td>0.111079</td>\n",
              "      <td>-0.002531</td>\n",
              "      <td>-0.086447</td>\n",
              "      <td>-0.117921</td>\n",
              "      <td>-0.121266</td>\n",
              "      <td>0.223787</td>\n",
              "      <td>-0.199725</td>\n",
              "      <td>-0.525048</td>\n",
              "      <td>-0.292833</td>\n",
              "      <td>-0.339180</td>\n",
              "      <td>-0.188680</td>\n",
              "      <td>-0.263997</td>\n",
              "      <td>-0.108436</td>\n",
              "      <td>0.241165</td>\n",
              "      <td>-0.453429</td>\n",
              "      <td>0.491994</td>\n",
              "      <td>-0.004904</td>\n",
              "      <td>0.101063</td>\n",
              "      <td>-0.032559</td>\n",
              "      <td>-0.132686</td>\n",
              "      <td>0.217015</td>\n",
              "      <td>-0.062151</td>\n",
              "      <td>-0.217022</td>\n",
              "      <td>-0.108325</td>\n",
              "      <td>-0.031540</td>\n",
              "      <td>-0.382796</td>\n",
              "      <td>-0.031889</td>\n",
              "      <td>-0.468149</td>\n",
              "      <td>-0.439473</td>\n",
              "      <td>0.124676</td>\n",
              "      <td>-0.411184</td>\n",
              "      <td>-0.011866</td>\n",
              "      <td>0.608151</td>\n",
              "      <td>0.461791</td>\n",
              "      <td>-0.013446</td>\n",
              "      <td>-0.093222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>0.122435</td>\n",
              "      <td>0.074935</td>\n",
              "      <td>-0.793541</td>\n",
              "      <td>0.163272</td>\n",
              "      <td>-0.021725</td>\n",
              "      <td>0.252507</td>\n",
              "      <td>0.452652</td>\n",
              "      <td>-0.000354</td>\n",
              "      <td>-0.300496</td>\n",
              "      <td>0.412642</td>\n",
              "      <td>-0.115198</td>\n",
              "      <td>0.086146</td>\n",
              "      <td>-0.025486</td>\n",
              "      <td>0.330015</td>\n",
              "      <td>-0.075621</td>\n",
              "      <td>0.208101</td>\n",
              "      <td>-0.191685</td>\n",
              "      <td>0.185371</td>\n",
              "      <td>-0.234954</td>\n",
              "      <td>-0.141375</td>\n",
              "      <td>0.122878</td>\n",
              "      <td>0.133157</td>\n",
              "      <td>-0.030439</td>\n",
              "      <td>-0.267787</td>\n",
              "      <td>-0.304097</td>\n",
              "      <td>-0.318081</td>\n",
              "      <td>0.105039</td>\n",
              "      <td>0.551188</td>\n",
              "      <td>0.265297</td>\n",
              "      <td>-0.204543</td>\n",
              "      <td>0.143062</td>\n",
              "      <td>-0.134667</td>\n",
              "      <td>-0.046789</td>\n",
              "      <td>-0.032005</td>\n",
              "      <td>0.109807</td>\n",
              "      <td>-0.436771</td>\n",
              "      <td>0.196143</td>\n",
              "      <td>0.069991</td>\n",
              "      <td>-0.119025</td>\n",
              "      <td>0.396273</td>\n",
              "      <td>0.108259</td>\n",
              "      <td>0.553124</td>\n",
              "      <td>-0.321606</td>\n",
              "      <td>0.382469</td>\n",
              "      <td>-0.669921</td>\n",
              "      <td>0.491477</td>\n",
              "      <td>-0.265221</td>\n",
              "      <td>0.091368</td>\n",
              "      <td>-0.201212</td>\n",
              "      <td>0.080597</td>\n",
              "      <td>-0.238494</td>\n",
              "      <td>0.502003</td>\n",
              "      <td>-0.472973</td>\n",
              "      <td>-0.713821</td>\n",
              "      <td>-0.584855</td>\n",
              "      <td>-0.182139</td>\n",
              "      <td>-0.311003</td>\n",
              "      <td>0.167933</td>\n",
              "      <td>-0.174559</td>\n",
              "      <td>0.214934</td>\n",
              "      <td>0.695605</td>\n",
              "      <td>-0.578652</td>\n",
              "      <td>-0.022588</td>\n",
              "      <td>-0.067676</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>867 rows × 64 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           1         2         3   ...        62        63        64\n",
              "0    0.039781 -0.335233 -0.248732  ... -0.330601  0.119765 -0.260214\n",
              "1   -0.075479 -0.294005 -0.046772  ...  0.112080 -0.335977 -0.044735\n",
              "2   -0.157106 -0.175712  0.023253  ...  0.010972  0.234457  0.067845\n",
              "3    0.013436 -0.148017  0.352701  ... -0.573249 -0.459089  0.024804\n",
              "4   -0.371445  0.159327  0.455429  ...  0.217657 -0.609400 -0.017193\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "862 -0.088815  0.022185  0.198475  ...  0.053674 -0.282951  0.118688\n",
              "863  0.513575  0.173657 -0.092450  ... -0.257563 -0.364858  0.593225\n",
              "864  0.055499  0.271936  0.131731  ...  0.140784 -0.182053  0.030330\n",
              "865 -0.181779 -0.363329 -0.423186  ...  0.461791 -0.013446 -0.093222\n",
              "866  0.122435  0.074935 -0.793541  ... -0.578652 -0.022588 -0.067676\n",
              "\n",
              "[867 rows x 64 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEWAiqxqREdu"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution(\n",
        "    config=None,\n",
        "    device_policy=None,\n",
        "    execution_mode=None\n",
        ")\n",
        "def dot(x, y, sparse=False):\n",
        "    x = tf.cast(x, dtype=tf.float32)\n",
        "    y = tf.cast(y, dtype=tf.float32)\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "      res = tf.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "      res = tf.matmul(x, y)\n",
        "    return res\n",
        "mf_vector = dot(Emdebding_train.detach().numpy(),tf.transpose(np.array(Embedding_Node2vec)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "164LkBUqSEkF",
        "outputId": "a77f981b-9248-4360-ec83-ba45c7aa1d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#mf_vector = tf.reshape(mf_vector,[-1,1])\n",
        "mf_vector.numpy()[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(837,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PS2zqyeUgjd"
      },
      "source": [
        "def get_train_instances(train, num_negatives):\n",
        "  global user_input,item_input\n",
        "  user_input, item_input, labels = [],[],[]\n",
        "  num_users, num_items = train.shape\n",
        "  for (u, i) in train.keys():\n",
        "    # positive instance\n",
        "    user_input.append(u)\n",
        "    item_input.append(i)\n",
        "    labels.append(1)\n",
        "    # negative instances\n",
        "    for t in range(num_negatives):\n",
        "      j = np.random.randint(num_users,num_items)\n",
        "      while (u, j) in train.keys():\n",
        "        j = np.random.randint(num_users,num_items)\n",
        "      user_input.append(u)\n",
        "      item_input.append(j)\n",
        "      labels.append(0)\n",
        "  # 遍历生成NeuMF需要的drug的vecter\n",
        "  drug_latent_vector, disease_latent_vector = [], []\n",
        "  for i in user_input:\n",
        "    drug_latent_vector.append(np.hstack([Emdebding_train[i].detach().numpy(),np.array(Embedding_Node2vec)[i]]))\n",
        "    # drug_latent_vector.append(mf_vector[i].numpy())\n",
        "  for j in item_input:\n",
        "    disease_latent_vector.append(np.hstack([Emdebding_train[j].detach().numpy(),np.array(Embedding_Node2vec)[j]]))\n",
        "    # disease_latent_vector.append(mf_vector[j].numpy())\n",
        "  return drug_latent_vector, disease_latent_vector, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-rMY9mf-JeU"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder,self).__init__()\n",
        "        self.encoder  =  nn.Sequential( \n",
        "            nn.Linear(1, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 3)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3,16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded,decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLp9fDiMYvsm",
        "outputId": "5233a0eb-b677-4527-e544-8e5b23b3ea2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.hstack((np.array(X_train1), np.array(X_train2)))[1].reshape((1,256)).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li0PcUZX-L3N",
        "outputId": "7c8433b0-67f3-46c5-ed5c-fff279115d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data as Data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import time\n",
        "starttime = time.time()\n",
        "\n",
        "torch.manual_seed(1)   #为了使用同样的随机初始化种子以形成相同的随机效果\n",
        "\n",
        "EPOCH = 10\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.005\n",
        "N_TEST_IMG = 5\n",
        "Coder = AutoEncoder()\n",
        "\n",
        "optimizer = torch.optim.Adam(Coder.parameters(),lr=LR)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "mf_vector = []\n",
        "\n",
        "#for epoch in range(EPOCH):\n",
        "for i in range(837):\n",
        "    encoded, decoded = Coder(torch.tensor(np.hstack((np.array(X_train1), np.array(X_train2)))[i].reshape((256,1)), dtype=torch.float32))\n",
        "    loss = loss_func(decoded,torch.tensor(np.hstack((np.array(X_train1), np.array(X_train2)))[i].reshape((256,1)), dtype=torch.float32))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    mf_vector.append(decoded)  \n",
        "#torch.save(Coder,'AutoEncoder.pkl')\n",
        "print('________________________________________')\n",
        "print('finish training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________\n",
            "finish training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB8U92ntLPa0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "drug_latent_vector, disease_latent_vector, labels = get_train_instances(train_Martrix, 1)\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(drug_latent_vector, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(disease_latent_vector, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#model1 = RandomForestClassifier(n_estimators=600, learning_rate=0.7)\n",
        "\n",
        "rfc_l = []\n",
        "for i in range(300，700):\n",
        "    model1 = RandomForestClassifier(n_estimators=i+21)\n",
        "    rfc_s = cross_val_score(model1,np.hstack((np.array(X_train1), np.array(X_train2))),np.array(y_train1),cv=5).mean()\n",
        "    rfc_l.append(rfc_s)\n",
        "print(max(rfc_l),rfc_l.index(max(rfc_l))+1)   \n",
        "plt.plot(range(21,700),rfc_l,label='随机森林')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# model1.fit(np.hstack((np.array(X_train1), np.array(X_train2))), np.array(y_train1))\n",
        "# y_score0 = model1.predict(np.hstack([np.array(X_test1), np.array(X_test2)]))\n",
        "y_score1 = model1.predict_proba(np.hstack([np.array(X_test1), np.array(X_test2)]))\n",
        "\n",
        "# 计算AUC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import time\n",
        "\n",
        "#y_score1 = model.predict([np.array(X_test1), np.array(X_test2)])\n",
        "\n",
        "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
        "#Y_train0为真实标签，Y_pred_0为预测标签，注意，这里roc_curve为一维的输入，Y_train0是一维的\n",
        "fpr, tpr, thresholds_keras = roc_curve(y_test1, y_score1[:,1])   \n",
        "auc = auc(fpr, tpr)\n",
        "print(\"AUC : \", auc)\n",
        "plt.figure()\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/'+ now + 'ROC.jpg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn2_FuSZEPtK",
        "outputId": "c003f8e6-ce85-4c0f-9aba-e9010f3dc774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = model1.score(np.hstack([np.array(X_test1), np.array(X_test2)]), np.array(y_test1))\n",
        "score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7994066417839027"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxIcAMZjqagG",
        "outputId": "5d404a88-fc97-4ac3-a7cb-04525a3040d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_test1, y_score0, labels=[1,0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1871, 1612],\n",
              "       [ 484, 6482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnOPrFh3jnRh"
      },
      "source": [
        "def get_train_instances(train, num_negatives):\n",
        "  # global user_input,item_input\n",
        "  user_input, item_input, labels = [],[],[]\n",
        "  num_users, num_items = train.shape\n",
        "  for (u, i) in train.keys():\n",
        "    # positive instance\n",
        "    user_input.append(u)\n",
        "    item_input.append(i)\n",
        "    labels.append(1)\n",
        "   \n",
        "  # 遍历生成NeuMF需要的drug的vecter\n",
        "  drug_latent_vector, disease_latent_vector = [], []\n",
        "  for i in user_input:\n",
        "    drug_latent_vector.append(Emdebding_train[i].detach().numpy())\n",
        "  for j in item_input:\n",
        "    disease_latent_vector.append(Emdebding_train[j].detach().numpy())\n",
        "  print(len(labels))\n",
        "  # negative instances\n",
        "  counter, x, y= 0, 0, 0\n",
        "  while counter < len(negative_data):\n",
        "    x = negative_data.iloc[x,0]\n",
        "    drug_latent_vector.append(Emdebding_train[x].detach().numpy())\n",
        "    y = negative_data.iloc[y,1]\n",
        "    disease_latent_vector.append(Emdebding_train[y].detach().numpy())\n",
        "    labels.append(0)\n",
        "    y +=1\n",
        "    x +=1\n",
        "    counter +=1\n",
        "  print(len(labels))  \n",
        "  return drug_latent_vector, disease_latent_vector, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64WXnC1BNX5u",
        "outputId": "738b72f8-54c2-4684-ed3d-ae25f0ce08b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        " from keras.layers.merge import multiply, concatenate\n",
        " import numpy as np\n",
        " drug_Embedded = [[1,2,3],[4,5,6],[4,5,6]]\n",
        " disease_Embedded = [[1,2,3],[4,5,6],[4,5,6]]\n",
        " mf_vector = concatenate([np.array(drug_Embedded), np.array(disease_Embedded)], axis = 0 )\n",
        " mf_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 3), dtype=int64, numpy=\n",
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [4, 5, 6],\n",
              "       [1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [4, 5, 6]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrRO6ir99RxT"
      },
      "source": [
        "# # Pre-GMF\n",
        "# from keras.layers.merge import multiply, concatenate\n",
        "# from keras.layers import Input, Dense\n",
        "# def GMF_get_model(num_users, num_items, latent_dim, regs=[0,0]):\n",
        "#   # Input variables\n",
        "#   drug_Embedded = Input(shape=(16,))\n",
        "#   disease_Embedded = Input(shape=(16,))\n",
        "    \n",
        "#   # Element-wise product of user and item embeddings \n",
        "#   predict_vector = multiply([drug_Embedded, disease_Embedded])\n",
        "    \n",
        "#   # Final prediction layer\n",
        "#   # prediction = Lambda(lambda x: K.sigmoid(K.sum(x)), output_shape=(1,))(predict_vector)\n",
        "#   prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = 'prediction')(predict_vector)\n",
        "    \n",
        "#   model = Model(input=[drug_Embedded, disease_Embedded], \n",
        "#                 output=prediction)\n",
        "#   return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-kJPd5ZK_Ou"
      },
      "source": [
        "from keras.layers.merge import multiply, concatenate\n",
        "from keras.layers import Input, Dense\n",
        "def NeuMF_getmodel(layers=[10], reg_layers=[0], reg_mf=0):\n",
        "  assert len(layers) == len(reg_layers)\n",
        "  num_layer = len(layers) #Number of layers in the MLP\n",
        "  # Input variables\n",
        "  drug_Embedded = Input(shape=(16,))\n",
        "  disease_Embedded = Input(shape=(16,))\n",
        "  \n",
        "  mf_vector = multiply([drug_Embedded, disease_Embedded])\n",
        "  # mf_vector = merge([drug_Embedded, disease_Embedded], mode='mul') # element-wise multiply\n",
        "\n",
        "  # MLP part \n",
        "  mlp_vector = concatenate([drug_Embedded, disease_Embedded])\n",
        "  # mlp_vector = merge([drug_Embedded, disease_Embedded], mode='concat')\n",
        "  for idx in range(1, num_layer):\n",
        "    layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
        "    mlp_vector = layer(mlp_vector)\n",
        "\n",
        "  # Concatenate MF and MLP parts\n",
        "  predict_vector = concatenate([mf_vector, mlp_vector])\n",
        "  # predict_vector = merge([mf_vector, mlp_vector], mode='concat')  \n",
        "  # Final prediction layer\n",
        "  prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector) # sigmoid\n",
        "    \n",
        "  model = Model(input=[drug_Embedded, disease_Embedded],output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbsRnpQ2PYJ",
        "outputId": "173f0ba7-2656-4572-bc45-bd7e68123888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "num_negatives = 1 # 4  1\n",
        "global num_scale \n",
        "num_scale = 27862 # 69650 27862\n",
        "batch_size = 256\n",
        "\n",
        "model = NeuMF_getmodel()\n",
        "model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "# load pima indians dataset\n",
        "drug_latent_vector, disease_latent_vector, labels = get_train_instances(train_Martrix, num_negatives)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(drug_latent_vector, labels, test_size=0.2, random_state=24)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(disease_latent_vector, labels, test_size=0.2, random_state=24)\n",
        "train_loss = []\n",
        "# Training model\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # Generate training instances\n",
        "  # Training\n",
        "  # hist = model.fit([np.array(drug_latent_vector[:num_scale]), np.array(disease_latent_vector[:num_scale])], #input \n",
        "  #                  np.array(labels[:num_scale]), # labels \n",
        "  #                  batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "  hist = model.fit([np.array(X_train1), np.array(X_train2)],  \n",
        "                   np.array(y_train1), \n",
        "                   batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "  train_loss.append(hist.history['loss'][0])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "        'loss_train: {:.4f}'.format(hist.history['loss'][0]),\n",
        "        'acc_train: {:.4f}'.format(hist.history['acc'][0]))\n",
        "  \n",
        "test_scores = model.evaluate([np.array(X_test1), np.array(X_test2)], y_test1)\n",
        "print(\"Test set results:\",\n",
        "      \"loss= {:.4f}\".format(test_scores[0]),\n",
        "      'acc_train: {:.4f}'.format(test_scores[1]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "epochs = len(train_loss)\n",
        "plt.plot(range(0,epochs,1), train_loss, label='train_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "#plt.savefig(\"/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/\"+time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))+\"Unet-过拟合C0.jpg\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"prediction\", kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 0.7234 acc_train: 0.4972\n",
            "Epoch: 0002 loss_train: 0.6985 acc_train: 0.4999\n",
            "Epoch: 0003 loss_train: 0.6944 acc_train: 0.5075\n",
            "Epoch: 0004 loss_train: 0.6927 acc_train: 0.5234\n",
            "Epoch: 0005 loss_train: 0.6915 acc_train: 0.5300\n",
            "Epoch: 0006 loss_train: 0.6908 acc_train: 0.5318\n",
            "Epoch: 0007 loss_train: 0.6904 acc_train: 0.5280\n",
            "Epoch: 0008 loss_train: 0.6899 acc_train: 0.5384\n",
            "Epoch: 0009 loss_train: 0.6896 acc_train: 0.5420\n",
            "Epoch: 0010 loss_train: 0.6894 acc_train: 0.5373\n",
            "Epoch: 0011 loss_train: 0.6891 acc_train: 0.5353\n",
            "Epoch: 0012 loss_train: 0.6891 acc_train: 0.5378\n",
            "Epoch: 0013 loss_train: 0.6888 acc_train: 0.5391\n",
            "Epoch: 0014 loss_train: 0.6887 acc_train: 0.5420\n",
            "Epoch: 0015 loss_train: 0.6884 acc_train: 0.5396\n",
            "Epoch: 0016 loss_train: 0.6883 acc_train: 0.5453\n",
            "Epoch: 0017 loss_train: 0.6881 acc_train: 0.5446\n",
            "Epoch: 0018 loss_train: 0.6882 acc_train: 0.5433\n",
            "Epoch: 0019 loss_train: 0.6879 acc_train: 0.5442\n",
            "Epoch: 0020 loss_train: 0.6880 acc_train: 0.5447\n",
            "Epoch: 0021 loss_train: 0.6877 acc_train: 0.5461\n",
            "Epoch: 0022 loss_train: 0.6878 acc_train: 0.5453\n",
            "Epoch: 0023 loss_train: 0.6876 acc_train: 0.5424\n",
            "Epoch: 0024 loss_train: 0.6875 acc_train: 0.5498\n",
            "Epoch: 0025 loss_train: 0.6873 acc_train: 0.5467\n",
            "Epoch: 0026 loss_train: 0.6873 acc_train: 0.5491\n",
            "Epoch: 0027 loss_train: 0.6871 acc_train: 0.5490\n",
            "Epoch: 0028 loss_train: 0.6872 acc_train: 0.5501\n",
            "Epoch: 0029 loss_train: 0.6870 acc_train: 0.5469\n",
            "Epoch: 0030 loss_train: 0.6869 acc_train: 0.5497\n",
            "Epoch: 0031 loss_train: 0.6869 acc_train: 0.5511\n",
            "Epoch: 0032 loss_train: 0.6869 acc_train: 0.5505\n",
            "Epoch: 0033 loss_train: 0.6868 acc_train: 0.5524\n",
            "Epoch: 0034 loss_train: 0.6866 acc_train: 0.5513\n",
            "Epoch: 0035 loss_train: 0.6868 acc_train: 0.5505\n",
            "Epoch: 0036 loss_train: 0.6867 acc_train: 0.5520\n",
            "Epoch: 0037 loss_train: 0.6868 acc_train: 0.5518\n",
            "Epoch: 0038 loss_train: 0.6866 acc_train: 0.5512\n",
            "Epoch: 0039 loss_train: 0.6866 acc_train: 0.5551\n",
            "Epoch: 0040 loss_train: 0.6865 acc_train: 0.5490\n",
            "Epoch: 0041 loss_train: 0.6864 acc_train: 0.5536\n",
            "Epoch: 0042 loss_train: 0.6865 acc_train: 0.5517\n",
            "Epoch: 0043 loss_train: 0.6864 acc_train: 0.5507\n",
            "Epoch: 0044 loss_train: 0.6863 acc_train: 0.5513\n",
            "Epoch: 0045 loss_train: 0.6863 acc_train: 0.5518\n",
            "Epoch: 0046 loss_train: 0.6863 acc_train: 0.5538\n",
            "Epoch: 0047 loss_train: 0.6863 acc_train: 0.5517\n",
            "Epoch: 0048 loss_train: 0.6862 acc_train: 0.5522\n",
            "Epoch: 0049 loss_train: 0.6863 acc_train: 0.5543\n",
            "Epoch: 0050 loss_train: 0.6861 acc_train: 0.5525\n",
            "Epoch: 0051 loss_train: 0.6863 acc_train: 0.5563\n",
            "Epoch: 0052 loss_train: 0.6862 acc_train: 0.5537\n",
            "Epoch: 0053 loss_train: 0.6862 acc_train: 0.5519\n",
            "Epoch: 0054 loss_train: 0.6863 acc_train: 0.5512\n",
            "Epoch: 0055 loss_train: 0.6860 acc_train: 0.5535\n",
            "Epoch: 0056 loss_train: 0.6860 acc_train: 0.5531\n",
            "Epoch: 0057 loss_train: 0.6860 acc_train: 0.5533\n",
            "Epoch: 0058 loss_train: 0.6860 acc_train: 0.5518\n",
            "Epoch: 0059 loss_train: 0.6858 acc_train: 0.5518\n",
            "Epoch: 0060 loss_train: 0.6859 acc_train: 0.5530\n",
            "Epoch: 0061 loss_train: 0.6859 acc_train: 0.5524\n",
            "Epoch: 0062 loss_train: 0.6859 acc_train: 0.5525\n",
            "Epoch: 0063 loss_train: 0.6859 acc_train: 0.5524\n",
            "Epoch: 0064 loss_train: 0.6859 acc_train: 0.5536\n",
            "Epoch: 0065 loss_train: 0.6858 acc_train: 0.5520\n",
            "Epoch: 0066 loss_train: 0.6859 acc_train: 0.5526\n",
            "Epoch: 0067 loss_train: 0.6858 acc_train: 0.5536\n",
            "Epoch: 0068 loss_train: 0.6857 acc_train: 0.5532\n",
            "Epoch: 0069 loss_train: 0.6857 acc_train: 0.5546\n",
            "Epoch: 0070 loss_train: 0.6857 acc_train: 0.5522\n",
            "Epoch: 0071 loss_train: 0.6858 acc_train: 0.5524\n",
            "Epoch: 0072 loss_train: 0.6857 acc_train: 0.5527\n",
            "Epoch: 0073 loss_train: 0.6858 acc_train: 0.5525\n",
            "Epoch: 0074 loss_train: 0.6857 acc_train: 0.5534\n",
            "Epoch: 0075 loss_train: 0.6856 acc_train: 0.5533\n",
            "Epoch: 0076 loss_train: 0.6857 acc_train: 0.5525\n",
            "Epoch: 0077 loss_train: 0.6856 acc_train: 0.5530\n",
            "Epoch: 0078 loss_train: 0.6857 acc_train: 0.5544\n",
            "Epoch: 0079 loss_train: 0.6856 acc_train: 0.5538\n",
            "Epoch: 0080 loss_train: 0.6856 acc_train: 0.5547\n",
            "Epoch: 0081 loss_train: 0.6856 acc_train: 0.5528\n",
            "Epoch: 0082 loss_train: 0.6855 acc_train: 0.5548\n",
            "Epoch: 0083 loss_train: 0.6855 acc_train: 0.5537\n",
            "Epoch: 0084 loss_train: 0.6855 acc_train: 0.5543\n",
            "Epoch: 0085 loss_train: 0.6856 acc_train: 0.5543\n",
            "Epoch: 0086 loss_train: 0.6856 acc_train: 0.5530\n",
            "Epoch: 0087 loss_train: 0.6856 acc_train: 0.5517\n",
            "Epoch: 0088 loss_train: 0.6856 acc_train: 0.5539\n",
            "Epoch: 0089 loss_train: 0.6855 acc_train: 0.5538\n",
            "Epoch: 0090 loss_train: 0.6856 acc_train: 0.5543\n",
            "Epoch: 0091 loss_train: 0.6855 acc_train: 0.5537\n",
            "Epoch: 0092 loss_train: 0.6856 acc_train: 0.5534\n",
            "Epoch: 0093 loss_train: 0.6855 acc_train: 0.5538\n",
            "Epoch: 0094 loss_train: 0.6854 acc_train: 0.5542\n",
            "Epoch: 0095 loss_train: 0.6854 acc_train: 0.5538\n",
            "Epoch: 0096 loss_train: 0.6854 acc_train: 0.5523\n",
            "Epoch: 0097 loss_train: 0.6856 acc_train: 0.5543\n",
            "Epoch: 0098 loss_train: 0.6853 acc_train: 0.5539\n",
            "Epoch: 0099 loss_train: 0.6853 acc_train: 0.5538\n",
            "Epoch: 0100 loss_train: 0.6855 acc_train: 0.5546\n",
            "Epoch: 0101 loss_train: 0.6854 acc_train: 0.5555\n",
            "Epoch: 0102 loss_train: 0.6855 acc_train: 0.5560\n",
            "Epoch: 0103 loss_train: 0.6853 acc_train: 0.5538\n",
            "Epoch: 0104 loss_train: 0.6854 acc_train: 0.5543\n",
            "Epoch: 0105 loss_train: 0.6854 acc_train: 0.5541\n",
            "Epoch: 0106 loss_train: 0.6854 acc_train: 0.5535\n",
            "Epoch: 0107 loss_train: 0.6853 acc_train: 0.5535\n",
            "Epoch: 0108 loss_train: 0.6853 acc_train: 0.5539\n",
            "Epoch: 0109 loss_train: 0.6853 acc_train: 0.5548\n",
            "Epoch: 0110 loss_train: 0.6853 acc_train: 0.5551\n",
            "Epoch: 0111 loss_train: 0.6852 acc_train: 0.5543\n",
            "Epoch: 0112 loss_train: 0.6854 acc_train: 0.5557\n",
            "Epoch: 0113 loss_train: 0.6857 acc_train: 0.5542\n",
            "Epoch: 0114 loss_train: 0.6854 acc_train: 0.5547\n",
            "Epoch: 0115 loss_train: 0.6852 acc_train: 0.5544\n",
            "Epoch: 0116 loss_train: 0.6854 acc_train: 0.5537\n",
            "Epoch: 0117 loss_train: 0.6854 acc_train: 0.5553\n",
            "Epoch: 0118 loss_train: 0.6855 acc_train: 0.5549\n",
            "Epoch: 0119 loss_train: 0.6854 acc_train: 0.5546\n",
            "Epoch: 0120 loss_train: 0.6851 acc_train: 0.5549\n",
            "Epoch: 0121 loss_train: 0.6852 acc_train: 0.5556\n",
            "Epoch: 0122 loss_train: 0.6852 acc_train: 0.5530\n",
            "Epoch: 0123 loss_train: 0.6852 acc_train: 0.5553\n",
            "Epoch: 0124 loss_train: 0.6852 acc_train: 0.5548\n",
            "Epoch: 0125 loss_train: 0.6851 acc_train: 0.5552\n",
            "Epoch: 0126 loss_train: 0.6853 acc_train: 0.5548\n",
            "Epoch: 0127 loss_train: 0.6851 acc_train: 0.5552\n",
            "Epoch: 0128 loss_train: 0.6852 acc_train: 0.5551\n",
            "Epoch: 0129 loss_train: 0.6851 acc_train: 0.5544\n",
            "Epoch: 0130 loss_train: 0.6853 acc_train: 0.5561\n",
            "Epoch: 0131 loss_train: 0.6853 acc_train: 0.5553\n",
            "Epoch: 0132 loss_train: 0.6851 acc_train: 0.5551\n",
            "Epoch: 0133 loss_train: 0.6852 acc_train: 0.5560\n",
            "Epoch: 0134 loss_train: 0.6852 acc_train: 0.5567\n",
            "Epoch: 0135 loss_train: 0.6852 acc_train: 0.5559\n",
            "Epoch: 0136 loss_train: 0.6852 acc_train: 0.5552\n",
            "Epoch: 0137 loss_train: 0.6853 acc_train: 0.5547\n",
            "Epoch: 0138 loss_train: 0.6852 acc_train: 0.5562\n",
            "Epoch: 0139 loss_train: 0.6853 acc_train: 0.5543\n",
            "Epoch: 0140 loss_train: 0.6852 acc_train: 0.5563\n",
            "Epoch: 0141 loss_train: 0.6851 acc_train: 0.5548\n",
            "Epoch: 0142 loss_train: 0.6852 acc_train: 0.5568\n",
            "Epoch: 0143 loss_train: 0.6851 acc_train: 0.5551\n",
            "Epoch: 0144 loss_train: 0.6851 acc_train: 0.5560\n",
            "Epoch: 0145 loss_train: 0.6851 acc_train: 0.5557\n",
            "Epoch: 0146 loss_train: 0.6852 acc_train: 0.5562\n",
            "Epoch: 0147 loss_train: 0.6851 acc_train: 0.5557\n",
            "Epoch: 0148 loss_train: 0.6852 acc_train: 0.5555\n",
            "Epoch: 0149 loss_train: 0.6853 acc_train: 0.5559\n",
            "Epoch: 0150 loss_train: 0.6850 acc_train: 0.5561\n",
            "Epoch: 0151 loss_train: 0.6850 acc_train: 0.5560\n",
            "Epoch: 0152 loss_train: 0.6851 acc_train: 0.5552\n",
            "Epoch: 0153 loss_train: 0.6852 acc_train: 0.5565\n",
            "Epoch: 0154 loss_train: 0.6851 acc_train: 0.5543\n",
            "Epoch: 0155 loss_train: 0.6851 acc_train: 0.5556\n",
            "Epoch: 0156 loss_train: 0.6851 acc_train: 0.5559\n",
            "Epoch: 0157 loss_train: 0.6850 acc_train: 0.5565\n",
            "Epoch: 0158 loss_train: 0.6850 acc_train: 0.5557\n",
            "Epoch: 0159 loss_train: 0.6851 acc_train: 0.5559\n",
            "Epoch: 0160 loss_train: 0.6850 acc_train: 0.5565\n",
            "Epoch: 0161 loss_train: 0.6851 acc_train: 0.5563\n",
            "Epoch: 0162 loss_train: 0.6850 acc_train: 0.5553\n",
            "Epoch: 0163 loss_train: 0.6851 acc_train: 0.5545\n",
            "Epoch: 0164 loss_train: 0.6850 acc_train: 0.5575\n",
            "Epoch: 0165 loss_train: 0.6851 acc_train: 0.5562\n",
            "Epoch: 0166 loss_train: 0.6851 acc_train: 0.5550\n",
            "Epoch: 0167 loss_train: 0.6850 acc_train: 0.5563\n",
            "Epoch: 0168 loss_train: 0.6851 acc_train: 0.5571\n",
            "Epoch: 0169 loss_train: 0.6850 acc_train: 0.5562\n",
            "Epoch: 0170 loss_train: 0.6850 acc_train: 0.5548\n",
            "Epoch: 0171 loss_train: 0.6850 acc_train: 0.5562\n",
            "Epoch: 0172 loss_train: 0.6852 acc_train: 0.5553\n",
            "Epoch: 0173 loss_train: 0.6850 acc_train: 0.5570\n",
            "Epoch: 0174 loss_train: 0.6850 acc_train: 0.5573\n",
            "Epoch: 0175 loss_train: 0.6851 acc_train: 0.5552\n",
            "Epoch: 0176 loss_train: 0.6850 acc_train: 0.5563\n",
            "Epoch: 0177 loss_train: 0.6852 acc_train: 0.5556\n",
            "Epoch: 0178 loss_train: 0.6850 acc_train: 0.5563\n",
            "Epoch: 0179 loss_train: 0.6849 acc_train: 0.5564\n",
            "Epoch: 0180 loss_train: 0.6850 acc_train: 0.5574\n",
            "Epoch: 0181 loss_train: 0.6849 acc_train: 0.5568\n",
            "Epoch: 0182 loss_train: 0.6849 acc_train: 0.5579\n",
            "Epoch: 0183 loss_train: 0.6850 acc_train: 0.5556\n",
            "Epoch: 0184 loss_train: 0.6851 acc_train: 0.5567\n",
            "Epoch: 0185 loss_train: 0.6850 acc_train: 0.5563\n",
            "Epoch: 0186 loss_train: 0.6850 acc_train: 0.5567\n",
            "Epoch: 0187 loss_train: 0.6849 acc_train: 0.5568\n",
            "Epoch: 0188 loss_train: 0.6850 acc_train: 0.5547\n",
            "Epoch: 0189 loss_train: 0.6850 acc_train: 0.5570\n",
            "Epoch: 0190 loss_train: 0.6849 acc_train: 0.5555\n",
            "Epoch: 0191 loss_train: 0.6850 acc_train: 0.5567\n",
            "Epoch: 0192 loss_train: 0.6849 acc_train: 0.5565\n",
            "Epoch: 0193 loss_train: 0.6848 acc_train: 0.5565\n",
            "Epoch: 0194 loss_train: 0.6851 acc_train: 0.5560\n",
            "Epoch: 0195 loss_train: 0.6849 acc_train: 0.5565\n",
            "Epoch: 0196 loss_train: 0.6849 acc_train: 0.5561\n",
            "Epoch: 0197 loss_train: 0.6850 acc_train: 0.5572\n",
            "Epoch: 0198 loss_train: 0.6848 acc_train: 0.5568\n",
            "Epoch: 0199 loss_train: 0.6850 acc_train: 0.5570\n",
            "Epoch: 0200 loss_train: 0.6849 acc_train: 0.5554\n",
            "6966/6966 [==============================] - 0s 25us/step\n",
            "Test set results: loss= 0.6847 acc_train: 0.5566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSy0U9aCEMHu",
        "outputId": "23979f15-6bf5-43bc-e850-5da0b0fbf5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test1[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 1, 0, 1, 1, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnmffMraGK_m",
        "outputId": "54c70284-14ec-4c12-d096-533df4efca8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test2[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 1, 0, 1, 1, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj_VaK90EPOl",
        "outputId": "fae24868-e21f-49c7-b8b3-0223f60a9059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "y_score1[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46111554],\n",
              "       [0.2501036 ],\n",
              "       [0.46137208],\n",
              "       [0.376176  ],\n",
              "       [0.46424112],\n",
              "       [0.60141325],\n",
              "       [0.5289528 ],\n",
              "       [0.48491535],\n",
              "       [0.5067387 ],\n",
              "       [0.519097  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxOuiXtkCtO",
        "outputId": "728e4e84-b681-415f-b2aa-1e563af20800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "# 计算AUC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import time\n",
        "\n",
        "#y_score1 = model.predict([np.array(X_test1), np.array(X_test2)])\n",
        "\n",
        "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
        "#Y_train0为真实标签，Y_pred_0为预测标签，注意，这里roc_curve为一维的输入，Y_train0是一维的\n",
        "fpr, tpr, thresholds_keras = roc_curve(y_test1, y_score0)   \n",
        "auc = auc(fpr, tpr)\n",
        "print(\"AUC : \", auc)\n",
        "plt.figure()\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "#plt.savefig('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/'+ now + 'ROC.jpg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC :  0.7387751030914025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uxl8iFFwVDM"
      },
      "source": [
        "# 归一化数据\n",
        "from sklearn import preprocessing  \n",
        "min_max_scaler = preprocessing.MinMaxScaler()  #标准化训练集数据 \n",
        "data_train_nomal = min_max_scaler.fit_transform(y_score1)  #对测试集数据进行相同的归一化处理\n",
        "data_train_nomal[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz-JEXmi8tE1"
      },
      "source": [
        "print('max:', data_train_nomal.max())\n",
        "print('min:', data_train_nomal.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXfQJR1oM6HI"
      },
      "source": [
        "def auc_calculate(labels,preds,n_bins=17420):\n",
        "    postive_len = sum(labels)   #正样本数量（因为正样本都是1）\n",
        "    negative_len = len(labels) - postive_len #负样本数量\n",
        "    print('负样本数量:', negative_len)\n",
        "    total_case = postive_len * negative_len #正负样本对\n",
        "    pos_histogram = [0 for _ in range(n_bins)] \n",
        "    neg_histogram = [0 for _ in range(n_bins)]\n",
        "    bin_width = 1.0 / n_bins\n",
        "    for i in range(len(labels)):\n",
        "        nth_bin = int(preds[i]/bin_width)\n",
        "        if labels[i]==1:\n",
        "            pos_histogram[nth_bin] += 1\n",
        "        else:\n",
        "            neg_histogram[nth_bin] += 1\n",
        "    accumulated_neg = 0\n",
        "    satisfied_pair = 0\n",
        "    for i in range(n_bins):\n",
        "        satisfied_pair += (pos_histogram[i]*accumulated_neg + pos_histogram[i]*neg_histogram[i]*0.5)\n",
        "        accumulated_neg += neg_histogram[i]\n",
        "    return satisfied_pair / float(total_case)\n",
        "print(\"验证:\",auc_calculate(labels[27862:], y_score1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R95RmynP0LsH"
      },
      "source": [
        "from sklearn import metrics\n",
        "'''使用real.csv和result.csv列数据，计算PR曲线的AUC值'''\n",
        "precision, recall, _thresholds = metrics.precision_recall_curve(labels[69650:], data_train_nomal)\n",
        "area = metrics.auc(recall, precision)\n",
        "print(area) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}