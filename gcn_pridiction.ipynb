{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_pridiction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XkvkckV6Lc5VVRFDhXiPCTrNYzg7vQ0W",
      "authorship_tag": "ABX9TyN1TMIDQI9lv7mlX2SI+65X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevejobws/Colaboratory/blob/master/gcn_pridiction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA0EeTTq6Gi-",
        "outputId": "7b6b76fb-836d-49a7-d7e7-f67c25b106ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install torchvision\n",
        "!pip install torch_sparse -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_scatter -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_geometric # 下载安装pytorch_geometric\n",
        "!pip install networkx # 画图\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric \n",
        "from torch_geometric.nn import GCNConv, ChebConv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.6/dist-packages (0.6.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch_sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch_sparse) (1.18.5)\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.6/dist-packages (2.0.5)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.6/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.48.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.11.2)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (3.20.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.18.5)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (5.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.6.0+cu101)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch_geometric) (49.6.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch_geometric) (0.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch_geometric) (1.1.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch_geometric) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch_geometric) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch_geometric) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch_geometric) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch_geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch_geometric) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch_geometric) (0.6.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch_geometric) (2.4.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch_geometric) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch_geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch_geometric) (1.2.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-70ab89b42959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChebConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataListLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[1;32m      9\u001b[0m                                    contains_self_loops, is_undirected)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             f'matches your PyTorch install.')\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseStorage\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_sparse/storage.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegment_csr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_scatter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt_minor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;34mf'Detected that PyTorch and torch_scatter were compiled with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34mf'different CUDA versions. PyTorch has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34mf'{t_major}.{t_minor} and torch_scatter has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torch_scatter were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_scatter has CUDA version 0.0. Please reinstall the torch_scatter that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5TCDF488n7",
        "outputId": "0cc9741b-cf79-4dbc-9854-660008920593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "! python -c \"import torch_geometric; print(torch_geometric.__version__)\" # 检查是否安装成功"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/__init__.py\", line 2, in <module>\n",
            "    import torch_geometric.nn\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/__init__.py\", line 2, in <module>\n",
            "    from .data_parallel import DataParallel\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/data_parallel.py\", line 5, in <module>\n",
            "    from torch_geometric.data import Batch\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/data/__init__.py\", line 1, in <module>\n",
            "    from .data import Data\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\", line 7, in <module>\n",
            "    from torch_sparse import coalesce, SparseTensor\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_sparse/__init__.py\", line 34, in <module>\n",
            "    from .storage import SparseStorage  # noqa\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_sparse/storage.py\", line 5, in <module>\n",
            "    from torch_scatter import segment_csr, scatter_add\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_scatter/__init__.py\", line 55, in <module>\n",
            "    f'Detected that PyTorch and torch_scatter were compiled with '\n",
            "RuntimeError: Detected that PyTorch and torch_scatter were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_scatter has CUDA version 0.0. Please reinstall the torch_scatter that matches your PyTorch install.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57sAyKd7EgIc",
        "outputId": "3faba9a1-84bd-4ff2-95d8-d5cd39cb8b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "! uname -a  # 查看系统  \n",
        "! python --version  # 查看python版本 \n",
        "! python -c 'import torch; print(torch.version.cuda)' # 查看cuda的版本，检查是否和cuda的一致\n",
        "! nvcc --version # 查看nvcc版本 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux e4e606ebf6e4 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Python 3.6.9\n",
            "10.1\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki24HI_7E2mL",
        "outputId": "fa4e9f0d-0cea-4594-b605-ce32e4a136e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(torch.version.cuda) # torch的cuda版本"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLtfimHc64d"
      },
      "source": [
        "# 1. 预处理数据集的格式，转化为GCN所需要的格式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqo1Ho-djcrH"
      },
      "source": [
        "### 1.1 import file is  attribute of node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScqvErTlc5iO",
        "outputId": "c75d45fc-e966-4a7b-b487-74def5f00ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "node_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNodeAttribute.csv',header = None) \n",
        "num = node_features.shape[0] # Number of nodes\n",
        "node_features  \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.770633</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.582718</td>\n",
              "      <td>-0.062620</td>\n",
              "      <td>-0.238564</td>\n",
              "      <td>-0.715823</td>\n",
              "      <td>1.022981</td>\n",
              "      <td>0.181646</td>\n",
              "      <td>-0.260391</td>\n",
              "      <td>-0.542033</td>\n",
              "      <td>0.233139</td>\n",
              "      <td>-0.283432</td>\n",
              "      <td>0.117144</td>\n",
              "      <td>0.531449</td>\n",
              "      <td>-0.061221</td>\n",
              "      <td>-0.229529</td>\n",
              "      <td>-0.710597</td>\n",
              "      <td>0.288485</td>\n",
              "      <td>0.098411</td>\n",
              "      <td>0.189655</td>\n",
              "      <td>-0.511962</td>\n",
              "      <td>-0.263284</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.198719</td>\n",
              "      <td>1.086848</td>\n",
              "      <td>-0.256804</td>\n",
              "      <td>0.026246</td>\n",
              "      <td>0.093899</td>\n",
              "      <td>-0.142497</td>\n",
              "      <td>0.018819</td>\n",
              "      <td>-0.234937</td>\n",
              "      <td>-0.153423</td>\n",
              "      <td>0.425772</td>\n",
              "      <td>0.673969</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>-0.294841</td>\n",
              "      <td>-0.315873</td>\n",
              "      <td>0.552618</td>\n",
              "      <td>0.437355</td>\n",
              "      <td>-0.087442</td>\n",
              "      <td>0.536133</td>\n",
              "      <td>-0.366451</td>\n",
              "      <td>0.656990</td>\n",
              "      <td>0.802005</td>\n",
              "      <td>-0.016102</td>\n",
              "      <td>-0.669421</td>\n",
              "      <td>0.856563</td>\n",
              "      <td>-0.101192</td>\n",
              "      <td>-0.165583</td>\n",
              "      <td>-0.405513</td>\n",
              "      <td>-0.410948</td>\n",
              "      <td>-0.027975</td>\n",
              "      <td>-0.397765</td>\n",
              "      <td>-0.509387</td>\n",
              "      <td>0.280567</td>\n",
              "      <td>0.208776</td>\n",
              "      <td>-0.099218</td>\n",
              "      <td>-0.229354</td>\n",
              "      <td>0.232348</td>\n",
              "      <td>-1.035032</td>\n",
              "      <td>1.209129</td>\n",
              "      <td>0.368444</td>\n",
              "      <td>0.414352</td>\n",
              "      <td>-1.065435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.882159</td>\n",
              "      <td>1.241338</td>\n",
              "      <td>2.232549</td>\n",
              "      <td>-1.932836</td>\n",
              "      <td>-0.574318</td>\n",
              "      <td>0.168684</td>\n",
              "      <td>2.937954</td>\n",
              "      <td>1.398610</td>\n",
              "      <td>-0.260359</td>\n",
              "      <td>-1.253366</td>\n",
              "      <td>-0.997117</td>\n",
              "      <td>-1.355796</td>\n",
              "      <td>0.847303</td>\n",
              "      <td>2.447680</td>\n",
              "      <td>-0.573743</td>\n",
              "      <td>0.211870</td>\n",
              "      <td>-0.960837</td>\n",
              "      <td>1.536916</td>\n",
              "      <td>2.928266</td>\n",
              "      <td>-0.955619</td>\n",
              "      <td>-1.152885</td>\n",
              "      <td>0.548847</td>\n",
              "      <td>0.525967</td>\n",
              "      <td>-0.445511</td>\n",
              "      <td>2.999283</td>\n",
              "      <td>0.190644</td>\n",
              "      <td>-0.916012</td>\n",
              "      <td>0.877579</td>\n",
              "      <td>1.501392</td>\n",
              "      <td>-2.191907</td>\n",
              "      <td>-0.344929</td>\n",
              "      <td>-0.643345</td>\n",
              "      <td>1.808293</td>\n",
              "      <td>0.922850</td>\n",
              "      <td>0.823318</td>\n",
              "      <td>0.399970</td>\n",
              "      <td>0.415868</td>\n",
              "      <td>0.005898</td>\n",
              "      <td>-2.257816</td>\n",
              "      <td>-0.475472</td>\n",
              "      <td>0.610879</td>\n",
              "      <td>-1.345582</td>\n",
              "      <td>1.256365</td>\n",
              "      <td>0.962199</td>\n",
              "      <td>-1.093309</td>\n",
              "      <td>-1.647974</td>\n",
              "      <td>1.594285</td>\n",
              "      <td>0.584256</td>\n",
              "      <td>-1.293162</td>\n",
              "      <td>-1.844852</td>\n",
              "      <td>-0.580833</td>\n",
              "      <td>2.215265</td>\n",
              "      <td>-1.795820</td>\n",
              "      <td>-2.811482</td>\n",
              "      <td>0.229828</td>\n",
              "      <td>0.073673</td>\n",
              "      <td>0.256524</td>\n",
              "      <td>-0.438187</td>\n",
              "      <td>-0.097229</td>\n",
              "      <td>-0.740352</td>\n",
              "      <td>1.529813</td>\n",
              "      <td>-0.640376</td>\n",
              "      <td>1.717616</td>\n",
              "      <td>-1.703426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.688133</td>\n",
              "      <td>-0.318813</td>\n",
              "      <td>0.999343</td>\n",
              "      <td>-0.651598</td>\n",
              "      <td>-0.407260</td>\n",
              "      <td>-0.314173</td>\n",
              "      <td>0.781863</td>\n",
              "      <td>0.242623</td>\n",
              "      <td>-0.107326</td>\n",
              "      <td>-0.972608</td>\n",
              "      <td>-0.235344</td>\n",
              "      <td>-0.511505</td>\n",
              "      <td>-0.138459</td>\n",
              "      <td>0.775548</td>\n",
              "      <td>-0.218838</td>\n",
              "      <td>0.545011</td>\n",
              "      <td>-0.670150</td>\n",
              "      <td>0.699975</td>\n",
              "      <td>0.817735</td>\n",
              "      <td>-0.134486</td>\n",
              "      <td>0.054736</td>\n",
              "      <td>0.374199</td>\n",
              "      <td>0.183914</td>\n",
              "      <td>-0.104466</td>\n",
              "      <td>1.410521</td>\n",
              "      <td>-0.258784</td>\n",
              "      <td>0.201976</td>\n",
              "      <td>0.270087</td>\n",
              "      <td>0.458823</td>\n",
              "      <td>-0.055942</td>\n",
              "      <td>0.035193</td>\n",
              "      <td>0.027436</td>\n",
              "      <td>1.150917</td>\n",
              "      <td>0.318230</td>\n",
              "      <td>0.150467</td>\n",
              "      <td>0.657710</td>\n",
              "      <td>-0.184495</td>\n",
              "      <td>-0.571191</td>\n",
              "      <td>-0.604887</td>\n",
              "      <td>-0.231566</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>-0.379828</td>\n",
              "      <td>0.181649</td>\n",
              "      <td>0.533105</td>\n",
              "      <td>-0.220025</td>\n",
              "      <td>-0.499370</td>\n",
              "      <td>0.676739</td>\n",
              "      <td>-0.635464</td>\n",
              "      <td>-0.520307</td>\n",
              "      <td>-0.607030</td>\n",
              "      <td>0.455186</td>\n",
              "      <td>0.421789</td>\n",
              "      <td>-0.637401</td>\n",
              "      <td>-1.185999</td>\n",
              "      <td>-0.661417</td>\n",
              "      <td>0.244160</td>\n",
              "      <td>0.266237</td>\n",
              "      <td>-0.213120</td>\n",
              "      <td>0.249394</td>\n",
              "      <td>-0.382235</td>\n",
              "      <td>0.886608</td>\n",
              "      <td>-0.040592</td>\n",
              "      <td>0.387883</td>\n",
              "      <td>-0.961931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.721908</td>\n",
              "      <td>0.388303</td>\n",
              "      <td>0.947409</td>\n",
              "      <td>-0.787672</td>\n",
              "      <td>-0.362458</td>\n",
              "      <td>-0.536777</td>\n",
              "      <td>1.290188</td>\n",
              "      <td>0.130461</td>\n",
              "      <td>-0.550028</td>\n",
              "      <td>-0.882603</td>\n",
              "      <td>-0.284483</td>\n",
              "      <td>0.195723</td>\n",
              "      <td>0.287096</td>\n",
              "      <td>0.741486</td>\n",
              "      <td>-0.613376</td>\n",
              "      <td>0.241035</td>\n",
              "      <td>-1.183460</td>\n",
              "      <td>0.651605</td>\n",
              "      <td>1.076140</td>\n",
              "      <td>0.284106</td>\n",
              "      <td>-1.281615</td>\n",
              "      <td>-0.727382</td>\n",
              "      <td>0.383454</td>\n",
              "      <td>-0.202577</td>\n",
              "      <td>1.619946</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>-0.379740</td>\n",
              "      <td>0.251197</td>\n",
              "      <td>0.567685</td>\n",
              "      <td>-0.998128</td>\n",
              "      <td>-0.046036</td>\n",
              "      <td>-0.882844</td>\n",
              "      <td>0.740021</td>\n",
              "      <td>0.870464</td>\n",
              "      <td>0.363524</td>\n",
              "      <td>-0.268316</td>\n",
              "      <td>0.146103</td>\n",
              "      <td>0.422400</td>\n",
              "      <td>0.236998</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>0.789005</td>\n",
              "      <td>-1.078728</td>\n",
              "      <td>0.767913</td>\n",
              "      <td>1.031454</td>\n",
              "      <td>-1.020215</td>\n",
              "      <td>-0.265524</td>\n",
              "      <td>1.290493</td>\n",
              "      <td>-0.687332</td>\n",
              "      <td>-0.580685</td>\n",
              "      <td>-0.275520</td>\n",
              "      <td>0.351817</td>\n",
              "      <td>0.330816</td>\n",
              "      <td>-0.828862</td>\n",
              "      <td>-1.068953</td>\n",
              "      <td>-0.534331</td>\n",
              "      <td>-0.844828</td>\n",
              "      <td>0.119608</td>\n",
              "      <td>0.585099</td>\n",
              "      <td>0.437665</td>\n",
              "      <td>-1.466863</td>\n",
              "      <td>1.006176</td>\n",
              "      <td>-0.079589</td>\n",
              "      <td>0.593564</td>\n",
              "      <td>-1.152426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.650453</td>\n",
              "      <td>-0.146959</td>\n",
              "      <td>0.802256</td>\n",
              "      <td>-0.280101</td>\n",
              "      <td>-0.731831</td>\n",
              "      <td>-0.456279</td>\n",
              "      <td>0.775432</td>\n",
              "      <td>-0.465780</td>\n",
              "      <td>0.143515</td>\n",
              "      <td>-0.736436</td>\n",
              "      <td>0.108304</td>\n",
              "      <td>-0.252066</td>\n",
              "      <td>0.123692</td>\n",
              "      <td>0.359086</td>\n",
              "      <td>-0.440906</td>\n",
              "      <td>-0.286168</td>\n",
              "      <td>-0.400844</td>\n",
              "      <td>0.719755</td>\n",
              "      <td>0.256401</td>\n",
              "      <td>0.432786</td>\n",
              "      <td>-0.412044</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.045897</td>\n",
              "      <td>-0.033026</td>\n",
              "      <td>0.904498</td>\n",
              "      <td>0.185858</td>\n",
              "      <td>-0.173960</td>\n",
              "      <td>-0.415105</td>\n",
              "      <td>-0.231409</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.299792</td>\n",
              "      <td>-0.222521</td>\n",
              "      <td>0.539466</td>\n",
              "      <td>0.284991</td>\n",
              "      <td>0.444050</td>\n",
              "      <td>0.151308</td>\n",
              "      <td>-0.550617</td>\n",
              "      <td>0.110464</td>\n",
              "      <td>-0.113436</td>\n",
              "      <td>-0.128214</td>\n",
              "      <td>-0.150840</td>\n",
              "      <td>-0.427944</td>\n",
              "      <td>0.444110</td>\n",
              "      <td>0.421779</td>\n",
              "      <td>-0.314363</td>\n",
              "      <td>-0.818235</td>\n",
              "      <td>0.472876</td>\n",
              "      <td>-0.264762</td>\n",
              "      <td>0.096127</td>\n",
              "      <td>-0.077573</td>\n",
              "      <td>0.012599</td>\n",
              "      <td>-0.271649</td>\n",
              "      <td>-0.296263</td>\n",
              "      <td>-0.772448</td>\n",
              "      <td>-0.508725</td>\n",
              "      <td>-0.160995</td>\n",
              "      <td>0.444162</td>\n",
              "      <td>0.061997</td>\n",
              "      <td>-0.419543</td>\n",
              "      <td>-0.450393</td>\n",
              "      <td>0.573340</td>\n",
              "      <td>-0.265548</td>\n",
              "      <td>0.279305</td>\n",
              "      <td>-0.208944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>832</td>\n",
              "      <td>0.092474</td>\n",
              "      <td>0.106150</td>\n",
              "      <td>0.033633</td>\n",
              "      <td>0.018442</td>\n",
              "      <td>0.365770</td>\n",
              "      <td>0.026157</td>\n",
              "      <td>0.192186</td>\n",
              "      <td>-0.005236</td>\n",
              "      <td>-0.217018</td>\n",
              "      <td>0.257617</td>\n",
              "      <td>0.053886</td>\n",
              "      <td>0.152891</td>\n",
              "      <td>-0.122927</td>\n",
              "      <td>0.130594</td>\n",
              "      <td>-0.034565</td>\n",
              "      <td>-0.123680</td>\n",
              "      <td>-0.024969</td>\n",
              "      <td>0.082651</td>\n",
              "      <td>-0.083302</td>\n",
              "      <td>-0.207902</td>\n",
              "      <td>-0.276132</td>\n",
              "      <td>0.104665</td>\n",
              "      <td>-0.138261</td>\n",
              "      <td>0.175597</td>\n",
              "      <td>0.038199</td>\n",
              "      <td>-0.013455</td>\n",
              "      <td>-0.133683</td>\n",
              "      <td>-0.142527</td>\n",
              "      <td>-0.042970</td>\n",
              "      <td>-0.077926</td>\n",
              "      <td>-0.069572</td>\n",
              "      <td>0.217869</td>\n",
              "      <td>-0.284175</td>\n",
              "      <td>0.278465</td>\n",
              "      <td>-0.007701</td>\n",
              "      <td>0.073461</td>\n",
              "      <td>-0.324276</td>\n",
              "      <td>-0.116313</td>\n",
              "      <td>0.068643</td>\n",
              "      <td>-0.304055</td>\n",
              "      <td>0.147916</td>\n",
              "      <td>-0.450253</td>\n",
              "      <td>0.243640</td>\n",
              "      <td>0.015325</td>\n",
              "      <td>0.139896</td>\n",
              "      <td>0.207937</td>\n",
              "      <td>-0.348907</td>\n",
              "      <td>-0.123072</td>\n",
              "      <td>0.070664</td>\n",
              "      <td>0.271669</td>\n",
              "      <td>0.138426</td>\n",
              "      <td>0.055811</td>\n",
              "      <td>0.010685</td>\n",
              "      <td>0.009590</td>\n",
              "      <td>-0.026956</td>\n",
              "      <td>0.143835</td>\n",
              "      <td>0.134296</td>\n",
              "      <td>-0.018494</td>\n",
              "      <td>-0.094349</td>\n",
              "      <td>0.005062</td>\n",
              "      <td>0.147475</td>\n",
              "      <td>0.069725</td>\n",
              "      <td>0.208504</td>\n",
              "      <td>-0.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>833</td>\n",
              "      <td>-0.072233</td>\n",
              "      <td>0.119163</td>\n",
              "      <td>0.096811</td>\n",
              "      <td>-0.164411</td>\n",
              "      <td>0.278347</td>\n",
              "      <td>-0.201982</td>\n",
              "      <td>0.081316</td>\n",
              "      <td>-0.061770</td>\n",
              "      <td>-0.086994</td>\n",
              "      <td>0.235601</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.266871</td>\n",
              "      <td>-0.139913</td>\n",
              "      <td>-0.100954</td>\n",
              "      <td>-0.204267</td>\n",
              "      <td>-0.009871</td>\n",
              "      <td>-0.132975</td>\n",
              "      <td>-0.115135</td>\n",
              "      <td>0.074661</td>\n",
              "      <td>-0.066648</td>\n",
              "      <td>-0.195009</td>\n",
              "      <td>0.198070</td>\n",
              "      <td>0.041986</td>\n",
              "      <td>0.103717</td>\n",
              "      <td>0.212957</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.222006</td>\n",
              "      <td>0.069433</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.136644</td>\n",
              "      <td>-0.192403</td>\n",
              "      <td>-0.032700</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>-0.030231</td>\n",
              "      <td>0.076892</td>\n",
              "      <td>-0.572047</td>\n",
              "      <td>0.032351</td>\n",
              "      <td>0.019072</td>\n",
              "      <td>-0.175355</td>\n",
              "      <td>0.103921</td>\n",
              "      <td>-0.086620</td>\n",
              "      <td>-0.005460</td>\n",
              "      <td>0.071132</td>\n",
              "      <td>0.409461</td>\n",
              "      <td>0.037381</td>\n",
              "      <td>-0.081257</td>\n",
              "      <td>0.180506</td>\n",
              "      <td>-0.170608</td>\n",
              "      <td>0.123282</td>\n",
              "      <td>0.220902</td>\n",
              "      <td>-0.175002</td>\n",
              "      <td>-0.025833</td>\n",
              "      <td>0.079203</td>\n",
              "      <td>-0.021000</td>\n",
              "      <td>-0.098711</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>0.020249</td>\n",
              "      <td>0.162249</td>\n",
              "      <td>0.026010</td>\n",
              "      <td>0.033548</td>\n",
              "      <td>0.223271</td>\n",
              "      <td>-0.203688</td>\n",
              "      <td>-0.038956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>834</td>\n",
              "      <td>-0.132909</td>\n",
              "      <td>0.104212</td>\n",
              "      <td>0.050959</td>\n",
              "      <td>-0.111834</td>\n",
              "      <td>0.357196</td>\n",
              "      <td>-0.133188</td>\n",
              "      <td>0.143785</td>\n",
              "      <td>-0.049507</td>\n",
              "      <td>-0.147305</td>\n",
              "      <td>0.251885</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.199905</td>\n",
              "      <td>-0.070194</td>\n",
              "      <td>-0.153948</td>\n",
              "      <td>-0.143667</td>\n",
              "      <td>-0.036536</td>\n",
              "      <td>-0.038589</td>\n",
              "      <td>-0.034700</td>\n",
              "      <td>0.060599</td>\n",
              "      <td>-0.025647</td>\n",
              "      <td>-0.243201</td>\n",
              "      <td>0.148838</td>\n",
              "      <td>-0.037812</td>\n",
              "      <td>0.111142</td>\n",
              "      <td>0.232710</td>\n",
              "      <td>-0.081121</td>\n",
              "      <td>-0.017971</td>\n",
              "      <td>0.097984</td>\n",
              "      <td>-0.012490</td>\n",
              "      <td>-0.099915</td>\n",
              "      <td>0.092442</td>\n",
              "      <td>-0.163616</td>\n",
              "      <td>-0.015880</td>\n",
              "      <td>0.066135</td>\n",
              "      <td>-0.025328</td>\n",
              "      <td>0.081988</td>\n",
              "      <td>-0.507710</td>\n",
              "      <td>0.107737</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>-0.200301</td>\n",
              "      <td>0.093443</td>\n",
              "      <td>-0.091668</td>\n",
              "      <td>0.068706</td>\n",
              "      <td>-0.022480</td>\n",
              "      <td>0.297613</td>\n",
              "      <td>0.091121</td>\n",
              "      <td>-0.158395</td>\n",
              "      <td>0.178886</td>\n",
              "      <td>-0.117903</td>\n",
              "      <td>0.128856</td>\n",
              "      <td>0.285126</td>\n",
              "      <td>-0.158185</td>\n",
              "      <td>-0.052871</td>\n",
              "      <td>0.033910</td>\n",
              "      <td>-0.026669</td>\n",
              "      <td>-0.054522</td>\n",
              "      <td>0.224450</td>\n",
              "      <td>0.091497</td>\n",
              "      <td>0.057863</td>\n",
              "      <td>0.031415</td>\n",
              "      <td>-0.005544</td>\n",
              "      <td>0.270046</td>\n",
              "      <td>-0.138641</td>\n",
              "      <td>-0.049895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>835</td>\n",
              "      <td>-0.079618</td>\n",
              "      <td>-0.154153</td>\n",
              "      <td>0.295142</td>\n",
              "      <td>-0.150358</td>\n",
              "      <td>0.500962</td>\n",
              "      <td>-0.328006</td>\n",
              "      <td>-0.015192</td>\n",
              "      <td>-0.219420</td>\n",
              "      <td>-0.377795</td>\n",
              "      <td>0.257173</td>\n",
              "      <td>-0.064142</td>\n",
              "      <td>0.492528</td>\n",
              "      <td>-0.080609</td>\n",
              "      <td>-0.044967</td>\n",
              "      <td>-0.154126</td>\n",
              "      <td>-0.143692</td>\n",
              "      <td>0.100103</td>\n",
              "      <td>0.163071</td>\n",
              "      <td>-0.012129</td>\n",
              "      <td>-0.236396</td>\n",
              "      <td>-0.380849</td>\n",
              "      <td>0.415483</td>\n",
              "      <td>0.029281</td>\n",
              "      <td>0.239447</td>\n",
              "      <td>0.465136</td>\n",
              "      <td>-0.272850</td>\n",
              "      <td>0.155511</td>\n",
              "      <td>0.075649</td>\n",
              "      <td>0.088600</td>\n",
              "      <td>-0.196622</td>\n",
              "      <td>0.121215</td>\n",
              "      <td>-0.192279</td>\n",
              "      <td>0.005945</td>\n",
              "      <td>0.065828</td>\n",
              "      <td>-0.015154</td>\n",
              "      <td>-0.131507</td>\n",
              "      <td>-0.721625</td>\n",
              "      <td>0.131845</td>\n",
              "      <td>-0.092926</td>\n",
              "      <td>-0.340395</td>\n",
              "      <td>0.505998</td>\n",
              "      <td>-0.165514</td>\n",
              "      <td>0.192104</td>\n",
              "      <td>-0.060656</td>\n",
              "      <td>0.373459</td>\n",
              "      <td>0.290300</td>\n",
              "      <td>-0.260470</td>\n",
              "      <td>0.298324</td>\n",
              "      <td>-0.158419</td>\n",
              "      <td>0.281835</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>0.106287</td>\n",
              "      <td>0.018379</td>\n",
              "      <td>-0.148565</td>\n",
              "      <td>-0.095972</td>\n",
              "      <td>0.352697</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.086388</td>\n",
              "      <td>0.010307</td>\n",
              "      <td>0.194430</td>\n",
              "      <td>0.168985</td>\n",
              "      <td>-0.259337</td>\n",
              "      <td>-0.073086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>836</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.008422</td>\n",
              "      <td>0.717436</td>\n",
              "      <td>0.134245</td>\n",
              "      <td>0.373170</td>\n",
              "      <td>-0.059227</td>\n",
              "      <td>0.102996</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>-0.102515</td>\n",
              "      <td>0.433158</td>\n",
              "      <td>0.158489</td>\n",
              "      <td>0.265962</td>\n",
              "      <td>-0.011416</td>\n",
              "      <td>-0.031199</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>-0.496083</td>\n",
              "      <td>-0.352736</td>\n",
              "      <td>-0.257647</td>\n",
              "      <td>0.161620</td>\n",
              "      <td>-0.230452</td>\n",
              "      <td>-0.116733</td>\n",
              "      <td>0.159655</td>\n",
              "      <td>-0.278142</td>\n",
              "      <td>0.125518</td>\n",
              "      <td>0.496362</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>-0.318031</td>\n",
              "      <td>-0.112670</td>\n",
              "      <td>-0.150040</td>\n",
              "      <td>-0.268167</td>\n",
              "      <td>-0.436891</td>\n",
              "      <td>0.335615</td>\n",
              "      <td>-0.349596</td>\n",
              "      <td>0.179373</td>\n",
              "      <td>-0.294503</td>\n",
              "      <td>0.210135</td>\n",
              "      <td>-0.615341</td>\n",
              "      <td>-0.007536</td>\n",
              "      <td>0.045706</td>\n",
              "      <td>-0.705685</td>\n",
              "      <td>-0.061241</td>\n",
              "      <td>-0.744448</td>\n",
              "      <td>0.617036</td>\n",
              "      <td>-0.066599</td>\n",
              "      <td>0.155325</td>\n",
              "      <td>0.487323</td>\n",
              "      <td>-0.912633</td>\n",
              "      <td>0.122922</td>\n",
              "      <td>0.049172</td>\n",
              "      <td>0.452203</td>\n",
              "      <td>-0.023311</td>\n",
              "      <td>-0.192136</td>\n",
              "      <td>0.286585</td>\n",
              "      <td>0.194225</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.318570</td>\n",
              "      <td>-0.017260</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>-0.195706</td>\n",
              "      <td>-0.051707</td>\n",
              "      <td>0.267640</td>\n",
              "      <td>-0.421967</td>\n",
              "      <td>0.063392</td>\n",
              "      <td>0.015846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>837 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0         1         2         3   ...        61        62        63        64\n",
              "0      0  0.770633  0.010733  0.582718  ...  1.209129  0.368444  0.414352 -1.065435\n",
              "1      1  2.882159  1.241338  2.232549  ...  1.529813 -0.640376  1.717616 -1.703426\n",
              "2      2  0.688133 -0.318813  0.999343  ...  0.886608 -0.040592  0.387883 -0.961931\n",
              "3      3  1.721908  0.388303  0.947409  ...  1.006176 -0.079589  0.593564 -1.152426\n",
              "4      4  0.650453 -0.146959  0.802256  ...  0.573340 -0.265548  0.279305 -0.208944\n",
              "..   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
              "832  832  0.092474  0.106150  0.033633  ...  0.147475  0.069725  0.208504 -0.096100\n",
              "833  833 -0.072233  0.119163  0.096811  ...  0.033548  0.223271 -0.203688 -0.038956\n",
              "834  834 -0.132909  0.104212  0.050959  ... -0.005544  0.270046 -0.138641 -0.049895\n",
              "835  835 -0.079618 -0.154153  0.295142  ...  0.194430  0.168985 -0.259337 -0.073086\n",
              "836  836  0.239000  0.008422  0.717436  ...  0.267640 -0.421967  0.063392  0.015846\n",
              "\n",
              "[837 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ozu5hHltHR",
        "outputId": "b5a02eb1-4986-4552-abf6-f55354878c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 将词向量提取为特征,第二列到倒数第一列\n",
        "features =node_features.iloc[:,1:]\n",
        " # 检查特征：共64个特征，837个样本点\n",
        "print(features.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(837, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gypPFc0lb_c",
        "outputId": "dc87fb70-f60d-41f2-ff53-37d5d3ead627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 提取节点标签\n",
        "node_label = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNode_label.csv',header = None)\n",
        "labels = node_label[1] # 提取节点标签列\n",
        "labels[:5]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yiryWH9Npol"
      },
      "source": [
        "filename1 = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv'\n",
        "def load_file_as_Adj_matrix(filename):\n",
        "  '''\n",
        "  Read .rating file and Return dok matrix.\n",
        "  The first line of .rating file is: num_users\\t num_items\n",
        "  '''\n",
        "  # # Get number of users and items\n",
        "  # num_users, num_items = 0, 0\n",
        "  # with open(filename, \"r\") as f:\n",
        "  #   line = f.readline()\n",
        "  #   while line != None and line != \"\":\n",
        "  #     arr = line.split(\",\")\n",
        "  #     u, i = int(arr[0]), int(arr[1])\n",
        "  #     num_users = max(num_users, u)\n",
        "  #     num_items = max(num_items, i)\n",
        "  #     line = f.readline()\n",
        "  # # Construct matrix\n",
        "  # print(num_users)\n",
        "  # print(num_items)\n",
        "  relation_matrix = np.zeros((837,837))\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      # user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "      # if (rating > 0):\n",
        "      user, item = int(arr[0]), int(arr[1])\n",
        "      relation_matrix[user, item] = 1\n",
        "      line = f.readline()    \n",
        "  return relation_matrix\n",
        "Adj = load_file_as_Adj_matrix(filename1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s8tkwn1Q1H1",
        "outputId": "8971a036-52ae-4a48-c040-02f8871e0f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Adj[0,267]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXVUiaWIMy_4",
        "outputId": "b478c1d8-36e8-46c3-815d-23defd601048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# 提取关系对\n",
        "node_relationship = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv',header = None)\n",
        "import networkx as nx\n",
        "G = nx.Graph() #画图\n",
        "G.add_edges_from([edge for edge in zip(node_relationship[0],node_relationship[1])]) \n",
        "\n",
        "# nx.draw(G,\n",
        "#         # pos = nx.random_layout(G),\n",
        "#         # pos = nx.spring_layout(G),\n",
        "#         # pos = nx.shell_layout(G),\n",
        "#         pos = nx.circular_layout(G),\n",
        "#         node_color = 'r',\n",
        "#         edge_color = 'b',\n",
        "#         with_labels = True,\n",
        "#         font_size =20,\n",
        "#         node_size =1000,\n",
        "#         alpha=0.3)\n",
        "\n",
        "\n",
        "Adj = nx.adjacency_matrix(G) # 构造邻接矩阵\n",
        "Adj.todense()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 0, 0, ..., 0, 0, 0],\n",
              "        [1, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEMcCle2ntbd",
        "outputId": "a9f54d32-0be9-4c6c-b066-b45f46bb45e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import scipy.sparse as sp\n",
        "#Adj = csr_matrix((labels,(node_relationship[0],node_relationship[1])),shape=(837,837))\n",
        "#Adj\n",
        "Adj = sp.csr_matrix(Adj, dtype=np.float32)\n",
        "Adj.todense()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUCPEuDYdh4"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "def load_data(adj,node_features,node_labels):\n",
        "  features = sp.csr_matrix(node_features, dtype=np.float32)  # 储存为csr型稀疏矩阵\n",
        "  # build symmetric adjacency matrix   论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
        "  # adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "  # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
        "  features = normalize(features)\n",
        "  adj = normalize(adj + sp.eye(adj.shape[0]))   # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
        "  # 对应公式A~=A+IN\n",
        "  # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
        "  idx_train = range(500)\n",
        "  idx_val = range(500, 660)\n",
        "  idx_test = range(660, 836)  \n",
        "  features = torch.FloatTensor(np.array(features.todense()))  # tensor为pytorch常用的数据结构\n",
        "  labels = torch.LongTensor(np.array(node_labels))\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj)   # 邻接矩阵转为tensor处理\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "  idx_val = torch.LongTensor(idx_val)\n",
        "  idx_test = torch.LongTensor(idx_test)\n",
        "  return adj, features, labels, idx_train, idx_val, idx_test  \n",
        "def normalize(mx):\n",
        "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))  # 对每一行求和\n",
        "  r_inv = np.power(rowsum, -1).flatten()  # 求倒数\n",
        "  r_inv[np.isinf(r_inv)] = 0.  # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
        "  r_mat_inv = sp.diags(r_inv)  # 构建对角元素为r_inv的对角矩阵\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
        "  return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels) # 使用type_as(tesnor)将张量转换为给定类型的张量。\n",
        "  correct = preds.eq(labels).double()  # 记录等于preds的label eq:equal\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):    # 把一个sparse matrix转为torch稀疏张量\n",
        "  \"\"\"\n",
        "  numpy中的ndarray转化成pytorch中的tensor : torch.from_numpy()\n",
        "  pytorch中的tensor转化成numpy中的ndarray : numpy()\n",
        "  \"\"\"\n",
        "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "  indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "  # 不懂的可以去看看COO性稀疏矩阵的结构\n",
        "  values = torch.from_numpy(sparse_mx.data)\n",
        "  shape = torch.Size(sparse_mx.shape)\n",
        "  return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEF3l9vGxWhX"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "  # 初始化层：输入feature，输出feature，权重，偏移\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    super(GraphConvolution, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # FloatTensor建立tensor\n",
        "    # 常见用法self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))：\n",
        "    # 首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter\n",
        "    # 绑定到这个module里面，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。\n",
        "    # 使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。\n",
        "    if bias:\n",
        "      self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "    else:\n",
        "      self.register_parameter('bias', None)\n",
        "      # Parameters与register_parameter都会向parameters写入参数，但是后者可以支持字符串命名\n",
        "      self.reset_parameters()\n",
        "\n",
        "  # 初始化权重\n",
        "  def reset_parameters(self):\n",
        "    stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "    # size()函数主要是用来统计矩阵元素个数，或矩阵某一维上的元素个数的函数  size（1）为行\n",
        "    self.weight.data.uniform_(-stdv, stdv)  # uniform() 方法将随机生成下一个实数，它在 [x, y] 范围内\n",
        "    if self.bias is not None:\n",
        "      self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  '''\n",
        "  前馈运算 即计算A~ X W(0)\n",
        "  input X与权重W相乘，然后adj矩阵与他们的积稀疏乘\n",
        "  直接输入与权重之间进行torch.mm操作，得到support，即XW\n",
        "  support与adj进行torch.spmm操作，得到output，即AXW选择是否加bias\n",
        "  '''\n",
        "  def forward(self, input, adj):\n",
        "    support = torch.mm(input, self.weight)\n",
        "    # torch.mm(a, b)是矩阵a和b矩阵相乘，torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等\n",
        "    output = torch.spmm(adj, support)\n",
        "    if self.bias is not None:\n",
        "      return output + self.bias\n",
        "    else:\n",
        "      return output\n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + ' (' \\\n",
        "    + str(self.in_features) + ' -> ' \\\n",
        "    + str(self.out_features) + ')'\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NWtCnVPB4nb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "    # 底层节点的参数，feature的个数；隐层节点个数；最终的分类数\n",
        "    super(GCN, self).__init__()  #  super()._init_()在利用父类里的对象构造函数\n",
        "    self.gc1 = GraphConvolution(nfeat, nhid)   # gc1输入尺寸nfeat，输出尺寸nhid\n",
        "    self.gc2 = GraphConvolution(nhid, nclass)  # gc2输入尺寸nhid，输出尺寸ncalss\n",
        "    self.dropout = dropout\n",
        "    # 输入分别是特征和邻接矩阵。最后输出为输出层做log_softmax变换的结果\n",
        "  def forward(self, x, adj):\n",
        "    x = F.relu(self.gc1(x, adj))    # adj即公式Z=softmax(A~Relu(A~XW(0))W(1))中的A~\n",
        "    x1 = F.dropout(x, self.dropout, training = self.training)  # x要dropout\n",
        "    x2 = self.gc2(x1, adj)\n",
        "    return F.log_softmax(x, dim = 1), x1  # 参数dim=1表示对每一行求softmax，那么每一行的值加起来都等于1。\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ZuShsc6eZJ",
        "outputId": "30af4a02-bb87-4bb7-abf2-74026cf79ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse  # argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(Adj,features,labels)\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=2,\n",
        "            dropout=0.1)\n",
        "output, x1 = model(features, adj)\n",
        "x1[:10]"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.1070e-35, 0.0000e+00, 4.7924e-43, 0.0000e+00, 4.8065e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [1.2270e-35, 0.0000e+00, 3.1249e-43, 0.0000e+00, 3.1669e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [1.8556e-35, 0.0000e+00, 4.5122e-43, 0.0000e+00, 4.5262e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [1.2534e-35, 0.0000e+00, 6.2638e-43, 0.0000e+00, 6.2918e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [1.0862e-35, 0.0000e+00, 4.7784e-43, 0.0000e+00, 4.8205e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [2.7567e-35, 0.0000e+00, 3.4752e-43, 0.0000e+00, 3.5032e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [3.0330e-35, 0.0000e+00, 3.2230e-43, 0.0000e+00, 3.2510e-43, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [3.5974e-35, 0.0000e+00, 3.8816e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [9.1177e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan],\n",
              "        [1.2336e-35, 0.0000e+00, 4.2319e-43, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "                nan,        nan,        nan,        nan,        nan,        nan,\n",
              "                nan,        nan,        nan,        nan]],\n",
              "       grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9JVSKDhxw00",
        "outputId": "56b2585a-c8c5-47c8-ba5e-81710c941661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse  # argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Training settings\n",
        "learning_rate = 0.01\n",
        "weight_decay = 5e-4\n",
        "epoch_num = 100\n",
        "dropout = 0.1\n",
        "#in_size = node_features  #设置输入层的维数\n",
        "hi_size = 16 #设置隐藏层的维数\n",
        "#out_size = node_label #设置输入层的维数\n",
        "\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(Adj,features,labels)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=hi_size,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# 数据写入cuda，便于后续加速\n",
        "\n",
        "# if args.cuda:\n",
        "#     model.cuda()   # . cuda()会分配到显存里（如果gpu可用）\n",
        "#     features = features.cuda()\n",
        "#     adj = adj.cuda()\n",
        "#     labels = labels.cuda()\n",
        "#     idx_train = idx_train.cuda()\n",
        "#     idx_val = idx_val.cuda()\n",
        "#     idx_test = idx_test.cuda()\n",
        "#global node_vec\n",
        "\n",
        "def train(epoch_num):\n",
        "  t = time.time()  # 返回当前时间\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  # optimizer.zero_grad()意思是把梯度置零，也就是把loss关于weight的导数变成0.\n",
        "  # pytorch中每一轮batch需要设置optimizer.zero_gra\n",
        "  global Emdebding_train\n",
        "  output, Emdebding_train = model(features, adj)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  # 由于在算output时已经使用了log_softmax，这里使用的损失函数就是NLLloss，如果前面没有加log运算，\n",
        "  # 这里就要使用CrossEntropyLoss了\n",
        "  # 损失函数NLLLoss() 的输入是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率，\n",
        "  # 适合最后一层是log_softmax()的网络. 损失函数 CrossEntropyLoss() 与 NLLLoss() 类似,\n",
        "  # 唯一的不同是它为我们去做 softmax.可以理解为：CrossEntropyLoss()=log_softmax() + NLLLoss()\n",
        "  # https://blog.csdn.net/hao5335156/article/details/80607732\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])  #计算准确率\n",
        "  loss_train.backward()  # 反向求导  Back Propagation\n",
        "  optimizer.step()  # 更新所有的参数  Gradient Descent\n",
        "    \n",
        "  #if not args.fastmode:\n",
        "      # Evaluate validation set performance separately,\n",
        "      # deactivates dropout during validation run.\n",
        "  model.eval()  # eval() 函数用来执行一个字符串表达式，并返回表达式的值\n",
        "  global Emdebding_eval\n",
        "  output, Emdebding_eval = model(features, adj)\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])    # 验证集的损失函数\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),     \n",
        "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "        'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
        "def test():\n",
        "  model.eval()\n",
        "  global Emdebding_test\n",
        "  output, Emdebding_test = model(features, adj)\n",
        "  loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "  acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "  print(\"Test set results:\",\n",
        "        \"loss= {:.4f}\".format(loss_test.item()),\n",
        "        \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "# Train model  逐个epoch进行train，最后test\n",
        "t_total = time.time()\n",
        "for epoch in range(epoch_num):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "test()\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0125s\n",
            "Epoch: 0002 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0056s\n",
            "Epoch: 0003 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0004 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0005 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0006 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0007 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0008 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0044s\n",
            "Epoch: 0009 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0055s\n",
            "Epoch: 0010 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0011 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0012 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0013 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0014 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0015 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0055s\n",
            "Epoch: 0016 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0017 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0018 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0019 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0055s\n",
            "Epoch: 0020 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0021 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0022 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0023 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0024 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0025 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0026 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0048s\n",
            "Epoch: 0027 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0028 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0048s\n",
            "Epoch: 0029 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0030 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0031 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0032 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0033 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0054s\n",
            "Epoch: 0034 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0054s\n",
            "Epoch: 0035 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0036 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0037 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0066s\n",
            "Epoch: 0038 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0066s\n",
            "Epoch: 0039 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0040 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0041 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0042 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0054s\n",
            "Epoch: 0043 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0044 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0045 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0046 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0047 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0048 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0049 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0050 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0051 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0048s\n",
            "Epoch: 0052 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0053 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0054 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0055 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0056 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0057 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0058 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0044s\n",
            "Epoch: 0059 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0060 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0061 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0043s\n",
            "Epoch: 0062 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0063 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0064 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0065 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0066 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0067 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0068 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0069 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0045s\n",
            "Epoch: 0070 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0071 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0054s\n",
            "Epoch: 0072 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0073 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0074 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0074s\n",
            "Epoch: 0075 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0054s\n",
            "Epoch: 0076 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0077 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0078 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0057s\n",
            "Epoch: 0079 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0047s\n",
            "Epoch: 0080 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0081 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0082 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0083 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0084 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0085 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0044s\n",
            "Epoch: 0086 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0087 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0048s\n",
            "Epoch: 0088 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0048s\n",
            "Epoch: 0089 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0050s\n",
            "Epoch: 0090 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0046s\n",
            "Epoch: 0091 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0092 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0054s\n",
            "Epoch: 0093 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0094 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0095 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Epoch: 0096 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0053s\n",
            "Epoch: 0097 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0056s\n",
            "Epoch: 0098 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0051s\n",
            "Epoch: 0099 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0049s\n",
            "Epoch: 0100 loss_train: nan acc_train: 0.5340 loss_val: nan acc_val: 0.0000 time: 0.0052s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.5719s\n",
            "Test set results: loss= nan accuracy= 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acXF0k_wpQfi",
        "outputId": "0385181e-c23c-418c-9e1f-cae7ea8683b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "Emdebding_eval[:10]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
              "       grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqrrHtd7SGT7"
      },
      "source": [
        "建立NeuMF层模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYWxFtfXrg9a"
      },
      "source": [
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "#from keras import initializations\n",
        "#from keras.regularizers import l1, l2, l1l2\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Dense, Lambda, Activation\n",
        "from keras.layers import Embedding, Input, Dense, merge, Reshape, Flatten, Dropout\n",
        "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
        "from time import time\n",
        "import sys\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kHfN6ORfXap"
      },
      "source": [
        "使用数据生成正负样本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNF9ZTvvoe3N"
      },
      "source": [
        "def load_rating_file_as_matrix(filename):\n",
        "  '''\n",
        "  Read .rating file and Return dok matrix.\n",
        "  The first line of .rating file is: num_users\\t num_items\n",
        "  '''\n",
        "  # Get number of users and items\n",
        "  num_users, num_items = 0, 0\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      u, i = int(arr[0]), int(arr[1])\n",
        "      num_users = max(num_users, u)\n",
        "      num_items = max(num_items, i)\n",
        "      line = f.readline()\n",
        "  # Construct matrix\n",
        "  print(num_users)\n",
        "  print(num_items)\n",
        "  mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      #user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "      #if (rating > 0):\n",
        "      user, item = int(arr[0]), int(arr[1])\n",
        "      mat[user, item] = 1.0\n",
        "      line = f.readline()    \n",
        "  return mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCaosFTwqXrM"
      },
      "source": [
        "filename = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv'\n",
        "train_Martrix = load_rating_file_as_matrix(filename)\n",
        "train_Martrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVndGHpv_wyR"
      },
      "source": [
        "print(train_Martrix.keys().shape)\n",
        "print(train_Martrix.values().shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDjZWK9vQ5R6"
      },
      "source": [
        "将vector of each nodes construct a serise of pairs of nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PS2zqyeUgjd"
      },
      "source": [
        "def get_train_instances(train, num_negatives):\n",
        "  user_input, item_input, labels = [],[],[]\n",
        "  num_users, num_items = train.shape\n",
        "  for (u, i) in train.keys():\n",
        "    # positive instance\n",
        "    user_input.append(u)\n",
        "    item_input.append(i)\n",
        "    labels.append(1)\n",
        "    # negative instances\n",
        "    for t in range(num_negatives):\n",
        "      j = np.random.randint(num_users,num_items)\n",
        "      while (u, j) in train.keys():\n",
        "        j = np.random.randint(num_users,num_items)\n",
        "      user_input.append(u)\n",
        "      item_input.append(j)\n",
        "      labels.append(0)\n",
        "  # 遍历生成NeuMF需要的drug的vecter\n",
        "  drug_latent_vector, disease_latent_vector = [], []\n",
        "  for i in user_input:\n",
        "    drug_latent_vector.append(Emdebding_eval[i].detach().numpy())\n",
        "  for j in item_input:\n",
        "    disease_latent_vector.append(Emdebding_eval[j].detach().numpy())\n",
        "  return drug_latent_vector, disease_latent_vector, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZhNi1P70sJw"
      },
      "source": [
        "len(Emdebding_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nZHG7uFzFFj"
      },
      "source": [
        "user_input[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqFdTGvZzK6F"
      },
      "source": [
        "item_input[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhcANAZLzP3K"
      },
      "source": [
        "labels[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-kJPd5ZK_Ou"
      },
      "source": [
        "from keras.layers.merge import multiply, concatenate\n",
        "from keras.layers import Input\n",
        "def NeuMF_getmodel(layers=[10], reg_layers=[0], reg_mf=0):\n",
        "  assert len(layers) == len(reg_layers)\n",
        "  num_layer = len(layers) #Number of layers in the MLP\n",
        "  # Input variables\n",
        "  drug_Embedded = Input(shape=(16,))\n",
        "  disease_Embedded = Input(shape=(16,))\n",
        "  \n",
        "  mf_vector = multiply([drug_Embedded, disease_Embedded])\n",
        "  # mf_vector = merge([drug_Embedded, disease_Embedded], mode='mul') # element-wise multiply\n",
        "\n",
        "  # MLP part \n",
        "  mlp_vector = concatenate([drug_Embedded, disease_Embedded])\n",
        "  # mlp_vector = merge([drug_Embedded, disease_Embedded], mode='concat')\n",
        "  for idx in range(1, num_layer):\n",
        "    layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
        "    mlp_vector = layer(mlp_vector)\n",
        "\n",
        "  # Concatenate MF and MLP parts\n",
        "  predict_vector = concatenate([mf_vector, mlp_vector])\n",
        "  # predict_vector = merge([mf_vector, mlp_vector], mode='concat')  \n",
        "  # Final prediction layer\n",
        "  prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector) # sigmoid\n",
        "    \n",
        "  model = Model(input=[drug_Embedded, disease_Embedded],\n",
        "                output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XCZ0dwSZmPl"
      },
      "source": [
        "drug_latent_vector, disease_latent_vector, labels = get_train_instances(train_Martrix, 1)\n",
        "mf_vector = multiply([drug_latent_vector, disease_latent_vector])\n",
        "mf_vector.shape()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-doCnCzgl1mB"
      },
      "source": [
        "mf_vector.shape()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbsRnpQ2PYJ"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "# dataset = np.hstack((drug_latent_vector,disease_latent_vector))\n",
        "# train_data = dataset[:69650]\n",
        "# test_data = dataset[69650:]\n",
        "# train_labels = labels[:69650]\n",
        "# test_labels = labels[69650:]\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "num_negatives = 1 # 4\n",
        "global num_scale \n",
        "num_scale = 27862 # 69650\n",
        "batch_size = 256\n",
        "\n",
        "model = NeuMF_getmodel()\n",
        "model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "# load pima indians dataset\n",
        "drug_latent_vector, disease_latent_vector, labels = get_train_instances(train_Martrix, num_negatives)\n",
        "train_loss = []\n",
        "# Training model\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # Generate training instances\n",
        "  # Training\n",
        "  hist = model.fit([np.array(drug_latent_vector[:num_scale]), np.array(disease_latent_vector[:num_scale])], #input \n",
        "                   np.array(labels[:num_scale]), # labels \n",
        "                   batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "  train_loss.append(hist.history['loss'][0])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "        'loss_train: {:.4f}'.format(hist.history['loss'][0]),\n",
        "        'acc_train: {:.4f}'.format(hist.history['acc'][0]))\n",
        "  \n",
        "test_scores = model.evaluate([np.array(drug_latent_vector[num_scale:]), np.array(disease_latent_vector[num_scale:])], labels[num_scale:])\n",
        "print(\"Test set results:\",\n",
        "      \"loss= {:.4f}\".format(test_scores[0]),\n",
        "      'acc_train: {:.4f}'.format(test_scores[1]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "epochs = len(train_loss)\n",
        "plt.plot(range(0,epochs,1), train_loss, label='train_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.savefig(\"/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/\"+time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))+\"Unet-过拟合C0.jpg\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4UdCL374F0X"
      },
      "source": [
        "hist.history['acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Oku26BpTb3"
      },
      "source": [
        "test_scores[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxOuiXtkCtO"
      },
      "source": [
        "# 计算AUC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import time\n",
        "\n",
        "y_score1 = model.predict([np.array(drug_latent_vector[num_scale:]), np.array(disease_latent_vector[num_scale:])])\n",
        "\n",
        "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
        "#Y_train0为真实标签，Y_pred_0为预测标签，注意，这里roc_curve为一维的输入，Y_train0是一维的\n",
        "fpr, tpr, thresholds_keras = roc_curve(labels[num_scale:], y_score1)   \n",
        "auc = auc(fpr, tpr)\n",
        "print(\"AUC : \", auc)\n",
        "plt.figure()\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/'+ now + 'ROC.jpg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uxl8iFFwVDM"
      },
      "source": [
        "# 归一化数据\n",
        "from sklearn import preprocessing  \n",
        "min_max_scaler = preprocessing.MinMaxScaler()  #标准化训练集数据 \n",
        "data_train_nomal = min_max_scaler.fit_transform(y_score1)  #对测试集数据进行相同的归一化处理\n",
        "data_train_nomal[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz-JEXmi8tE1"
      },
      "source": [
        "print('max:', data_train_nomal.max())\n",
        "print('min:', data_train_nomal.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXfQJR1oM6HI"
      },
      "source": [
        "def auc_calculate(labels,preds,n_bins=17420):\n",
        "    postive_len = sum(labels)   #正样本数量（因为正样本都是1）\n",
        "    negative_len = len(labels) - postive_len #负样本数量\n",
        "    print('负样本数量:', negative_len)\n",
        "    total_case = postive_len * negative_len #正负样本对\n",
        "    pos_histogram = [0 for _ in range(n_bins)] \n",
        "    neg_histogram = [0 for _ in range(n_bins)]\n",
        "    bin_width = 1.0 / n_bins\n",
        "    for i in range(len(labels)):\n",
        "        nth_bin = int(preds[i]/bin_width)\n",
        "        if labels[i]==1:\n",
        "            pos_histogram[nth_bin] += 1\n",
        "        else:\n",
        "            neg_histogram[nth_bin] += 1\n",
        "    accumulated_neg = 0\n",
        "    satisfied_pair = 0\n",
        "    for i in range(n_bins):\n",
        "        satisfied_pair += (pos_histogram[i]*accumulated_neg + pos_histogram[i]*neg_histogram[i]*0.5)\n",
        "        accumulated_neg += neg_histogram[i]\n",
        "    return satisfied_pair / float(total_case)\n",
        "print(\"验证:\",auc_calculate(labels[27862:], y_score1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R95RmynP0LsH"
      },
      "source": [
        "from sklearn import metrics\n",
        "'''使用real.csv和result.csv列数据，计算PR曲线的AUC值'''\n",
        "precision, recall, _thresholds = metrics.precision_recall_curve(labels[69650:], data_train_nomal)\n",
        "area = metrics.auc(recall, precision)\n",
        "print(area) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8QW2jP7JV2"
      },
      "source": [
        "import tensorflow as tf # Orange 1.14.0\n",
        "print(tf.__version__)\n",
        "import keras # 2.2.5\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TkV_5eTmCFU"
      },
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "!pip install keras==2.2.5"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}