{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_pridiction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XkvkckV6Lc5VVRFDhXiPCTrNYzg7vQ0W",
      "authorship_tag": "ABX9TyMN7J6LqegI8OGUi+8KVeNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevejobws/Colaboratory/blob/master/gcn_pridiction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA0EeTTq6Gi-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b6b76fb-836d-49a7-d7e7-f67c25b106ba"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install torchvision\n",
        "!pip install torch_sparse -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_scatter -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_geometric # 下载安装pytorch_geometric\n",
        "!pip install networkx # 画图\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric \n",
        "from torch_geometric.nn import GCNConv, ChebConv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.6/dist-packages (0.6.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch_sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch_sparse) (1.18.5)\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.6/dist-packages (2.0.5)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.6/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.48.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.11.2)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (3.20.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.18.5)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (5.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.6.0+cu101)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch_geometric) (49.6.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch_geometric) (0.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch_geometric) (1.1.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch_geometric) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch_geometric) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch_geometric) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch_geometric) (4.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch_geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch_geometric) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch_geometric) (0.6.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch_geometric) (2.4.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch_geometric) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch_geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch_geometric) (1.2.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-70ab89b42959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChebConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataListLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[1;32m      9\u001b[0m                                    contains_self_loops, is_undirected)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             f'matches your PyTorch install.')\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseStorage\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_sparse/storage.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegment_csr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_scatter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt_minor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;34mf'Detected that PyTorch and torch_scatter were compiled with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34mf'different CUDA versions. PyTorch has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34mf'{t_major}.{t_minor} and torch_scatter has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torch_scatter were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_scatter has CUDA version 0.0. Please reinstall the torch_scatter that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5TCDF488n7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "0cc9741b-cf79-4dbc-9854-660008920593"
      },
      "source": [
        "! python -c \"import torch_geometric; print(torch_geometric.__version__)\" # 检查是否安装成功"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/__init__.py\", line 2, in <module>\n",
            "    import torch_geometric.nn\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/__init__.py\", line 2, in <module>\n",
            "    from .data_parallel import DataParallel\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/data_parallel.py\", line 5, in <module>\n",
            "    from torch_geometric.data import Batch\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/data/__init__.py\", line 1, in <module>\n",
            "    from .data import Data\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\", line 7, in <module>\n",
            "    from torch_sparse import coalesce, SparseTensor\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_sparse/__init__.py\", line 34, in <module>\n",
            "    from .storage import SparseStorage  # noqa\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_sparse/storage.py\", line 5, in <module>\n",
            "    from torch_scatter import segment_csr, scatter_add\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch_scatter/__init__.py\", line 55, in <module>\n",
            "    f'Detected that PyTorch and torch_scatter were compiled with '\n",
            "RuntimeError: Detected that PyTorch and torch_scatter were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_scatter has CUDA version 0.0. Please reinstall the torch_scatter that matches your PyTorch install.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57sAyKd7EgIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "3faba9a1-84bd-4ff2-95d8-d5cd39cb8b10"
      },
      "source": [
        "! uname -a  # 查看系统  \n",
        "! python --version  # 查看python版本 \n",
        "! python -c 'import torch; print(torch.version.cuda)' # 查看cuda的版本，检查是否和cuda的一致\n",
        "! nvcc --version # 查看nvcc版本 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux e4e606ebf6e4 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Python 3.6.9\n",
            "10.1\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki24HI_7E2mL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa4e9f0d-0cea-4594-b605-ce32e4a136e8"
      },
      "source": [
        "print(torch.version.cuda) # torch的cuda版本"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiGI-kjVP8lQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 导入数据\n",
        "import pandas as pd\n",
        "\n",
        "Adj = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv')\n",
        "node_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNodeAttribute.csv')\n",
        "node_label = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNode_label.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngcbuz7yXU-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "3f77b7e0-f19d-4c04-e3a3-109ec800b647"
      },
      "source": [
        "Adj"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>267</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17408</th>\n",
              "      <td>266</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17409</th>\n",
              "      <td>266</td>\n",
              "      <td>403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17410</th>\n",
              "      <td>266</td>\n",
              "      <td>404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17411</th>\n",
              "      <td>266</td>\n",
              "      <td>663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17412</th>\n",
              "      <td>266</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17413 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0  267\n",
              "0        0  268\n",
              "1        0  269\n",
              "2        0  270\n",
              "3        0  271\n",
              "4        0  272\n",
              "...    ...  ...\n",
              "17408  266  660\n",
              "17409  266  403\n",
              "17410  266  404\n",
              "17411  266  663\n",
              "17412  266  354\n",
              "\n",
              "[17413 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YErpqu5IXXZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "35da3afb-1499-449b-f18e-9780c5f52b63"
      },
      "source": [
        "node_features"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.7706328</th>\n",
              "      <th>0.010733238</th>\n",
              "      <th>0.5827176</th>\n",
              "      <th>-0.06261978</th>\n",
              "      <th>-0.23856387</th>\n",
              "      <th>-0.715823</th>\n",
              "      <th>1.0229808</th>\n",
              "      <th>0.18164602</th>\n",
              "      <th>-0.26039103</th>\n",
              "      <th>-0.5420329</th>\n",
              "      <th>0.23313928</th>\n",
              "      <th>-0.28343192</th>\n",
              "      <th>0.11714408</th>\n",
              "      <th>0.5314488</th>\n",
              "      <th>-0.06122111</th>\n",
              "      <th>-0.22952928</th>\n",
              "      <th>-0.7105971</th>\n",
              "      <th>0.28848463</th>\n",
              "      <th>0.09841086</th>\n",
              "      <th>0.18965468</th>\n",
              "      <th>-0.5119621</th>\n",
              "      <th>-0.2632837</th>\n",
              "      <th>0.0783294</th>\n",
              "      <th>0.19871941</th>\n",
              "      <th>1.0868483999999998</th>\n",
              "      <th>-0.2568045</th>\n",
              "      <th>0.026245711</th>\n",
              "      <th>0.09389911</th>\n",
              "      <th>-0.14249657</th>\n",
              "      <th>0.01881871</th>\n",
              "      <th>-0.23493719</th>\n",
              "      <th>-0.15342341</th>\n",
              "      <th>0.42577162</th>\n",
              "      <th>0.6739689</th>\n",
              "      <th>0.025524665</th>\n",
              "      <th>-0.2948406</th>\n",
              "      <th>-0.31587312</th>\n",
              "      <th>0.55261844</th>\n",
              "      <th>0.43735474</th>\n",
              "      <th>-0.08744249</th>\n",
              "      <th>0.5361331</th>\n",
              "      <th>-0.36645055</th>\n",
              "      <th>0.6569897</th>\n",
              "      <th>0.802005</th>\n",
              "      <th>-0.016102083</th>\n",
              "      <th>-0.66942054</th>\n",
              "      <th>0.8565628000000001</th>\n",
              "      <th>-0.101191685</th>\n",
              "      <th>-0.16558306</th>\n",
              "      <th>-0.40551326</th>\n",
              "      <th>-0.41094795</th>\n",
              "      <th>-0.027974876</th>\n",
              "      <th>-0.39776492</th>\n",
              "      <th>-0.509387</th>\n",
              "      <th>0.28056657</th>\n",
              "      <th>0.20877618</th>\n",
              "      <th>-0.099218085</th>\n",
              "      <th>-0.22935389</th>\n",
              "      <th>0.23234765</th>\n",
              "      <th>-1.0350324</th>\n",
              "      <th>1.2091292</th>\n",
              "      <th>0.36844373</th>\n",
              "      <th>0.41435245</th>\n",
              "      <th>-1.0654348</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.882159</td>\n",
              "      <td>1.241338</td>\n",
              "      <td>2.232549</td>\n",
              "      <td>-1.932836</td>\n",
              "      <td>-0.574318</td>\n",
              "      <td>0.168684</td>\n",
              "      <td>2.937954</td>\n",
              "      <td>1.398610</td>\n",
              "      <td>-0.260359</td>\n",
              "      <td>-1.253366</td>\n",
              "      <td>-0.997117</td>\n",
              "      <td>-1.355796</td>\n",
              "      <td>0.847303</td>\n",
              "      <td>2.447680</td>\n",
              "      <td>-0.573743</td>\n",
              "      <td>0.211870</td>\n",
              "      <td>-0.960837</td>\n",
              "      <td>1.536916</td>\n",
              "      <td>2.928266</td>\n",
              "      <td>-0.955619</td>\n",
              "      <td>-1.152885</td>\n",
              "      <td>0.548847</td>\n",
              "      <td>0.525967</td>\n",
              "      <td>-0.445511</td>\n",
              "      <td>2.999283</td>\n",
              "      <td>0.190644</td>\n",
              "      <td>-0.916012</td>\n",
              "      <td>0.877579</td>\n",
              "      <td>1.501392</td>\n",
              "      <td>-2.191907</td>\n",
              "      <td>-0.344929</td>\n",
              "      <td>-0.643345</td>\n",
              "      <td>1.808293</td>\n",
              "      <td>0.922850</td>\n",
              "      <td>0.823318</td>\n",
              "      <td>0.399970</td>\n",
              "      <td>0.415868</td>\n",
              "      <td>0.005898</td>\n",
              "      <td>-2.257816</td>\n",
              "      <td>-0.475472</td>\n",
              "      <td>0.610879</td>\n",
              "      <td>-1.345582</td>\n",
              "      <td>1.256365</td>\n",
              "      <td>0.962199</td>\n",
              "      <td>-1.093309</td>\n",
              "      <td>-1.647974</td>\n",
              "      <td>1.594285</td>\n",
              "      <td>0.584256</td>\n",
              "      <td>-1.293162</td>\n",
              "      <td>-1.844852</td>\n",
              "      <td>-0.580833</td>\n",
              "      <td>2.215265</td>\n",
              "      <td>-1.795820</td>\n",
              "      <td>-2.811482</td>\n",
              "      <td>0.229828</td>\n",
              "      <td>0.073673</td>\n",
              "      <td>0.256524</td>\n",
              "      <td>-0.438187</td>\n",
              "      <td>-0.097229</td>\n",
              "      <td>-0.740352</td>\n",
              "      <td>1.529813</td>\n",
              "      <td>-0.640376</td>\n",
              "      <td>1.717616</td>\n",
              "      <td>-1.703426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.688133</td>\n",
              "      <td>-0.318813</td>\n",
              "      <td>0.999343</td>\n",
              "      <td>-0.651598</td>\n",
              "      <td>-0.407260</td>\n",
              "      <td>-0.314173</td>\n",
              "      <td>0.781863</td>\n",
              "      <td>0.242623</td>\n",
              "      <td>-0.107326</td>\n",
              "      <td>-0.972608</td>\n",
              "      <td>-0.235344</td>\n",
              "      <td>-0.511505</td>\n",
              "      <td>-0.138459</td>\n",
              "      <td>0.775548</td>\n",
              "      <td>-0.218838</td>\n",
              "      <td>0.545011</td>\n",
              "      <td>-0.670150</td>\n",
              "      <td>0.699975</td>\n",
              "      <td>0.817735</td>\n",
              "      <td>-0.134486</td>\n",
              "      <td>0.054736</td>\n",
              "      <td>0.374199</td>\n",
              "      <td>0.183914</td>\n",
              "      <td>-0.104466</td>\n",
              "      <td>1.410521</td>\n",
              "      <td>-0.258784</td>\n",
              "      <td>0.201976</td>\n",
              "      <td>0.270087</td>\n",
              "      <td>0.458823</td>\n",
              "      <td>-0.055942</td>\n",
              "      <td>0.035193</td>\n",
              "      <td>0.027436</td>\n",
              "      <td>1.150917</td>\n",
              "      <td>0.318230</td>\n",
              "      <td>0.150467</td>\n",
              "      <td>0.657710</td>\n",
              "      <td>-0.184495</td>\n",
              "      <td>-0.571191</td>\n",
              "      <td>-0.604887</td>\n",
              "      <td>-0.231566</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>-0.379828</td>\n",
              "      <td>0.181649</td>\n",
              "      <td>0.533105</td>\n",
              "      <td>-0.220025</td>\n",
              "      <td>-0.499370</td>\n",
              "      <td>0.676739</td>\n",
              "      <td>-0.635464</td>\n",
              "      <td>-0.520307</td>\n",
              "      <td>-0.607030</td>\n",
              "      <td>0.455186</td>\n",
              "      <td>0.421789</td>\n",
              "      <td>-0.637401</td>\n",
              "      <td>-1.185999</td>\n",
              "      <td>-0.661417</td>\n",
              "      <td>0.244160</td>\n",
              "      <td>0.266237</td>\n",
              "      <td>-0.213120</td>\n",
              "      <td>0.249394</td>\n",
              "      <td>-0.382235</td>\n",
              "      <td>0.886608</td>\n",
              "      <td>-0.040592</td>\n",
              "      <td>0.387883</td>\n",
              "      <td>-0.961931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.721908</td>\n",
              "      <td>0.388303</td>\n",
              "      <td>0.947409</td>\n",
              "      <td>-0.787672</td>\n",
              "      <td>-0.362458</td>\n",
              "      <td>-0.536777</td>\n",
              "      <td>1.290188</td>\n",
              "      <td>0.130461</td>\n",
              "      <td>-0.550028</td>\n",
              "      <td>-0.882603</td>\n",
              "      <td>-0.284483</td>\n",
              "      <td>0.195723</td>\n",
              "      <td>0.287096</td>\n",
              "      <td>0.741486</td>\n",
              "      <td>-0.613376</td>\n",
              "      <td>0.241035</td>\n",
              "      <td>-1.183460</td>\n",
              "      <td>0.651605</td>\n",
              "      <td>1.076140</td>\n",
              "      <td>0.284106</td>\n",
              "      <td>-1.281615</td>\n",
              "      <td>-0.727382</td>\n",
              "      <td>0.383454</td>\n",
              "      <td>-0.202577</td>\n",
              "      <td>1.619946</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>-0.379740</td>\n",
              "      <td>0.251197</td>\n",
              "      <td>0.567685</td>\n",
              "      <td>-0.998128</td>\n",
              "      <td>-0.046036</td>\n",
              "      <td>-0.882844</td>\n",
              "      <td>0.740021</td>\n",
              "      <td>0.870464</td>\n",
              "      <td>0.363524</td>\n",
              "      <td>-0.268316</td>\n",
              "      <td>0.146103</td>\n",
              "      <td>0.422400</td>\n",
              "      <td>0.236998</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>0.789005</td>\n",
              "      <td>-1.078728</td>\n",
              "      <td>0.767913</td>\n",
              "      <td>1.031454</td>\n",
              "      <td>-1.020215</td>\n",
              "      <td>-0.265524</td>\n",
              "      <td>1.290493</td>\n",
              "      <td>-0.687332</td>\n",
              "      <td>-0.580685</td>\n",
              "      <td>-0.275520</td>\n",
              "      <td>0.351817</td>\n",
              "      <td>0.330816</td>\n",
              "      <td>-0.828862</td>\n",
              "      <td>-1.068953</td>\n",
              "      <td>-0.534331</td>\n",
              "      <td>-0.844828</td>\n",
              "      <td>0.119608</td>\n",
              "      <td>0.585099</td>\n",
              "      <td>0.437665</td>\n",
              "      <td>-1.466863</td>\n",
              "      <td>1.006176</td>\n",
              "      <td>-0.079589</td>\n",
              "      <td>0.593564</td>\n",
              "      <td>-1.152426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.650453</td>\n",
              "      <td>-0.146959</td>\n",
              "      <td>0.802256</td>\n",
              "      <td>-0.280101</td>\n",
              "      <td>-0.731831</td>\n",
              "      <td>-0.456279</td>\n",
              "      <td>0.775432</td>\n",
              "      <td>-0.465780</td>\n",
              "      <td>0.143515</td>\n",
              "      <td>-0.736436</td>\n",
              "      <td>0.108304</td>\n",
              "      <td>-0.252066</td>\n",
              "      <td>0.123692</td>\n",
              "      <td>0.359086</td>\n",
              "      <td>-0.440906</td>\n",
              "      <td>-0.286168</td>\n",
              "      <td>-0.400844</td>\n",
              "      <td>0.719755</td>\n",
              "      <td>0.256401</td>\n",
              "      <td>0.432786</td>\n",
              "      <td>-0.412044</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.045897</td>\n",
              "      <td>-0.033026</td>\n",
              "      <td>0.904498</td>\n",
              "      <td>0.185858</td>\n",
              "      <td>-0.173960</td>\n",
              "      <td>-0.415105</td>\n",
              "      <td>-0.231409</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.299792</td>\n",
              "      <td>-0.222521</td>\n",
              "      <td>0.539466</td>\n",
              "      <td>0.284991</td>\n",
              "      <td>0.444050</td>\n",
              "      <td>0.151308</td>\n",
              "      <td>-0.550617</td>\n",
              "      <td>0.110464</td>\n",
              "      <td>-0.113436</td>\n",
              "      <td>-0.128214</td>\n",
              "      <td>-0.150840</td>\n",
              "      <td>-0.427944</td>\n",
              "      <td>0.444110</td>\n",
              "      <td>0.421779</td>\n",
              "      <td>-0.314363</td>\n",
              "      <td>-0.818235</td>\n",
              "      <td>0.472876</td>\n",
              "      <td>-0.264762</td>\n",
              "      <td>0.096127</td>\n",
              "      <td>-0.077573</td>\n",
              "      <td>0.012599</td>\n",
              "      <td>-0.271649</td>\n",
              "      <td>-0.296263</td>\n",
              "      <td>-0.772448</td>\n",
              "      <td>-0.508725</td>\n",
              "      <td>-0.160995</td>\n",
              "      <td>0.444162</td>\n",
              "      <td>0.061997</td>\n",
              "      <td>-0.419543</td>\n",
              "      <td>-0.450393</td>\n",
              "      <td>0.573340</td>\n",
              "      <td>-0.265548</td>\n",
              "      <td>0.279305</td>\n",
              "      <td>-0.208944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.109805</td>\n",
              "      <td>-0.189683</td>\n",
              "      <td>0.301222</td>\n",
              "      <td>0.189257</td>\n",
              "      <td>0.174485</td>\n",
              "      <td>-0.396088</td>\n",
              "      <td>0.268056</td>\n",
              "      <td>0.363967</td>\n",
              "      <td>-0.306494</td>\n",
              "      <td>-0.857515</td>\n",
              "      <td>-0.064044</td>\n",
              "      <td>-0.278492</td>\n",
              "      <td>0.366741</td>\n",
              "      <td>0.323727</td>\n",
              "      <td>-0.041202</td>\n",
              "      <td>-0.457527</td>\n",
              "      <td>-0.592256</td>\n",
              "      <td>0.058035</td>\n",
              "      <td>0.381119</td>\n",
              "      <td>0.333784</td>\n",
              "      <td>-0.362219</td>\n",
              "      <td>-0.243467</td>\n",
              "      <td>0.141437</td>\n",
              "      <td>-0.012475</td>\n",
              "      <td>0.771640</td>\n",
              "      <td>0.328352</td>\n",
              "      <td>0.333306</td>\n",
              "      <td>0.156913</td>\n",
              "      <td>-0.125118</td>\n",
              "      <td>0.432134</td>\n",
              "      <td>-0.109195</td>\n",
              "      <td>0.065296</td>\n",
              "      <td>0.371117</td>\n",
              "      <td>0.092156</td>\n",
              "      <td>0.183601</td>\n",
              "      <td>-0.000676</td>\n",
              "      <td>-0.002262</td>\n",
              "      <td>0.320204</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>-0.053546</td>\n",
              "      <td>-0.182191</td>\n",
              "      <td>0.104103</td>\n",
              "      <td>0.101422</td>\n",
              "      <td>0.806096</td>\n",
              "      <td>0.072904</td>\n",
              "      <td>-0.428988</td>\n",
              "      <td>0.693647</td>\n",
              "      <td>-0.312175</td>\n",
              "      <td>-0.171792</td>\n",
              "      <td>-0.153653</td>\n",
              "      <td>-0.176002</td>\n",
              "      <td>0.244299</td>\n",
              "      <td>-0.264175</td>\n",
              "      <td>-0.228033</td>\n",
              "      <td>0.227927</td>\n",
              "      <td>0.060284</td>\n",
              "      <td>0.086738</td>\n",
              "      <td>-0.333917</td>\n",
              "      <td>0.296612</td>\n",
              "      <td>-0.501233</td>\n",
              "      <td>0.995152</td>\n",
              "      <td>0.179497</td>\n",
              "      <td>0.314474</td>\n",
              "      <td>-0.486160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>832</td>\n",
              "      <td>0.092474</td>\n",
              "      <td>0.106150</td>\n",
              "      <td>0.033633</td>\n",
              "      <td>0.018442</td>\n",
              "      <td>0.365770</td>\n",
              "      <td>0.026157</td>\n",
              "      <td>0.192186</td>\n",
              "      <td>-0.005236</td>\n",
              "      <td>-0.217018</td>\n",
              "      <td>0.257617</td>\n",
              "      <td>0.053886</td>\n",
              "      <td>0.152891</td>\n",
              "      <td>-0.122927</td>\n",
              "      <td>0.130594</td>\n",
              "      <td>-0.034565</td>\n",
              "      <td>-0.123680</td>\n",
              "      <td>-0.024969</td>\n",
              "      <td>0.082651</td>\n",
              "      <td>-0.083302</td>\n",
              "      <td>-0.207902</td>\n",
              "      <td>-0.276132</td>\n",
              "      <td>0.104665</td>\n",
              "      <td>-0.138261</td>\n",
              "      <td>0.175597</td>\n",
              "      <td>0.038199</td>\n",
              "      <td>-0.013455</td>\n",
              "      <td>-0.133683</td>\n",
              "      <td>-0.142527</td>\n",
              "      <td>-0.042970</td>\n",
              "      <td>-0.077926</td>\n",
              "      <td>-0.069572</td>\n",
              "      <td>0.217869</td>\n",
              "      <td>-0.284175</td>\n",
              "      <td>0.278465</td>\n",
              "      <td>-0.007701</td>\n",
              "      <td>0.073461</td>\n",
              "      <td>-0.324276</td>\n",
              "      <td>-0.116313</td>\n",
              "      <td>0.068643</td>\n",
              "      <td>-0.304055</td>\n",
              "      <td>0.147916</td>\n",
              "      <td>-0.450253</td>\n",
              "      <td>0.243640</td>\n",
              "      <td>0.015325</td>\n",
              "      <td>0.139896</td>\n",
              "      <td>0.207937</td>\n",
              "      <td>-0.348907</td>\n",
              "      <td>-0.123072</td>\n",
              "      <td>0.070664</td>\n",
              "      <td>0.271669</td>\n",
              "      <td>0.138426</td>\n",
              "      <td>0.055811</td>\n",
              "      <td>0.010685</td>\n",
              "      <td>0.009590</td>\n",
              "      <td>-0.026956</td>\n",
              "      <td>0.143835</td>\n",
              "      <td>0.134296</td>\n",
              "      <td>-0.018494</td>\n",
              "      <td>-0.094349</td>\n",
              "      <td>0.005062</td>\n",
              "      <td>0.147475</td>\n",
              "      <td>0.069725</td>\n",
              "      <td>0.208504</td>\n",
              "      <td>-0.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>833</td>\n",
              "      <td>-0.072233</td>\n",
              "      <td>0.119163</td>\n",
              "      <td>0.096811</td>\n",
              "      <td>-0.164411</td>\n",
              "      <td>0.278347</td>\n",
              "      <td>-0.201982</td>\n",
              "      <td>0.081316</td>\n",
              "      <td>-0.061770</td>\n",
              "      <td>-0.086994</td>\n",
              "      <td>0.235601</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.266871</td>\n",
              "      <td>-0.139913</td>\n",
              "      <td>-0.100954</td>\n",
              "      <td>-0.204267</td>\n",
              "      <td>-0.009871</td>\n",
              "      <td>-0.132975</td>\n",
              "      <td>-0.115135</td>\n",
              "      <td>0.074661</td>\n",
              "      <td>-0.066648</td>\n",
              "      <td>-0.195009</td>\n",
              "      <td>0.198070</td>\n",
              "      <td>0.041986</td>\n",
              "      <td>0.103717</td>\n",
              "      <td>0.212957</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.222006</td>\n",
              "      <td>0.069433</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.136644</td>\n",
              "      <td>-0.192403</td>\n",
              "      <td>-0.032700</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>-0.030231</td>\n",
              "      <td>0.076892</td>\n",
              "      <td>-0.572047</td>\n",
              "      <td>0.032351</td>\n",
              "      <td>0.019072</td>\n",
              "      <td>-0.175355</td>\n",
              "      <td>0.103921</td>\n",
              "      <td>-0.086620</td>\n",
              "      <td>-0.005460</td>\n",
              "      <td>0.071132</td>\n",
              "      <td>0.409461</td>\n",
              "      <td>0.037381</td>\n",
              "      <td>-0.081257</td>\n",
              "      <td>0.180506</td>\n",
              "      <td>-0.170608</td>\n",
              "      <td>0.123282</td>\n",
              "      <td>0.220902</td>\n",
              "      <td>-0.175002</td>\n",
              "      <td>-0.025833</td>\n",
              "      <td>0.079203</td>\n",
              "      <td>-0.021000</td>\n",
              "      <td>-0.098711</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>0.020249</td>\n",
              "      <td>0.162249</td>\n",
              "      <td>0.026010</td>\n",
              "      <td>0.033548</td>\n",
              "      <td>0.223271</td>\n",
              "      <td>-0.203688</td>\n",
              "      <td>-0.038956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>834</td>\n",
              "      <td>-0.132909</td>\n",
              "      <td>0.104212</td>\n",
              "      <td>0.050959</td>\n",
              "      <td>-0.111834</td>\n",
              "      <td>0.357196</td>\n",
              "      <td>-0.133188</td>\n",
              "      <td>0.143785</td>\n",
              "      <td>-0.049507</td>\n",
              "      <td>-0.147305</td>\n",
              "      <td>0.251885</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.199905</td>\n",
              "      <td>-0.070194</td>\n",
              "      <td>-0.153948</td>\n",
              "      <td>-0.143667</td>\n",
              "      <td>-0.036536</td>\n",
              "      <td>-0.038589</td>\n",
              "      <td>-0.034700</td>\n",
              "      <td>0.060599</td>\n",
              "      <td>-0.025647</td>\n",
              "      <td>-0.243201</td>\n",
              "      <td>0.148838</td>\n",
              "      <td>-0.037812</td>\n",
              "      <td>0.111142</td>\n",
              "      <td>0.232710</td>\n",
              "      <td>-0.081121</td>\n",
              "      <td>-0.017971</td>\n",
              "      <td>0.097984</td>\n",
              "      <td>-0.012490</td>\n",
              "      <td>-0.099915</td>\n",
              "      <td>0.092442</td>\n",
              "      <td>-0.163616</td>\n",
              "      <td>-0.015880</td>\n",
              "      <td>0.066135</td>\n",
              "      <td>-0.025328</td>\n",
              "      <td>0.081988</td>\n",
              "      <td>-0.507710</td>\n",
              "      <td>0.107737</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>-0.200301</td>\n",
              "      <td>0.093443</td>\n",
              "      <td>-0.091668</td>\n",
              "      <td>0.068706</td>\n",
              "      <td>-0.022480</td>\n",
              "      <td>0.297613</td>\n",
              "      <td>0.091121</td>\n",
              "      <td>-0.158395</td>\n",
              "      <td>0.178886</td>\n",
              "      <td>-0.117903</td>\n",
              "      <td>0.128856</td>\n",
              "      <td>0.285126</td>\n",
              "      <td>-0.158185</td>\n",
              "      <td>-0.052871</td>\n",
              "      <td>0.033910</td>\n",
              "      <td>-0.026669</td>\n",
              "      <td>-0.054522</td>\n",
              "      <td>0.224450</td>\n",
              "      <td>0.091497</td>\n",
              "      <td>0.057863</td>\n",
              "      <td>0.031415</td>\n",
              "      <td>-0.005544</td>\n",
              "      <td>0.270046</td>\n",
              "      <td>-0.138641</td>\n",
              "      <td>-0.049895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>835</td>\n",
              "      <td>-0.079618</td>\n",
              "      <td>-0.154153</td>\n",
              "      <td>0.295142</td>\n",
              "      <td>-0.150358</td>\n",
              "      <td>0.500962</td>\n",
              "      <td>-0.328006</td>\n",
              "      <td>-0.015192</td>\n",
              "      <td>-0.219420</td>\n",
              "      <td>-0.377795</td>\n",
              "      <td>0.257173</td>\n",
              "      <td>-0.064142</td>\n",
              "      <td>0.492528</td>\n",
              "      <td>-0.080609</td>\n",
              "      <td>-0.044967</td>\n",
              "      <td>-0.154126</td>\n",
              "      <td>-0.143692</td>\n",
              "      <td>0.100103</td>\n",
              "      <td>0.163071</td>\n",
              "      <td>-0.012129</td>\n",
              "      <td>-0.236396</td>\n",
              "      <td>-0.380849</td>\n",
              "      <td>0.415483</td>\n",
              "      <td>0.029281</td>\n",
              "      <td>0.239447</td>\n",
              "      <td>0.465136</td>\n",
              "      <td>-0.272850</td>\n",
              "      <td>0.155511</td>\n",
              "      <td>0.075649</td>\n",
              "      <td>0.088600</td>\n",
              "      <td>-0.196622</td>\n",
              "      <td>0.121215</td>\n",
              "      <td>-0.192279</td>\n",
              "      <td>0.005945</td>\n",
              "      <td>0.065828</td>\n",
              "      <td>-0.015154</td>\n",
              "      <td>-0.131507</td>\n",
              "      <td>-0.721625</td>\n",
              "      <td>0.131845</td>\n",
              "      <td>-0.092926</td>\n",
              "      <td>-0.340395</td>\n",
              "      <td>0.505998</td>\n",
              "      <td>-0.165514</td>\n",
              "      <td>0.192104</td>\n",
              "      <td>-0.060656</td>\n",
              "      <td>0.373459</td>\n",
              "      <td>0.290300</td>\n",
              "      <td>-0.260470</td>\n",
              "      <td>0.298324</td>\n",
              "      <td>-0.158419</td>\n",
              "      <td>0.281835</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>0.106287</td>\n",
              "      <td>0.018379</td>\n",
              "      <td>-0.148565</td>\n",
              "      <td>-0.095972</td>\n",
              "      <td>0.352697</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.086388</td>\n",
              "      <td>0.010307</td>\n",
              "      <td>0.194430</td>\n",
              "      <td>0.168985</td>\n",
              "      <td>-0.259337</td>\n",
              "      <td>-0.073086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>836</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.008422</td>\n",
              "      <td>0.717436</td>\n",
              "      <td>0.134245</td>\n",
              "      <td>0.373170</td>\n",
              "      <td>-0.059227</td>\n",
              "      <td>0.102996</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>-0.102515</td>\n",
              "      <td>0.433158</td>\n",
              "      <td>0.158489</td>\n",
              "      <td>0.265962</td>\n",
              "      <td>-0.011416</td>\n",
              "      <td>-0.031199</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>-0.496083</td>\n",
              "      <td>-0.352736</td>\n",
              "      <td>-0.257647</td>\n",
              "      <td>0.161620</td>\n",
              "      <td>-0.230452</td>\n",
              "      <td>-0.116733</td>\n",
              "      <td>0.159655</td>\n",
              "      <td>-0.278142</td>\n",
              "      <td>0.125518</td>\n",
              "      <td>0.496362</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>-0.318031</td>\n",
              "      <td>-0.112670</td>\n",
              "      <td>-0.150040</td>\n",
              "      <td>-0.268167</td>\n",
              "      <td>-0.436891</td>\n",
              "      <td>0.335615</td>\n",
              "      <td>-0.349596</td>\n",
              "      <td>0.179373</td>\n",
              "      <td>-0.294503</td>\n",
              "      <td>0.210135</td>\n",
              "      <td>-0.615341</td>\n",
              "      <td>-0.007536</td>\n",
              "      <td>0.045706</td>\n",
              "      <td>-0.705685</td>\n",
              "      <td>-0.061241</td>\n",
              "      <td>-0.744448</td>\n",
              "      <td>0.617036</td>\n",
              "      <td>-0.066599</td>\n",
              "      <td>0.155325</td>\n",
              "      <td>0.487323</td>\n",
              "      <td>-0.912633</td>\n",
              "      <td>0.122922</td>\n",
              "      <td>0.049172</td>\n",
              "      <td>0.452203</td>\n",
              "      <td>-0.023311</td>\n",
              "      <td>-0.192136</td>\n",
              "      <td>0.286585</td>\n",
              "      <td>0.194225</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.318570</td>\n",
              "      <td>-0.017260</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>-0.195706</td>\n",
              "      <td>-0.051707</td>\n",
              "      <td>0.267640</td>\n",
              "      <td>-0.421967</td>\n",
              "      <td>0.063392</td>\n",
              "      <td>0.015846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  0.7706328  0.010733238  ...  0.36844373  0.41435245  -1.0654348\n",
              "0      1   2.882159     1.241338  ...   -0.640376    1.717616   -1.703426\n",
              "1      2   0.688133    -0.318813  ...   -0.040592    0.387883   -0.961931\n",
              "2      3   1.721908     0.388303  ...   -0.079589    0.593564   -1.152426\n",
              "3      4   0.650453    -0.146959  ...   -0.265548    0.279305   -0.208944\n",
              "4      5   0.109805    -0.189683  ...    0.179497    0.314474   -0.486160\n",
              "..   ...        ...          ...  ...         ...         ...         ...\n",
              "831  832   0.092474     0.106150  ...    0.069725    0.208504   -0.096100\n",
              "832  833  -0.072233     0.119163  ...    0.223271   -0.203688   -0.038956\n",
              "833  834  -0.132909     0.104212  ...    0.270046   -0.138641   -0.049895\n",
              "834  835  -0.079618    -0.154153  ...    0.168985   -0.259337   -0.073086\n",
              "835  836   0.239000     0.008422  ...   -0.421967    0.063392    0.015846\n",
              "\n",
              "[836 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWjtGE-_XZ35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "0e2ae3f0-12ac-42dc-ae5b-c0b63bcfd809"
      },
      "source": [
        "node_label"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>832</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>833</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>834</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>835</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>836</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  5\n",
              "0      1  3\n",
              "1      2  3\n",
              "2      3  1\n",
              "3      4  6\n",
              "4      5  4\n",
              "..   ... ..\n",
              "831  832  3\n",
              "832  833  4\n",
              "833  834  5\n",
              "834  835  5\n",
              "835  836  5\n",
              "\n",
              "[836 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRUU-06HfTR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "'''\n",
        "先将所有由字符串表示的标签数组用set保存，set的重要特征就是元素没有重复，\n",
        "因此表示成set后可以直接得到所有标签的总数，随后为每个标签分配一个编号，创建一个单位矩阵，\n",
        "单位矩阵的每一行对应一个one-hot向量，也就是np.identity(len(classes))[i, :]，\n",
        "再将每个数据对应的标签表示成的one-hot向量，类型为numpy数组\n",
        "'''\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)  # set() 函数创建一个无序不重复元素集\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in  # identity创建方矩阵\n",
        "                    enumerate(classes)}     # 字典 key为label的值，value为矩阵的每一行\n",
        "    # enumerate函数用于将一个可遍历的数据对象组合为一个索引序列\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),  # get函数得到字典key对应的value\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "    # map() 会根据提供的函数对指定序列做映射\n",
        "    # 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表\n",
        "    #  map(lambda x: x ** 2, [1, 2, 3, 4, 5])\n",
        "    #  output:[1, 4, 9, 16, 25]\n",
        "\n",
        "\n",
        "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # 储存为csr型稀疏矩阵\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "    # content file的每一行的格式为 ： <paper_id> <word_attributes>+ <class_label>\n",
        "    #    分别对应 0, 1:-1, -1\n",
        "    # feature为第二列到倒数第二列，labels为最后一列\n",
        "\n",
        "    # build graph\n",
        "    # cites file的每一行格式为：  <cited paper ID>  <citing paper ID>\n",
        "    # 根据前面的contents与这里的cites创建图，算出edges矩阵与adj 矩阵\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    # 由于文件中节点并非是按顺序排列的，因此建立一个编号为0-(node_size-1)的哈希表idx_map，\n",
        "    # 哈希表中每一项为id: number，即节点id对应的编号为number\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    # edges_unordered为直接从边表文件中直接读取的结果，是一个(edge_num, 2)的数组，每一行表示一条边两个端点的idx\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),  # flatten：降维，返回一维数组\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    # 边的edges_unordered中存储的是端点id，要将每一项的id换成编号。\n",
        "    # 在idx_map中以idx作为键查找得到对应节点的编号，reshape成与edges_unordered形状一样的数组\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),  # coo型稀疏矩阵\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "    # 根据coo矩阵性质，这一段的作用就是，网络有多少条边，邻接矩阵就有多少个1，\n",
        "    # 所以先创建一个长度为edge_num的全1数组，每个1的填充位置就是一条边中两个端点的编号，\n",
        "    # 即edges[:, 0], edges[:, 1]，矩阵的形状为(node_size, node_size)。\n",
        "\n",
        "\n",
        "    # build symmetric adjacency matrix   论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))   # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
        "    # 对应公式A~=A+IN\n",
        "\n",
        "    # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))  # tensor为pytorch常用的数据结构\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)   # 邻接矩阵转为tensor处理\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))  # 对每一行求和\n",
        "    r_inv = np.power(rowsum, -1).flatten()  # 求倒数\n",
        "    r_inv[np.isinf(r_inv)] = 0.  # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
        "    r_mat_inv = sp.diags(r_inv)  # 构建对角元素为r_inv的对角矩阵\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
        "    return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels) # 使用type_as(tesnor)将张量转换为给定类型的张量。\n",
        "    correct = preds.eq(labels).double()  # 记录等于preds的label eq:equal\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):    # 把一个sparse matrix转为torch稀疏张量\n",
        "    \"\"\"\n",
        "    numpy中的ndarray转化成pytorch中的tensor : torch.from_numpy()\n",
        "    pytorch中的tensor转化成numpy中的ndarray : numpy()\n",
        "    \"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    # 不懂的可以去看看COO性稀疏矩阵的结构\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLtfimHc64d",
        "colab_type": "text"
      },
      "source": [
        "预处理数据集的格式，转化为GCN所需要的格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScqvErTlc5iO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "65fdd819-a441-4c71-cb37-d5a01001ace0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import file is  attribute of node\n",
        "node_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNodeAttribute.csv',header = None) \n",
        "num = node_features.shape[0] # Number of nodes\n",
        "node_features  \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.770633</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.582718</td>\n",
              "      <td>-0.062620</td>\n",
              "      <td>-0.238564</td>\n",
              "      <td>-0.715823</td>\n",
              "      <td>1.022981</td>\n",
              "      <td>0.181646</td>\n",
              "      <td>-0.260391</td>\n",
              "      <td>-0.542033</td>\n",
              "      <td>0.233139</td>\n",
              "      <td>-0.283432</td>\n",
              "      <td>0.117144</td>\n",
              "      <td>0.531449</td>\n",
              "      <td>-0.061221</td>\n",
              "      <td>-0.229529</td>\n",
              "      <td>-0.710597</td>\n",
              "      <td>0.288485</td>\n",
              "      <td>0.098411</td>\n",
              "      <td>0.189655</td>\n",
              "      <td>-0.511962</td>\n",
              "      <td>-0.263284</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.198719</td>\n",
              "      <td>1.086848</td>\n",
              "      <td>-0.256804</td>\n",
              "      <td>0.026246</td>\n",
              "      <td>0.093899</td>\n",
              "      <td>-0.142497</td>\n",
              "      <td>0.018819</td>\n",
              "      <td>-0.234937</td>\n",
              "      <td>-0.153423</td>\n",
              "      <td>0.425772</td>\n",
              "      <td>0.673969</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>-0.294841</td>\n",
              "      <td>-0.315873</td>\n",
              "      <td>0.552618</td>\n",
              "      <td>0.437355</td>\n",
              "      <td>-0.087442</td>\n",
              "      <td>0.536133</td>\n",
              "      <td>-0.366451</td>\n",
              "      <td>0.656990</td>\n",
              "      <td>0.802005</td>\n",
              "      <td>-0.016102</td>\n",
              "      <td>-0.669421</td>\n",
              "      <td>0.856563</td>\n",
              "      <td>-0.101192</td>\n",
              "      <td>-0.165583</td>\n",
              "      <td>-0.405513</td>\n",
              "      <td>-0.410948</td>\n",
              "      <td>-0.027975</td>\n",
              "      <td>-0.397765</td>\n",
              "      <td>-0.509387</td>\n",
              "      <td>0.280567</td>\n",
              "      <td>0.208776</td>\n",
              "      <td>-0.099218</td>\n",
              "      <td>-0.229354</td>\n",
              "      <td>0.232348</td>\n",
              "      <td>-1.035032</td>\n",
              "      <td>1.209129</td>\n",
              "      <td>0.368444</td>\n",
              "      <td>0.414352</td>\n",
              "      <td>-1.065435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.882159</td>\n",
              "      <td>1.241338</td>\n",
              "      <td>2.232549</td>\n",
              "      <td>-1.932836</td>\n",
              "      <td>-0.574318</td>\n",
              "      <td>0.168684</td>\n",
              "      <td>2.937954</td>\n",
              "      <td>1.398610</td>\n",
              "      <td>-0.260359</td>\n",
              "      <td>-1.253366</td>\n",
              "      <td>-0.997117</td>\n",
              "      <td>-1.355796</td>\n",
              "      <td>0.847303</td>\n",
              "      <td>2.447680</td>\n",
              "      <td>-0.573743</td>\n",
              "      <td>0.211870</td>\n",
              "      <td>-0.960837</td>\n",
              "      <td>1.536916</td>\n",
              "      <td>2.928266</td>\n",
              "      <td>-0.955619</td>\n",
              "      <td>-1.152885</td>\n",
              "      <td>0.548847</td>\n",
              "      <td>0.525967</td>\n",
              "      <td>-0.445511</td>\n",
              "      <td>2.999283</td>\n",
              "      <td>0.190644</td>\n",
              "      <td>-0.916012</td>\n",
              "      <td>0.877579</td>\n",
              "      <td>1.501392</td>\n",
              "      <td>-2.191907</td>\n",
              "      <td>-0.344929</td>\n",
              "      <td>-0.643345</td>\n",
              "      <td>1.808293</td>\n",
              "      <td>0.922850</td>\n",
              "      <td>0.823318</td>\n",
              "      <td>0.399970</td>\n",
              "      <td>0.415868</td>\n",
              "      <td>0.005898</td>\n",
              "      <td>-2.257816</td>\n",
              "      <td>-0.475472</td>\n",
              "      <td>0.610879</td>\n",
              "      <td>-1.345582</td>\n",
              "      <td>1.256365</td>\n",
              "      <td>0.962199</td>\n",
              "      <td>-1.093309</td>\n",
              "      <td>-1.647974</td>\n",
              "      <td>1.594285</td>\n",
              "      <td>0.584256</td>\n",
              "      <td>-1.293162</td>\n",
              "      <td>-1.844852</td>\n",
              "      <td>-0.580833</td>\n",
              "      <td>2.215265</td>\n",
              "      <td>-1.795820</td>\n",
              "      <td>-2.811482</td>\n",
              "      <td>0.229828</td>\n",
              "      <td>0.073673</td>\n",
              "      <td>0.256524</td>\n",
              "      <td>-0.438187</td>\n",
              "      <td>-0.097229</td>\n",
              "      <td>-0.740352</td>\n",
              "      <td>1.529813</td>\n",
              "      <td>-0.640376</td>\n",
              "      <td>1.717616</td>\n",
              "      <td>-1.703426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.688133</td>\n",
              "      <td>-0.318813</td>\n",
              "      <td>0.999343</td>\n",
              "      <td>-0.651598</td>\n",
              "      <td>-0.407260</td>\n",
              "      <td>-0.314173</td>\n",
              "      <td>0.781863</td>\n",
              "      <td>0.242623</td>\n",
              "      <td>-0.107326</td>\n",
              "      <td>-0.972608</td>\n",
              "      <td>-0.235344</td>\n",
              "      <td>-0.511505</td>\n",
              "      <td>-0.138459</td>\n",
              "      <td>0.775548</td>\n",
              "      <td>-0.218838</td>\n",
              "      <td>0.545011</td>\n",
              "      <td>-0.670150</td>\n",
              "      <td>0.699975</td>\n",
              "      <td>0.817735</td>\n",
              "      <td>-0.134486</td>\n",
              "      <td>0.054736</td>\n",
              "      <td>0.374199</td>\n",
              "      <td>0.183914</td>\n",
              "      <td>-0.104466</td>\n",
              "      <td>1.410521</td>\n",
              "      <td>-0.258784</td>\n",
              "      <td>0.201976</td>\n",
              "      <td>0.270087</td>\n",
              "      <td>0.458823</td>\n",
              "      <td>-0.055942</td>\n",
              "      <td>0.035193</td>\n",
              "      <td>0.027436</td>\n",
              "      <td>1.150917</td>\n",
              "      <td>0.318230</td>\n",
              "      <td>0.150467</td>\n",
              "      <td>0.657710</td>\n",
              "      <td>-0.184495</td>\n",
              "      <td>-0.571191</td>\n",
              "      <td>-0.604887</td>\n",
              "      <td>-0.231566</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>-0.379828</td>\n",
              "      <td>0.181649</td>\n",
              "      <td>0.533105</td>\n",
              "      <td>-0.220025</td>\n",
              "      <td>-0.499370</td>\n",
              "      <td>0.676739</td>\n",
              "      <td>-0.635464</td>\n",
              "      <td>-0.520307</td>\n",
              "      <td>-0.607030</td>\n",
              "      <td>0.455186</td>\n",
              "      <td>0.421789</td>\n",
              "      <td>-0.637401</td>\n",
              "      <td>-1.185999</td>\n",
              "      <td>-0.661417</td>\n",
              "      <td>0.244160</td>\n",
              "      <td>0.266237</td>\n",
              "      <td>-0.213120</td>\n",
              "      <td>0.249394</td>\n",
              "      <td>-0.382235</td>\n",
              "      <td>0.886608</td>\n",
              "      <td>-0.040592</td>\n",
              "      <td>0.387883</td>\n",
              "      <td>-0.961931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.721908</td>\n",
              "      <td>0.388303</td>\n",
              "      <td>0.947409</td>\n",
              "      <td>-0.787672</td>\n",
              "      <td>-0.362458</td>\n",
              "      <td>-0.536777</td>\n",
              "      <td>1.290188</td>\n",
              "      <td>0.130461</td>\n",
              "      <td>-0.550028</td>\n",
              "      <td>-0.882603</td>\n",
              "      <td>-0.284483</td>\n",
              "      <td>0.195723</td>\n",
              "      <td>0.287096</td>\n",
              "      <td>0.741486</td>\n",
              "      <td>-0.613376</td>\n",
              "      <td>0.241035</td>\n",
              "      <td>-1.183460</td>\n",
              "      <td>0.651605</td>\n",
              "      <td>1.076140</td>\n",
              "      <td>0.284106</td>\n",
              "      <td>-1.281615</td>\n",
              "      <td>-0.727382</td>\n",
              "      <td>0.383454</td>\n",
              "      <td>-0.202577</td>\n",
              "      <td>1.619946</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>-0.379740</td>\n",
              "      <td>0.251197</td>\n",
              "      <td>0.567685</td>\n",
              "      <td>-0.998128</td>\n",
              "      <td>-0.046036</td>\n",
              "      <td>-0.882844</td>\n",
              "      <td>0.740021</td>\n",
              "      <td>0.870464</td>\n",
              "      <td>0.363524</td>\n",
              "      <td>-0.268316</td>\n",
              "      <td>0.146103</td>\n",
              "      <td>0.422400</td>\n",
              "      <td>0.236998</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>0.789005</td>\n",
              "      <td>-1.078728</td>\n",
              "      <td>0.767913</td>\n",
              "      <td>1.031454</td>\n",
              "      <td>-1.020215</td>\n",
              "      <td>-0.265524</td>\n",
              "      <td>1.290493</td>\n",
              "      <td>-0.687332</td>\n",
              "      <td>-0.580685</td>\n",
              "      <td>-0.275520</td>\n",
              "      <td>0.351817</td>\n",
              "      <td>0.330816</td>\n",
              "      <td>-0.828862</td>\n",
              "      <td>-1.068953</td>\n",
              "      <td>-0.534331</td>\n",
              "      <td>-0.844828</td>\n",
              "      <td>0.119608</td>\n",
              "      <td>0.585099</td>\n",
              "      <td>0.437665</td>\n",
              "      <td>-1.466863</td>\n",
              "      <td>1.006176</td>\n",
              "      <td>-0.079589</td>\n",
              "      <td>0.593564</td>\n",
              "      <td>-1.152426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.650453</td>\n",
              "      <td>-0.146959</td>\n",
              "      <td>0.802256</td>\n",
              "      <td>-0.280101</td>\n",
              "      <td>-0.731831</td>\n",
              "      <td>-0.456279</td>\n",
              "      <td>0.775432</td>\n",
              "      <td>-0.465780</td>\n",
              "      <td>0.143515</td>\n",
              "      <td>-0.736436</td>\n",
              "      <td>0.108304</td>\n",
              "      <td>-0.252066</td>\n",
              "      <td>0.123692</td>\n",
              "      <td>0.359086</td>\n",
              "      <td>-0.440906</td>\n",
              "      <td>-0.286168</td>\n",
              "      <td>-0.400844</td>\n",
              "      <td>0.719755</td>\n",
              "      <td>0.256401</td>\n",
              "      <td>0.432786</td>\n",
              "      <td>-0.412044</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.045897</td>\n",
              "      <td>-0.033026</td>\n",
              "      <td>0.904498</td>\n",
              "      <td>0.185858</td>\n",
              "      <td>-0.173960</td>\n",
              "      <td>-0.415105</td>\n",
              "      <td>-0.231409</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.299792</td>\n",
              "      <td>-0.222521</td>\n",
              "      <td>0.539466</td>\n",
              "      <td>0.284991</td>\n",
              "      <td>0.444050</td>\n",
              "      <td>0.151308</td>\n",
              "      <td>-0.550617</td>\n",
              "      <td>0.110464</td>\n",
              "      <td>-0.113436</td>\n",
              "      <td>-0.128214</td>\n",
              "      <td>-0.150840</td>\n",
              "      <td>-0.427944</td>\n",
              "      <td>0.444110</td>\n",
              "      <td>0.421779</td>\n",
              "      <td>-0.314363</td>\n",
              "      <td>-0.818235</td>\n",
              "      <td>0.472876</td>\n",
              "      <td>-0.264762</td>\n",
              "      <td>0.096127</td>\n",
              "      <td>-0.077573</td>\n",
              "      <td>0.012599</td>\n",
              "      <td>-0.271649</td>\n",
              "      <td>-0.296263</td>\n",
              "      <td>-0.772448</td>\n",
              "      <td>-0.508725</td>\n",
              "      <td>-0.160995</td>\n",
              "      <td>0.444162</td>\n",
              "      <td>0.061997</td>\n",
              "      <td>-0.419543</td>\n",
              "      <td>-0.450393</td>\n",
              "      <td>0.573340</td>\n",
              "      <td>-0.265548</td>\n",
              "      <td>0.279305</td>\n",
              "      <td>-0.208944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>832</td>\n",
              "      <td>0.092474</td>\n",
              "      <td>0.106150</td>\n",
              "      <td>0.033633</td>\n",
              "      <td>0.018442</td>\n",
              "      <td>0.365770</td>\n",
              "      <td>0.026157</td>\n",
              "      <td>0.192186</td>\n",
              "      <td>-0.005236</td>\n",
              "      <td>-0.217018</td>\n",
              "      <td>0.257617</td>\n",
              "      <td>0.053886</td>\n",
              "      <td>0.152891</td>\n",
              "      <td>-0.122927</td>\n",
              "      <td>0.130594</td>\n",
              "      <td>-0.034565</td>\n",
              "      <td>-0.123680</td>\n",
              "      <td>-0.024969</td>\n",
              "      <td>0.082651</td>\n",
              "      <td>-0.083302</td>\n",
              "      <td>-0.207902</td>\n",
              "      <td>-0.276132</td>\n",
              "      <td>0.104665</td>\n",
              "      <td>-0.138261</td>\n",
              "      <td>0.175597</td>\n",
              "      <td>0.038199</td>\n",
              "      <td>-0.013455</td>\n",
              "      <td>-0.133683</td>\n",
              "      <td>-0.142527</td>\n",
              "      <td>-0.042970</td>\n",
              "      <td>-0.077926</td>\n",
              "      <td>-0.069572</td>\n",
              "      <td>0.217869</td>\n",
              "      <td>-0.284175</td>\n",
              "      <td>0.278465</td>\n",
              "      <td>-0.007701</td>\n",
              "      <td>0.073461</td>\n",
              "      <td>-0.324276</td>\n",
              "      <td>-0.116313</td>\n",
              "      <td>0.068643</td>\n",
              "      <td>-0.304055</td>\n",
              "      <td>0.147916</td>\n",
              "      <td>-0.450253</td>\n",
              "      <td>0.243640</td>\n",
              "      <td>0.015325</td>\n",
              "      <td>0.139896</td>\n",
              "      <td>0.207937</td>\n",
              "      <td>-0.348907</td>\n",
              "      <td>-0.123072</td>\n",
              "      <td>0.070664</td>\n",
              "      <td>0.271669</td>\n",
              "      <td>0.138426</td>\n",
              "      <td>0.055811</td>\n",
              "      <td>0.010685</td>\n",
              "      <td>0.009590</td>\n",
              "      <td>-0.026956</td>\n",
              "      <td>0.143835</td>\n",
              "      <td>0.134296</td>\n",
              "      <td>-0.018494</td>\n",
              "      <td>-0.094349</td>\n",
              "      <td>0.005062</td>\n",
              "      <td>0.147475</td>\n",
              "      <td>0.069725</td>\n",
              "      <td>0.208504</td>\n",
              "      <td>-0.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>833</td>\n",
              "      <td>-0.072233</td>\n",
              "      <td>0.119163</td>\n",
              "      <td>0.096811</td>\n",
              "      <td>-0.164411</td>\n",
              "      <td>0.278347</td>\n",
              "      <td>-0.201982</td>\n",
              "      <td>0.081316</td>\n",
              "      <td>-0.061770</td>\n",
              "      <td>-0.086994</td>\n",
              "      <td>0.235601</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.266871</td>\n",
              "      <td>-0.139913</td>\n",
              "      <td>-0.100954</td>\n",
              "      <td>-0.204267</td>\n",
              "      <td>-0.009871</td>\n",
              "      <td>-0.132975</td>\n",
              "      <td>-0.115135</td>\n",
              "      <td>0.074661</td>\n",
              "      <td>-0.066648</td>\n",
              "      <td>-0.195009</td>\n",
              "      <td>0.198070</td>\n",
              "      <td>0.041986</td>\n",
              "      <td>0.103717</td>\n",
              "      <td>0.212957</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.222006</td>\n",
              "      <td>0.069433</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.136644</td>\n",
              "      <td>-0.192403</td>\n",
              "      <td>-0.032700</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>-0.030231</td>\n",
              "      <td>0.076892</td>\n",
              "      <td>-0.572047</td>\n",
              "      <td>0.032351</td>\n",
              "      <td>0.019072</td>\n",
              "      <td>-0.175355</td>\n",
              "      <td>0.103921</td>\n",
              "      <td>-0.086620</td>\n",
              "      <td>-0.005460</td>\n",
              "      <td>0.071132</td>\n",
              "      <td>0.409461</td>\n",
              "      <td>0.037381</td>\n",
              "      <td>-0.081257</td>\n",
              "      <td>0.180506</td>\n",
              "      <td>-0.170608</td>\n",
              "      <td>0.123282</td>\n",
              "      <td>0.220902</td>\n",
              "      <td>-0.175002</td>\n",
              "      <td>-0.025833</td>\n",
              "      <td>0.079203</td>\n",
              "      <td>-0.021000</td>\n",
              "      <td>-0.098711</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>0.020249</td>\n",
              "      <td>0.162249</td>\n",
              "      <td>0.026010</td>\n",
              "      <td>0.033548</td>\n",
              "      <td>0.223271</td>\n",
              "      <td>-0.203688</td>\n",
              "      <td>-0.038956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>834</td>\n",
              "      <td>-0.132909</td>\n",
              "      <td>0.104212</td>\n",
              "      <td>0.050959</td>\n",
              "      <td>-0.111834</td>\n",
              "      <td>0.357196</td>\n",
              "      <td>-0.133188</td>\n",
              "      <td>0.143785</td>\n",
              "      <td>-0.049507</td>\n",
              "      <td>-0.147305</td>\n",
              "      <td>0.251885</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.199905</td>\n",
              "      <td>-0.070194</td>\n",
              "      <td>-0.153948</td>\n",
              "      <td>-0.143667</td>\n",
              "      <td>-0.036536</td>\n",
              "      <td>-0.038589</td>\n",
              "      <td>-0.034700</td>\n",
              "      <td>0.060599</td>\n",
              "      <td>-0.025647</td>\n",
              "      <td>-0.243201</td>\n",
              "      <td>0.148838</td>\n",
              "      <td>-0.037812</td>\n",
              "      <td>0.111142</td>\n",
              "      <td>0.232710</td>\n",
              "      <td>-0.081121</td>\n",
              "      <td>-0.017971</td>\n",
              "      <td>0.097984</td>\n",
              "      <td>-0.012490</td>\n",
              "      <td>-0.099915</td>\n",
              "      <td>0.092442</td>\n",
              "      <td>-0.163616</td>\n",
              "      <td>-0.015880</td>\n",
              "      <td>0.066135</td>\n",
              "      <td>-0.025328</td>\n",
              "      <td>0.081988</td>\n",
              "      <td>-0.507710</td>\n",
              "      <td>0.107737</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>-0.200301</td>\n",
              "      <td>0.093443</td>\n",
              "      <td>-0.091668</td>\n",
              "      <td>0.068706</td>\n",
              "      <td>-0.022480</td>\n",
              "      <td>0.297613</td>\n",
              "      <td>0.091121</td>\n",
              "      <td>-0.158395</td>\n",
              "      <td>0.178886</td>\n",
              "      <td>-0.117903</td>\n",
              "      <td>0.128856</td>\n",
              "      <td>0.285126</td>\n",
              "      <td>-0.158185</td>\n",
              "      <td>-0.052871</td>\n",
              "      <td>0.033910</td>\n",
              "      <td>-0.026669</td>\n",
              "      <td>-0.054522</td>\n",
              "      <td>0.224450</td>\n",
              "      <td>0.091497</td>\n",
              "      <td>0.057863</td>\n",
              "      <td>0.031415</td>\n",
              "      <td>-0.005544</td>\n",
              "      <td>0.270046</td>\n",
              "      <td>-0.138641</td>\n",
              "      <td>-0.049895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>835</td>\n",
              "      <td>-0.079618</td>\n",
              "      <td>-0.154153</td>\n",
              "      <td>0.295142</td>\n",
              "      <td>-0.150358</td>\n",
              "      <td>0.500962</td>\n",
              "      <td>-0.328006</td>\n",
              "      <td>-0.015192</td>\n",
              "      <td>-0.219420</td>\n",
              "      <td>-0.377795</td>\n",
              "      <td>0.257173</td>\n",
              "      <td>-0.064142</td>\n",
              "      <td>0.492528</td>\n",
              "      <td>-0.080609</td>\n",
              "      <td>-0.044967</td>\n",
              "      <td>-0.154126</td>\n",
              "      <td>-0.143692</td>\n",
              "      <td>0.100103</td>\n",
              "      <td>0.163071</td>\n",
              "      <td>-0.012129</td>\n",
              "      <td>-0.236396</td>\n",
              "      <td>-0.380849</td>\n",
              "      <td>0.415483</td>\n",
              "      <td>0.029281</td>\n",
              "      <td>0.239447</td>\n",
              "      <td>0.465136</td>\n",
              "      <td>-0.272850</td>\n",
              "      <td>0.155511</td>\n",
              "      <td>0.075649</td>\n",
              "      <td>0.088600</td>\n",
              "      <td>-0.196622</td>\n",
              "      <td>0.121215</td>\n",
              "      <td>-0.192279</td>\n",
              "      <td>0.005945</td>\n",
              "      <td>0.065828</td>\n",
              "      <td>-0.015154</td>\n",
              "      <td>-0.131507</td>\n",
              "      <td>-0.721625</td>\n",
              "      <td>0.131845</td>\n",
              "      <td>-0.092926</td>\n",
              "      <td>-0.340395</td>\n",
              "      <td>0.505998</td>\n",
              "      <td>-0.165514</td>\n",
              "      <td>0.192104</td>\n",
              "      <td>-0.060656</td>\n",
              "      <td>0.373459</td>\n",
              "      <td>0.290300</td>\n",
              "      <td>-0.260470</td>\n",
              "      <td>0.298324</td>\n",
              "      <td>-0.158419</td>\n",
              "      <td>0.281835</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>0.106287</td>\n",
              "      <td>0.018379</td>\n",
              "      <td>-0.148565</td>\n",
              "      <td>-0.095972</td>\n",
              "      <td>0.352697</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.086388</td>\n",
              "      <td>0.010307</td>\n",
              "      <td>0.194430</td>\n",
              "      <td>0.168985</td>\n",
              "      <td>-0.259337</td>\n",
              "      <td>-0.073086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>836</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.008422</td>\n",
              "      <td>0.717436</td>\n",
              "      <td>0.134245</td>\n",
              "      <td>0.373170</td>\n",
              "      <td>-0.059227</td>\n",
              "      <td>0.102996</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>-0.102515</td>\n",
              "      <td>0.433158</td>\n",
              "      <td>0.158489</td>\n",
              "      <td>0.265962</td>\n",
              "      <td>-0.011416</td>\n",
              "      <td>-0.031199</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>-0.496083</td>\n",
              "      <td>-0.352736</td>\n",
              "      <td>-0.257647</td>\n",
              "      <td>0.161620</td>\n",
              "      <td>-0.230452</td>\n",
              "      <td>-0.116733</td>\n",
              "      <td>0.159655</td>\n",
              "      <td>-0.278142</td>\n",
              "      <td>0.125518</td>\n",
              "      <td>0.496362</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>-0.318031</td>\n",
              "      <td>-0.112670</td>\n",
              "      <td>-0.150040</td>\n",
              "      <td>-0.268167</td>\n",
              "      <td>-0.436891</td>\n",
              "      <td>0.335615</td>\n",
              "      <td>-0.349596</td>\n",
              "      <td>0.179373</td>\n",
              "      <td>-0.294503</td>\n",
              "      <td>0.210135</td>\n",
              "      <td>-0.615341</td>\n",
              "      <td>-0.007536</td>\n",
              "      <td>0.045706</td>\n",
              "      <td>-0.705685</td>\n",
              "      <td>-0.061241</td>\n",
              "      <td>-0.744448</td>\n",
              "      <td>0.617036</td>\n",
              "      <td>-0.066599</td>\n",
              "      <td>0.155325</td>\n",
              "      <td>0.487323</td>\n",
              "      <td>-0.912633</td>\n",
              "      <td>0.122922</td>\n",
              "      <td>0.049172</td>\n",
              "      <td>0.452203</td>\n",
              "      <td>-0.023311</td>\n",
              "      <td>-0.192136</td>\n",
              "      <td>0.286585</td>\n",
              "      <td>0.194225</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.318570</td>\n",
              "      <td>-0.017260</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>-0.195706</td>\n",
              "      <td>-0.051707</td>\n",
              "      <td>0.267640</td>\n",
              "      <td>-0.421967</td>\n",
              "      <td>0.063392</td>\n",
              "      <td>0.015846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>837 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0         1         2         3   ...        61        62        63        64\n",
              "0      0  0.770633  0.010733  0.582718  ...  1.209129  0.368444  0.414352 -1.065435\n",
              "1      1  2.882159  1.241338  2.232549  ...  1.529813 -0.640376  1.717616 -1.703426\n",
              "2      2  0.688133 -0.318813  0.999343  ...  0.886608 -0.040592  0.387883 -0.961931\n",
              "3      3  1.721908  0.388303  0.947409  ...  1.006176 -0.079589  0.593564 -1.152426\n",
              "4      4  0.650453 -0.146959  0.802256  ...  0.573340 -0.265548  0.279305 -0.208944\n",
              "..   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
              "832  832  0.092474  0.106150  0.033633  ...  0.147475  0.069725  0.208504 -0.096100\n",
              "833  833 -0.072233  0.119163  0.096811  ...  0.033548  0.223271 -0.203688 -0.038956\n",
              "834  834 -0.132909  0.104212  0.050959  ... -0.005544  0.270046 -0.138641 -0.049895\n",
              "835  835 -0.079618 -0.154153  0.295142  ...  0.194430  0.168985 -0.259337 -0.073086\n",
              "836  836  0.239000  0.008422  0.717436  ...  0.267640 -0.421967  0.063392  0.015846\n",
              "\n",
              "[837 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ozu5hHltHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ce91d4a-4a98-4636-9710-6851562232d0"
      },
      "source": [
        "# 将词向量提取为特征,第二列到倒数第一列\n",
        "features =node_features.iloc[:,1:]\n",
        " # 检查特征：共64个特征，837个样本点\n",
        "print(features.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(837, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gypPFc0lb_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "feaebb16-7c0a-4a6d-a984-111fdca1138a"
      },
      "source": [
        "# 提取节点标签\n",
        "node_label = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNode_label.csv',header = None)\n",
        "labels = node_label[1] # 提取节点标签列\n",
        "labels.head()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5\n",
              "1    3\n",
              "2    3\n",
              "3    1\n",
              "4    6\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS1ZXvh74a2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2f87bfd-e8cf-4035-8985-3856b7ffd38c"
      },
      "source": [
        "np.where(labels)[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
              "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
              "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
              "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
              "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
              "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
              "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
              "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
              "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
              "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
              "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
              "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
              "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
              "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
              "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
              "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
              "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
              "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
              "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
              "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
              "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
              "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
              "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
              "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
              "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
              "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
              "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
              "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
              "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
              "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
              "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
              "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
              "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
              "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
              "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
              "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
              "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
              "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
              "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
              "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
              "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
              "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
              "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
              "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
              "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
              "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
              "       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
              "       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
              "       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
              "       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
              "       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
              "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
              "       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
              "       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
              "       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n",
              "       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
              "       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n",
              "       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n",
              "       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n",
              "       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
              "       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831,\n",
              "       832, 833, 834, 835, 836])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkxZFVTTefEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "86fd0618-c9e3-4ff3-a3e6-5a10cc04932f"
      },
      "source": [
        "!pip install networkx # 画图\n",
        "# 提取关系对\n",
        "node_relationship = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv',header = None)\n",
        "\n",
        "# # 创建一个规模和邻接矩阵一样大小的矩阵\n",
        "# matrix = np.zeros((num,num))\n",
        "# # 创建邻接矩阵\n",
        "# for i ,j in zip(node_relationship[0],node_relationship[1]):\n",
        "#     x = map[i] ; y = map[j]  # 替换论文编号为[0,2707]\n",
        "#     matrix[x][y] = matrix[y][x] = 1 #有引用关系的样本点之间取1\n",
        "# # 查看邻接矩阵的元素和（按每列汇总）\n",
        "# print(sum(matrix))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f5c4bc8307ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 创建邻接矩阵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_relationship\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_relationship\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 替换论文编号为[0,2707]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#有引用关系的样本点之间取1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxR3g67qNgHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install networkx # 画图"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yiryWH9Npol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "b441ab10-2ec4-41d5-9d8b-bcde8c90c2be"
      },
      "source": [
        "node_relationship = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv',header = None)\n",
        "node_relationship"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17409</th>\n",
              "      <td>266</td>\n",
              "      <td>660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17410</th>\n",
              "      <td>266</td>\n",
              "      <td>403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17411</th>\n",
              "      <td>266</td>\n",
              "      <td>404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17412</th>\n",
              "      <td>266</td>\n",
              "      <td>663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17413</th>\n",
              "      <td>266</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17414 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1\n",
              "0        0  267\n",
              "1        0  268\n",
              "2        0  269\n",
              "3        0  270\n",
              "4        0  271\n",
              "...    ...  ...\n",
              "17409  266  660\n",
              "17410  266  403\n",
              "17411  266  404\n",
              "17412  266  663\n",
              "17413  266  354\n",
              "\n",
              "[17414 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXVUiaWIMy_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "138b02e7-728e-4e33-fbe9-874eaae64ec7"
      },
      "source": [
        "\n",
        "# 提取关系对\n",
        "node_relationship = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv',header = None)\n",
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "G.add_edges_from([edge for edge in zip(node_relationship[0],node_relationship[1])])\n",
        "\n",
        "nx.draw(G,\n",
        "        # pos = nx.random_layout(G),\n",
        "        # pos = nx.spring_layout(G),\n",
        "        # pos = nx.shell_layout(G),\n",
        "        pos = nx.circular_layout(G),\n",
        "        node_color = 'r',\n",
        "        edge_color = 'b',\n",
        "        with_labels = True,\n",
        "        font_size =20,\n",
        "        node_size =1000,\n",
        "        alpha=0.3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9d5ydZZ33/z5les/UJJNeJiQkIfSuWLChrq5lXcUCa9tdfNR1y2/l5Vp2X4+L+ODjqgiiYmwrAj4riKAjIhACJIGEJIQQkkzaZJKZZHo/5ffH5/rOdc+QxARmQsr1eb3OnDP3udu57vv+fq5vj2Wz2SwBAQEBAQGnCeKv9AkEBAQEBAQcTwTiCwgICAg4rRCILyAgICDgtEIgvoCAgICA0wqB+AICAgICTisE4gsICAgIOK2QfKVPICDgmDE8DO3t0NoKmzbB9u2wZQu88ALs2QM9PdDXp/ViMb3bZwDL4Bn7Phax2OG/O5kw9ncc6nfFYnolk3rF45BIaL14HPLzobAQ6upgzhw44wyYOxcWLoTqaqiogJyc4/u7AgJeImIhjy/ghMbwMOzfD9u2weOPw5o1Irv2dujt1ffpNAwNSUhns16w22ssooL/VCG3l4sjjUM87teJxTwpxuOQlwclJVBWBvPnw5lnigwDIQacwAjEF3BiwAhu507YvBnWroWNG2HrVujogP5+EVz0ds1mtSxgYnE4UjQNOh73pJhISGMsLobaWpg+HS68EK64QppiIMKAEwCB+AJeGRjRrVsHv/2ttLmWFhFcf780uEzmxQI33K4nPhIJT5bxuN6TSWmGNTWwaBGce64IcfZsLQtkGHAcEYgvYOIR9cmtXg0rVsATT8CuXTJXmnkykxmt1QUz5KkBu46mFUZ9iSUl0gzPOw8uuUTvwUQaMMEIxBcw/jCi27YN/vhHaXPbt8Pu3T7oJJPRuhZAYf8fR+wGfg9sA8rcsk5gFrAI2AtsBDYA+4AhoNutU+D+7wRiQAIYcMuyQAYwI2zM/R8DctznuNvGlmcjnw0Zt34SKHLfdQMpt61tl+fWiQO5bnkhMA2oc/tKAQfdPkuAamCK+74KH969DKg/+iF8eYjH9cpkvJm0qAjq62HmzGAiDZgwBOILePkYS3SPPio/3f79MlmmUsfdF7cbeBpoAw4AuxBJlALNQAcigmFEWAcRacUQiZQg4upw/yfdugA9wKDbn/2qhPs/hScve4/jSa3QHSfttom7fcXdazjyOR3ZR9a9Yu67HHesNCK7rNuvnUsssr4RZTki7Hz33YDbdqpbp9/tIxeoQaTYC8x3x6sEzgbOY4LCwaMkGI9LI8zJgaoqmDVLGuFll8lUGsyjAS8DgfgCjh3mn9u4USS3YoVI78ABGBgQ0UWjK8cRUS3NNLP9eII7gIR6KyKULUCX+5zr3jNI0MeRMO9AmlQuEv79SLBn3L4KESENIy1qwO2zwK0zgIhhijuPFNIgBxDxzAHWIZKa7c6tGxFOlzvn2YiQdrrPGaDPvQ+6/YK0szqkaXYjQjPts8/9pmpgj/tcgrTVGLAYWAi0u+O0u21MW024Vy4i90L3eaHbT6tbfwoi0W53TmlgBjAJqAAmA1cAxS+6ei8BYyNJi4pg0iRFj151FVx+ubTDQIIBx4BAfAFHh+FhaGqChx+Ge++FDRvg4MHRpstxupUOZ4IsRAK9Da8V9SGtxDS6QURc1W79XW77IfdKuf9NC4rjtZxiRAa4z0aSxUjIxxDZGrnWIoLcg4T+VGATIqIFwFr3eR7whNum2p1/HiJdO7/F7ncPAmcgQhx22/S4bZLuGAn3u/MRaeci4h1EJtEGdx64Y693Y3SlO+5Kt/4+RHgFbhxqgfPdOT0XGbdJSFOuddek1+3HSH2Su047gK1u3KsRQRcCl7n1q9y5vWRCNI3QgmcSCSgtVYDMm94EV14ZzKIBR4WQwB5waETNl7/7Hdx3n1ILenr0HRyTb64REdJaJFRTQAve31WHBPoct2w3IoZNSDh3IxNlBm9yzEMaxhAigCEk/HH73IH3kw267YrdNvvcemaCLEGk0e2+r0Ka5KDbPsedX2/kNyXcu5FoizuHae58QKTSivfv9SANqRyviZYg8tznftMeRHhm9jzotkm68+tFpGd+u1y8qTUfkVTCrWsPeAxNHvbh/YZ9kd9iptMMIsputz/7DfnuXAaQ79P224bIuMydb41bt9ONXxGwecz5TMFrqhVu/QuRlnhE2L1mZvNMRgFTBw4oaOqmm5RUf9FF8IY3wAUXKFAmIGAMgsYXMBqtrfDkk0oxeOIJT3Yppysd5e0yluh24/1KXYjoBpEWcR4SmP3AdkRY5pfrR8KzHwlWE/jm5zKCMo2uG6/B9ETWM3PkDEQ629z6JvDz8ORWgCfBVrePFN6HFnfrF7jlKbywTyAtq9e94kjYd7ltzVeY737jUGSZkVuO+998dcN44kwjcsqP/OYk3hdo/rl2t2w6Iioj9wzSwroQacXwgTPT3Hg+jUjRfIdGqNOA5911s4CaUqTxdbmX7SvH7cM027mI5Fa67ee432KTgj63rxnAxcBS5E/M4ygRNatHfYPnnAMf+pCiRYNfMMAhEF+ANLgNG+AHP4AHH4TmZpkw0+mjCkppRAItqsmZOdCCPfKRUDOtYQ4ilm4kOFN4wksg0upCgi/XrRclESMD3PJC99kEdgEinkxkm3xEuDORJtnr1h2OfG++OSOTIXcOPZHPpuMm8YEipiEVMdrHl3bH2+vObVJk/R63LzO5GoGaCTcaFRrHE3omck5GkPYQ27sRYi4+SCbPbW+BMBbMY369XLdNrzuX8sgYzsSbjnPc50G86djILgevFVuEa6E7drH7rgB4J7rWjyJCrI/sI44mSnnI3/kqN27z3PtRwcyi5hssL4ezzpJf8HWvg3nzAgmexgjEd7rCfHaNjSK8zZthcFBEdwR/nWlyIG1uCGlPzYwmtDb3uQ9pQrlIALYjc1spnmzmI5Ndq1uWdvs1grHIyyw+2jHPLR9EArUEH4AyDCwBmpDJ0TTNOBLmU915dbtjdLjv8/FanAV8xNz6ZpatRcK5BU+OA26bOneMDqR1tbpjNyBNKYWEfCc+ItTSC3DnVOTGqR+ffmAm1bjbL3g/2yBes6p2v2+fO78cd7xWd6wavFZsUZ9pt14pPjXDCL4Ir8VVuXNKM1qbrnTLLY3CAofAB8hUut+SdN91umtW485hGfAOdB/ZMVa6Y09hdCrGTKQ9HlMahgXIZLOQm6vcwYYG+MAH4LWvDcExpyGCj+90wvCwijk/+CD893+r5mVnp8juMBGYhzNZFiESM4GYh4huHhLETyEBthsJ6UEkwAvxWlIvErJP4bUT89eZic3Mi0m8ZmTmvE48WZg/rB+fklDk1rEgkWH3fWfk2P14gstzy6ItS8rxKRAZvCaZdeeR536nne8g0kry3H7z3CvhzimNBHmBO5+MO89BvG/Ojp9w65t51wJtjARqEMkNIoKbgtfoDrrfNDMynuX4aM9SPGlWuPNJuON0jdlHhduHEbBpizXAWe4cOtBkx0zSJfjI2FY3ZuVubGwis80dpx/5AfvQJONM996K7p+L3XHWoijdJne+eW5sEsBvgYuQhngJYxCt2To4qOfg8cflF6yqkhn06quVKhF8gqcFgsZ3qsPI7re/hV/8Qp97el5c93IMoubLDJq970cCqxC4AAmcSiQctyNB2unW3YqIwUjOtAKL6GvGh8P340kxjc9rs/QDIw3TAIvcu5k4O9x3/fhglMlIcPfgTaIWSGNpCtGwHDPHmrZiZGZ+NvvfiMVy5GKH+N8S080XaeZU8KZIOw8i+7TzsPVsmZFrEkWLWuBMLSKCQUR6Uf/hVrffaWisk0gj7HTHzcUTXIU7VwuSOYg3tRpxJxGJ9iFtsgddyyr8/WFashG+Ebxp0Hl4QrUJTZE7fjn+ehXhNdMa4J+Ac4AnEcEV4s3BfW48Bt26ZSgydQ7SIqMFAQ6LRELFtqdNg7e+VT7BhoagBZ7CCMR3KiJqxvzpT6XZGdkdwmd3uIhLS/CejA8SqcEHpKzCC8FnkSAtQUKsBwmhBD5XzaqLWISh+d86kFCsw5vCzARnCdpmXrPcsl4kePORVmDBIQYjpLH/23s88r8Fi9QhYZt2+zPtz0jKgl/SjCYwM8OaP81I2tIk8tz4WjSlkbn59cy3ZQnpphUaMUbP3URxJvJ90o2LBar0u9+Rj8yCe/C+NTNlmpnTtLy4W7/Krd/njmX7PAtd+ycRkabdMY3YCtz47UXX2ojeJiGmjQ/iI1vz8bmPFmhk4xw1py7Ap7JkkFmzBt1zpe67CndeUQ1/oft9FSjIZx4ixGkcoRFpTo5SJM45Bz7+8aAFnqIIxHcqobUVHnkEfvxjWLUK2toUjTnGZ3e0EZdJJAhnu+U78Tl0ZUgQmabQjzebxfAh+GYeM7Oe+cVMM7I8vHLko9mH17py3H5Ne7KADfBEEPUFWhCIvYy4it1+zMdnIzHgllsgxxJ8bluu+zzsfqvlyllFF0tyH3K/tQBpv1344JN+JNxNKE9C/syn3H7nI015CHg9IpVWfHCPab4W2WkmyhQ+cAZ8ZRn7P5oKURBZtxxv3rTxsnfTbG2szVxsfsnFKIfxQXeOlah4QDeyBCTwhQD6I/szU6ldEwvoAWmgVej+68Nf06S7ZubrNRNwu9v3FLfMolavRlGgP3fX7CG3z3nuGKDr1+fGo9Btd6b7DWdwCFjlmOnT4Y1vhGuuUdJ80AJPCQTiO9kxPKzAlOXL4Z57VPjZglTGXNo/Z76cjATMMiR0tiGNDrdeC14QViAhb1qBmTMtj8z2bbl1Q27f+fjoyUIk+LuRVhHHh/1bLUsjTNPQzBdomoEhg4ilFh+9uA6fdL7Tnddlbj+rkAA34qtCgnAv3rfV587bAmeMyM2Uab85gXxSFvmZ687JyppZaH+JG7fd7vtyfEUWi9KM1uG0aEyr+2ljYO+2zJLJU3it2rRQ8BqQaatRmCZvx4v6K8FrfDbmRppVbrxNe5vsxiKqXZpmaZOd6KSnDEX57nHj0o/XFG1yY9ryVPd/C95HWYgP3rkMmWGb3Rhsceu9G3gjcBewBn+f2oQu143T+4CrOAKsYsyiRfCJTyhZPmiBJzVCcMvJio4OePpp+Pa34bHHlGweLf7sSM/ILhp92Yf3k1QgwTIfCbJnkRDahK/4UY4P7DDf2IBbnkICqBtPcFZzsggviM1PZAEkCbwWFg3oiApKMw0ORfZj+yhyL9OsDrj1y/Ch8c/gUypMy5iKCNmCMMCbKJsREXVGjmUC38yP+fhUCosqtRw+07pMG7TcQfMfWt6aEX++297IzUx8UZMfjE6jMDKNjo9pU6apxSKfzT9qRGapD6ZJRqNLx6ZE2Odo2kIcH3mawhOeaVTmY7Wgk2GkXVUgMurHBwc1Mzo304i3KHJeRk79kXVsEmLpE7+PjJlNWCz3cAE+GGohegbMh2mTv18hn+gsZAqdy5j8wXQaurqU17p6tfIB3/pW+NjHghZ4kiJofCcbBgbg/vsVqPLggyI8a8iazR7SjHkQn/Bs5sCl6CHfhhdmTwMv4IkoiwSAEZzl4sWQAO90x+pBwszC6s230os3bXUj4WSC1CIxLZjF6l5aInMPo6ulWIUPy/szQqlzv2+bWzcHHyXZhjf5pdzyuW4Yd+A1zSn4wIt97hyG3bIy9/uL3OdKt69epKmYX9OCcyYjAToPRR/Wcogow2PEZpQOUew+H3S/OxdNTg6g67AFP/GwvMdJ+AlLhdufEcluZJ5M4TVrmyAYEUb9i2PNkFZxx7axtJAEGitL4l/itrHcSZttD6Bxtftrn9tXqVu/x61jpeGGEXmCv/ZV6NpH01EG3fnNdfu2VI/FwB/Q9S1HZs42PEk24X2P70L3xWFzB+NxKCiAJUuCFngSIhDfyYLhYXU++PrXYc0aaXyRNIQjmTGH0cx3Fj4CcjoS/mvwJrZ+JASK8VpIJZotl+LJzYpBW0BChTtWn9v2Mvd/MxIeGffZtDzT+IxczHyW645nZb7M7ziIr9mZRkIshSfMPZHPNvM3s2TUTFjkzq8ICbxeJBwX4jXLrfiAj3J8FZZuPImeizTHq/CJ8ycSNgN/ROP5OjTmY4nTAoOeQ2bgTjSGVeja70WkbkQ6hHxsZoqNaqSWW2macIzRaR+lbh1LhreIT9x31fgE/ziaPFQj60MWXSOLAG7Da5XleB+yWQ/yI/uZhi//ZrmSCWQReCfwVuBufHrIM3hirXP7KkQJ9OejSd8hTWQ5OTB5MrzznXDttSEi9CRAIL4THRawcuONsHatNL6jNGPmIAEdR+kHg8jx34Jvy2PmoWK8/6ocCR6raWmdDywi0QIvEkhw1SESTSGSvBAJ3h4kfMwUaXl5Ft7fF/kcQzPrarzQbMf3vovhOyjU4CNHu/EdBirxwSxDeKKqwheVNuLtQQJ4LhJwJe41Cx/IcSr6AaKkOMv9f7jPB93rWXQ9OtzLJkrR6NVoOyUY3Q4JvGkVvK/XKtwUofvDzOZnonuuCd0Ti91xtjBaQytA17oYkbeZXAvdb8hDAUNWjq7dnWsBilJdgu4lizp9CE0QOlBA13Q0MehGZFmAyC/fneNVkd8LiOwqKhQJ+nd/B8uWqWJMwAmHQHwnKlpaFJ15550qJ9bXB7EYjdnsKFNmMxIcvYgkKtFDugCRVhbN3Pvd+vvd7ivwfiPrtbbI7acJrwFYTzqLDLTZupHQpMj/5fioyefxof3mF4yN+RztJZdwvyMf79exHLwqPBFOQjUc5yKiuhMJswb3G3a6/bW545S770xDzCLh9Qbg1cdwOU4HHM6kWozM4B3oPnoa3VMx/CQomsZhkaFmejZySOPLwZkPzcqkmXZvBQP249Mk3ozu3SZ8gYI0uv7mMzZLRgm+RVOZ23e5+96ClfLcMUybXYgI0LT6YnRv1AO34RPm6/F+3wG3ziwUkTsKOTkivNe8Bt77XplB8/OPNPQBxxmB+E40dHfDzTcrSnPXLuXfZTKHNWUOIoE/Hf+Qz3Pf/RFfSszMiwWISIqRdjXb7dNm3CVum9144WKJ5ObPigowC3Sxai6mmZlQycebGy1QxFr1WFSlCcZoQIZVXYkhbTUPmaLKkEZp/eFWu2PPdvuxPK0l+AjAqzjKROaAQ+JwWuIiRIL/g09DsZw8899G66lacEoy8oreF6DrboFEZkpfiA+MAa+1VeNLz1l9UfP5WjWhGNIAi/GFxXHH60TWD7t3i9HEahlwDbKiPIkP2sngO2qYT7ca+QPPwxdWGEE8LgI8/3z47Gfh1a8OJtATBIH4ThS0tsLdd8PXvga7d0MqRWM6/SLtLmrKtIjIK5C5xqISu1H/t714X1cFIkh7YK3c0wwkAFYisrQoSMsBs9JWNsM3c6d1JbAgCCspZrlXlutn9RTNx2dBKQvxEZRW79IiKacirW6f2+58t90zeHOYmU2r3O8/D2+2DDh+GEuKm9B91IVMpLuQpm7l4uzeMtOnRZ0m0GSml9HFrk1jtDQLi6Kd7t73uv2bphlt92T5f9G8xPn44KQYvvJNOXouDrrzeDs+QT/uftce/DPUgALB9uI7crwWaYVzkFl9BJYOcd558PnPBxPoCYBAfK80du2C226Dn/0Mtm+HdPqI2p1FZC7A54BZvcT1blm7e+9CM9jpSCMyP435ScxE1OW2yaKH3YoSm4nTAl9MUEXD360MWTEiOsv7K8ObU1vwJkjzI85AgtICUwrx0X02U7fIzCIkSMqQSaoBCTX77dOOacADJgqH0gzbEEF0oHvAomYz+EhS8PeWafuW0mLdOSrwOZ92H/S5/SXwGh2IvMxsalGrWfd/FT5Psh6RsxXNzsFXDTKzu0XCViHSK3bbLUVpFM34aNRqd27FqL7opYypEpNIqED2G94QTKCvMALxvVJoaVHAyh13wP79NA4Ojup6sAU9hKX4nDBrpjoPPUzWuDSDLx1m+VMWFGIFmCfja2k2uXVtBm0J4jFEMNOQkLE+btHQdvPHRHPRqhG57sR37rbtomHpJlxiKFLuk8hM9id8ugDuGBXuPFJI4LwHX4XjZICN19jPpxOOlgitlJlF39pYGQHmoAmcRYbmIzO3RW8WuO+tg4ZFjObjW1iZRgk+sd5K6tmkzvIizephk69eNDGzXMC3IHPoT4AN7rgxdL/OQJaJHve5CmmAoxrtxmJKfbjgAvjXf4ULLzzGkQ14uQjEd7zR3a2k829/W+SXzdKYTrMSPfi1iDx2udVnodnlY+gB7UcP7Hb0gFajB7mV0RX3B/Bdxa30VBvSHDvwydvg/XhWxT/aky2a7FyML2S81B23GZHUACLVTnybG9u35efV4PP6LnT724X3U05FRH4uqsQxhyPUVJwgHImkotVMDvc5M+Zz7BCfxx7naD5Hj3MkHOncXmkcziy6DWlelndprYnMfGmmUevpZ35kGx/rJWidNixK2NIwrBrMTHTvtuEnfgarOGRVbqyjRhbfNzKOtL2p6Lkzwgb5Fs90/z/lfpd1CClDz8Qs5K8eqRJjJtA3vxluuEFFsgOOCwLxHS90dChC8z//UwWk02kaYzFWZjIj/jszo1jS+RQ0u8wDHsEnhJv50BK69yCNzkLIS9AD24HXuPa7d8vVM0IxV7uZUaNlq0zDszD/GmROHcIHn2TQQ2/rG6GaKWqmW7YbX0w5jU9RmIIq7y90+5kTOafxwLEIflvXNNy4ow7/gMSIuf8PtT9tF498r7UP9TlGxq0X/Xvk842NbP9igrRzG/t7Yof47lDHeSW00mgU6Sb3qkREaP7ddry1w9JLLJXCci8LkLZVjKwUKXyBazNdViC/cS/SNC21wQjW9mc5iPnINDoZ3bu73PdleB+hmTfnAleiMbRk90fcbzM/vBH2LOCvGNNdPh6XCfSaa+AznwkEeBwQiG+iMTAAv/61Es/XrRsxaY4NWDGt5zKUE2d5eDORgOhmdB3GPe7dSmBNw89iz0adrQ/gK6OYJmIJ3uZfsbqWGURK1sjVggpiiMSmood9D5oxW25UAmlvSfSgT3XnYv9H876mIk0xGv3WAFzOxPvpTOPSuyehqJZmMAJIE2OIHLLkuBJaceJu2xRx0uQwSC7tlNHrphspV/cmyfBRfU6TIEGKJJkIOcWcuS1OmiQJ0iPb5DNAHn2U00U+g8RIEydDnCwZYuSQIkmaJCkSZIm96NcdDqLD2IuWHD9CPBQRDiHy2oCvGDQY2cbuXSs7Z8XC7bpa/c6piKCM+PYxOurUJo2W3lODzPcF7jy63XlZkevt+EpFtcBrkClzBpqUPoC02zLkX5/nfls30iyvQkn0oyJBYzGYMgU++EH41Kegru7YBzHgqBCIbyKxejX827/Bo4/S2NV1xICVGuT3uhjNFlehh9TKb+XhE7qtUr2ZMOvd8na3nwTyo1junSUMR81BFmE5DV9H0TqSm5myCx9sYr3X9uNLiJmJ6TNIsNzgvrPIOZtpn+P2UcEhcp7GEWYWg8PXncwQd+QnuhgmQYoc0iTIEGOAAjooY4A8BskjS5wMCboooZ0q9lLHcyzgeRawi+m0UkNqXHXUo0OSYarZzzR2Mp/NLOA56thLJQcppZM4afLpp5A+iughjyFyGSJOhqz79UnSJBxBJki/SGOMIqoZHg/zs5lFLWVmG7pfn0X3tlVYMY3NEC1mHg2YyUfPSTOajKXRPT0Q2ccw3j+YdNsUo3vaCiQUo8naOqQJGlkWokCri9z6G5D2ORvl+01Dz/5e910cEfFilHozKh0iLw9mzBD5XXttCICZAATimwgMDMCXv6xozfZ2GlMpVjI6YAU0M0ygh6Icmft6kLa2F9/7rg496I/g/W9WtmkGIrwWfOUKK2Zspscs3rQIviyZBQWUIr+EdRowP94weqAL8TUaLeG4HF+mbIk75j581ZdJiOiuRG1fxkNYHsoMGE2IjybIZxyxZZxYTxEn5UiunwKaqWMHc9jB9BOCyCYSSYapoJ1KWjmDTcxkO9PYxRSaKecgZXQwiYPkMEQRfW6UUiRGRjLrfGaZkXEe68/kEJ/HC6YJFiLSaEP5dQMoUGsQnw5j3Sfs3a6k+agtpSIH30UjiW+nNLbhsCXLm5mzCj2XO5H2ae6A6LNifsZc5Pf7S0R+d6FnuNkd11Io+tEzfjF6nhNAQyzG/Px89QW86SY499yXMnQBh0EgvvHG6tXwsY/RuG4d/ZnMKJPmgFulFt3gzciUmUWz2Di+C3W0l5t1R7C2Pc/ig0VKkVAwn4NFplmEHPjCzYX4TuWgh9NSDra4ZZbjV4P3e9QhcrWals/hG4tafc48xtdfZ2RmiEWW2w0rLS0+oqkMk3SGwGIGyKeFGvYw47Qgt5eLseQ4h6008Cyz2cYUmimkj3z6yWWQHFJOb85GiDA7anKTZfRkZDwJ8VngZ2gSacXS+/D1Pu0esXfLLQXvQjCNcBK+c3wfPufQfkN0smXl/Ca7Y1lRCAsCq3D/d+ODuKaitAYrq2auhCI04S3E1wi1snm1SBN8LzDL/H9/+7fwhS8E7W+cEIhvvNDdDddfT+Ott7JyYODPmjStsv4u97IZ4lBkXav0b0R2JXAr8jmYSbKb0U1JLZrNKlbMQA/kc3gTZ1QoWHpBCj2In3LLWxHJtqFZ7gtuvRb3XQ4y+VS43wQvzV93qKCO6DL5YeTBkscqwTBJBsljiDy6KWEzDazhHDaymE0s5ADVtFMRyG2cMJYUF/Asy3iahTxLJa0UMEjSGf2kFZpmGHOhPi+OUB0PbEbdFvaj+8S0QHMR2LNh3SisHNkk9Mz1omeuBD1LlWji2RJZP0re4H3fltJjmq+VYatFz91zbvsSfLUYS6qfjnz58xCJt+Oftzh65uPIdFqLcw/E47B0Kdx6a9D+xgGB+F4uLFrzy1+mcdeuI5o0p+IDVja69azGYAwRTB++weuZSHNajfKG4sC9bn+5bv/WJcEuovnyQA9gNvK9RZH1Rda3Emf5yCT5EXeun0UCoxoJCPBtbobduV3DsefWRUktqsWNzhf0+kSGOAPkczGsRCsAACAASURBVIAKttDAJs5gHWcF7e0EgPkZz2Ajl/AoF7KSybRQw36K6CWXQReJmh5VvNrex1MbjGqBaTQZa0YTTwt4aUa+wjS+wlHUdFuInqlat40FlVk5vyisypDVEy1Gk8C3Id/kDnfMXvT82bu17fqEO48mpBGuQfmsA3hrSwyZV0eVRCsqkt/v3/9dmmDAS0IgvpeKSLRm41NPsTKVepFJ06pEGNllkbnD8uksTNseomieXAGaLW5FD4wVgo62x4l2AjezZIPb705eXBbKOq2XIwJrd8e7Bs2A+/BtbH7mzrHInctk5Lg/GxHly2nH482VFouokIssMYbJpYdidjCDlVzE/byJDSwNBHcSYKxmuIj1nMtq5rGZGtrIp88F0WScYTQzUkVlvIjwUIW2QRPIWSggpRhVXXkYPQP57pVFk84kslpMdv9bOkM0FSgHX7YPt/1k9JxvwZOvrduCN6PmoQjQV6Fn/ALgduAe9Lw1IDfBC26bctTT8Wp8JRnq6+GLX4S//MtQ/uwlIBDfS8EhojVNy+uLrDYVPQyt6IbtQjPQg/hkbyvwnIvMkuBrY7a573PdukaAZlqxfL18fIPUpNvWziOGZrFWwcKc6IP48mIVaCa8FGl9qxHpzUbJ5GaWeSm0Mza60nLdLGR/iFw6KWMHM3iEy/gDr2cTiwLRnUKIaoavpZHLeJjp7KCUXnIYIMd5ycyECKMruLwUMuxFWuCT6Nkx4utB5PI88qu9gJ5dM2GaWyKFnrup7j2FN/NbOkU0eMzO2YK/EmjiW4YI0/oIWhpRDpIN5yMifsx9NxNNMJchM+52d04JVBT83e77IlDB62XLVAD77W8P/r9jQCC+Y4GL1mz8zndY2dXFUDY70gfPtLw4esgq0U1vJpUq9PBYbUDrbmCam5kuu/HEaEEo5jy3GXEcX0sQfG1Lc+yX47twJ9Hs0To0THPHmoaEAohA9+GDZQoRCb4BpSYcC6IEZ+erV5w0CQbJo4cSdjA9EN1pirEm0ktYwSy2UU0rBQwQcx5r0wKjn4+WBLchk2MrvvdjIbrPrfi6me0PIBJ6CEWNRtti2f1cgYK3QBpgH751l3UfGVsJphZfDcb8jfb8Wg5gtC5tEk0wz0eEuwqfttSNyLIKnwR/pvuO0lK49FL40peC/+8oEYjvaLF6NY1/9Ves3LaN57LZUX3w2vGBK1PxJFSArxO4FT109jAV4RPArQJKXuR/63ZtD7nVLLQHPx/fvgX8bLMapRfsReaZWhQddhfS+mahGeYU4JfueGe6/0fMKG4fVyACPxpYaLsRnfnp+smjlVq2M4sVXMIKLgtEFzAKZiKdyTZey+95M79hDlsppodchomPhG4JY/2Fh0I38qM/jPxnXej5slZCFXitcjG6//8HBY7tRs9gPr5LhJXzm4uCTXag1IQ+9709i9Eya1apyDo6mKC1ajRWTKIYn4JkbbW24y064Au7p5C2WoJkzQxElksTCc6sqiL+N38D118ftL8/g0B8fw4DA/Af/0HjTTexsrd3lEnT6iq04gNXLJpsMnqYnkVO6zZ87T6rMhFHxGgJsjn4QtK9jPbdgS/IG626YpGauW77anx/vjejGeweFEEKcqzPRU71Z5Fp8yv4UkvHAvsNmZH/46TIoZtStjKH+3gTf+D1NDEnRFkGHDWSDDODJi7hEd7CPSxmPXXso4B+onmFhiPliBoBbkImzU1o8lmOns/Z6Nk5iLeW7MA3vbWqSsOR/80MWoiv4WlamQWCRcuhWVCY1ey0CjNWezeN1wCt9m47IuVafKJ7KUp9eMGdTzQQpx9ZaD6Un6/+f7fdBvPm/ZmRPn0RiO9IeP55+OhHaXzsMVamUmxBDw2IcKz7uBFRPyLAYjRbXINmj5bvU4IvIm0kZw5z61htCbFD+BljoVtuVVtS7hg1yEwzxS3fjh6GKvQA/QPQiIh3j1tvAd5sdBB4B3pgjgbRGyWq4fVRRAu1rGcp93IVj3EZO5gZiC7gZSNqFr2S+3kTDzCV3RQwQIJhrHaqPTtHMoWO1QLb8UWkrZCDaYGT0ASyH1VpeRgfmWmvNHqO6922VljC/PdWCcZKo1mlI2utZe4Mq4Zkpf6sHGHGncMyZLXpQuR9ALlXrI/gFLf/qcC1QJWVPvva1+B97zvGET89EIjvcPj5z2m87jr6DxwYaRPUE/m6DZklbAY4hEjlPHRT/hrNzNL4pPBcfKdqawhrQSv2EFnejxGlEWAD6tFnJpgGd+y0+zwfmTPTyEw5jEyVQyhCswPfIHMxcpRfypEbt5oQiSaNa3mMIfLZxWTu4W38jjcF82XAcUGSYeawhdfwB97CvSxlLZPocLV5MiP3a+II+4gS4PN4f7T16zOrSREiwCdRhGgzclkk8YFn1tKoDD2HCXwe4G73nQWl2YSzAJGfmV6t/Vc9IjPL6bMgtjIUTb0QWXLWIuLOwVt3trttZwMfQlGg5ObC1VfDt74VTJ9jEIhvLAYG4FOfovFHP2Ll0BBpNDO0NkEZRmtj1g2hG/kNitHDsRlfGmnQvbKRd+uCAL6Zay162FrxxWxt+wzS0HLxFVH2uP+XoIfm93gzzgBKgG1BBHkF8H70YBxN+bCoGdOCUwbJYzf1/I438kM+wibODEQX8IrBtMGzWcXV/IiLeIJq2pwmKNjk7VD3fDcyaW5E5HYQX5bMqrCYCXQSeg4trcF6C7biq7SUoqjMUpROZGXULBneNEWz6hTi/fxz3HoH3Lu1XTLffj6aWM9Gz/0Wt/2ZKPJ6LV5zrEXP+gXuMwsXwo9+FAJfIgjEF8Xzz9P49rezcvPmkQCWYvdVG7phrXddHpptlaMHYgfS8Lrc+hYebTexRWZaJGcK7xuwCLOp6EHb6/ZpvsAOt24JMmssQzM8yzfahwhzp1vXwq8XoQfRAlWO5MeLRo3KjBlzDvc8WqnhKc7hJ1zNY1zKgZOqJWzA6YJKWrmIR/kY3+V8VlNCNwlSWDunw1WOOZQJ1NoSVfFiE6gR4GRgBfA4PqLaCK0Ab7kxd0gFIso2vDwoRtaas9157HP7Nh9fDl5jrEDEbObUWcBfuHMzIrZOKLWIEF+P5AClpXDddSHwxSEQn0Pjv/4r/d/9Lmvb218UwGLBJZYMXoRvhdKBb6RpDmfriGAVIsyUYtXezaFtFSWMdOyGtuaZ+W6dFHqYFiGzxjnA3fiuCV3oQTGTyWXIXGPh11Vo9ne4UmJjK6ekyKGVKp7mbH7FO4PPLuCkQpJh5rOZv2Y5b+Me6tlFPoMjHSgspeDPEaBFgprfPEqAi9HztMOt/3t8nV2b1Jaj5zeNCLMcaWS78OkNcbefCxHJ7UflzqwptRW6SOHlgVmQLkbuipVun3n4pPoSfBDOG1DkZyKRoGHZMub/7GenfeBLID5n2rzhhz+kL5UaZdYcQDdrH7qR8pBJEWS/70cPx1ZkUkxiwR664YbxTmora2R1+8zPZ1VVYmg22Is3hUxBN+wedNN/Apk4hvAd2K3fnZlS5qA+X2WRn3gxo9MSogEAZtIcJEEb1TzOhSznQzzNecFnF3DSw7TAq1nOeayillaSLu08WjEmirGRoNvRs1+OrDKz8P73/W79PkR8L6ACEO34whSVbpssmtiaOdMS5Qvdfi2YpgMRbQ/SAK2yTI1734jkw+vcemsRAQ8i7XGyew0gbdT6dc5FXSIunjKF+I03ntaBL6c18TXefjv9X/0qbNnCXZkMGWRO2IWPEstDN7YltVo1Bouy3Ihu+H34XJ1+fKoC6CatcNtaBKi1DbJ+X5a3Z5rgfJRE3olu6mlIa6tEFR12um3NVDrVbXcuR47StPBq9R9L0EU5qziH7/GJYMYMOGVhKRKv4g9cyw84g40UMDCq1RIcmgQfRcntbehZK8Hn0dokchKysuxBxLYWpR5Y8FuB+z6NnnerstSDT2VKIi3PNMbdbv0qt//Z+Hqjk5BsGkRyYI87j7cjq9AGJHM2ufOxGr6VRNIerr4avvnN09L0edoS3w0f/jDrfvlLMn19lKMbFXSTtSOncRwfdWUNLOP4Vj59yDneimZ/ZoawqDLT3PLwnZstRwf33SSkNQ4iE2oRvjnlRry/YQq6wachDXOj298UNJM7mr53pt0NkWAn07iLd/NTPsgWGoJmF3DaIMkwZ7CBa7mNt/Jr6thPzkjjohc/Q1ET6LNuWT4iwQp8FOhBt64FsGxHZkiL8kzgTac2QbZC8pYIb5WcZIXxDZwt2b3DbX+xO9Y2JFva0PM9F8mUeuQ3bEQ5vGkkO+qJpD3k5MAFF8APfnDamT5PO+JrbGyk/2c/466f/ITdw8NMRubBFehmXYaPmBpCN1sX3iZv6QZF6GYzf6Bpa9FIrBw0g7MQZ/MPGtFZ52aL2rKSRBe6Y63HB8lYR+hypJEWAa91r8PV0Yw20kwTI00eO6nnRv6RX/OOoN0FnPaopJXXcT8f5xbO5inyGWBsb0HTAl+K9jcF1eF8FJFWt9tXFXpm89z21gi3F1/+cBhf9CKFyDGBj+60Ck89bt8WQFONilecC/wANZlOoECYFneuZyH/4NmJBJxxBtx4I7zhaDN6T34kvvjFL37xlT6J44k7P/Qhdv7mN+xMpUZy22Yis0KO+78Q3wcviWzrA/g2JkOIfLrwbYEsoqsKX13dmoYMohva/HsW3GIPyTJ0sw6gG7oU7y80h7nNR62f1/uAv3bbjc1Z8mXDYJgYPZSwjXl8g0/zSb7Hk1xMv8rcBgSc1uiniI0s5adczT28lUJ6mEYzSYZdB3r/LOXh6/BakEq0nJk1ka1FpFSF18Ly8YXih/BFLCwYzqI8+xjdk9MiwC34zSbWpfiWYC2Mru1rlqRilE98EGl7VyJr0V40qV4L5GWzlO/fT/GKFVBbC2ee+fIH9STAaaPxNTY2svJzn+OhdevIIJIx4qtBN84CpD0dRPb5JnQjWhTnFkbb5qOdnc2cORvd0FbA1tIZLBfIyhqZtvc24C+BX6Fk2jh6YOrceaxDN/0UdKPXI//fGzh0lGa05U8feWxgMXfzLn7K1ewfKbIWEBBwOExhF9dwG3/Fz5nFNnJIvygS1LS/Z9Dz3IyIrwyvxVm3BWsnNuTWX4+e7Syy4JThUxZ6EEGqOJu3Gk1Fk/MD+IT5IrePXnxAnblZapAc2ejWOcvtYy/m7jhE2kNVFfznf8I114zLOJ7IOC2Ir/G++1h53XVs2baNF2BE12nDz9DiKF0gi28D0olmSAfd/5ZYbiHDVoC6Cl+weia6Oc2n14R3Zltkp3VNmIVajMxAvoMBZLroQyR3AG8ueTc+D29slKZBtQFjDFLA0yzl//CPIWAlIOAlopJW3sav+Bw3MI3d5Ea0QNBzvAM9o2uAp92yMvT82jRzrAl0PXKtHETPexrvxxtE5DeI9xUWIIJM4Ikxqv1ZtZlu910xkkdTkZ8xjSb0WaQdTnP7HUByZAgR9ceA84qKSN54I3ziE+MwgicuTnnia7z9dlb+y7+wZd++kfBgMyu2I7PhFHSjWJUWczjPQTOm1fj0hD34otJWduwMdDOnkEZmPfA68ZFZw+iBqEc3bykqK1SJHph+NEub77brQjPIduByRHyHg3x4cdopZzXn8w0+yyO8OgSsBASMA4ro5qPczAdZzkyaKKR3JILbIkJfSgDMdmRu3I3kQwmSJz34Li5WUCLu9leHyGsA3+aoAGmZ1qGlyr32u+PMR2RojW270WTbtMIhZFnKQ1avufE4S973Pq5avpx4/GjqPJ18OGWJr7Gxkf4//Ym13/oWWzo6Rups7sPn5Fl4bz66MXPwPbws/WCKW9aJTKMWnZWPyLLYLe90n2fiOyY0u+VmHpmCt/ubybIauN/tYwre7FGIEtHfga8eE4WZNNMkOEAlD/Ia7uQ9/I43MXjECpwBAQEvBTW08Nf8mKv5MfN5jjyGD2sCfYijD4BZh9woLYgALdXJYgxi+LgAC5wzc6VZk3DHqUMyJINknRW+sNqie936VyB5s8Wd41b8hLwGybGPvuUtzLnzzlMy3eGUJL7GxkZW3nQT6Yceor2vb1SdzXZEahaZWYZIZxciwnakbWWRSaAUmTF68S1HqvDRksVoxhZHQS1T3fI9yMwJulnNlp9EN/w0RHxvAv4bRV5Voxu1Atnk38CLQ6ttBpglRjvl/JHXcDN/zzOcRSflL33QAgICjgpRE+gMdpJkeIT8DqX9JfH1eMdqfyV48tuJZEanO06BW9+e6qirxUqjHcC3TbJ84dLI/9OQD28zvvNDO4pFsLSHK1Dk6WYkb84F3gLUxWKUX3QR3H77KZfucMoRX2NjIyu/8hW2rFhBRVploHfh2wZZp/MydEP1o1ybDrxj2EoETXPfH0Q3kpUis0isPHQz7cb7+qLJ6xWR9euQBtiNbrpBdw516IYrRjecVYW41J1rFFbcupMKfsk7+TafDjl4AQGvEMwEei3fZyY7yGVwVJNc0/7WIDlh9Ttz8AEqY02fW5C8svSnyfi+f9H+gNYP0OIV9uHJzqJBS5D2lkFmT4tJMJlUhdIe3gv8CBXGKEak14lkTVksxvsWLqTq618/pdIdTql0hsbGRlZefz1bVq6kJ5MZKdvVgS52HUodOBORD8AqNDuzjukWqWndlK1dUCW+0koGmQkuwjeFNC3QTKpWIqjFHb8O3Ww9iFg7ERFa/c4lKOF0JtL2xpKe6n5WcBsf5cP8mF9wNW3UkDliA5aAgICJwjB5PM4l3MW76aCM2WynlC7iLq7a0h9SqP7mdnx5s35EVl1I3pQhk2QdPm+3F5koTauzlAfrNpGLnwzn4eWVBb7UII3O/H8D+G7xhYgYy93xf4Um8JXuXKyYdjPQ2tpKzsqVTJ46VZ0eTgGcEhpfY2MjK1eu5Lnbb6d527aRBNBKREoF6Kaah69u/iwKWnkW3VhxvN/P2gVNx5sVUojEzKRhPr49+Py/IURs81HOzEHgdrf8AkRqj6Eb1MKRX4e0POurNYmxve+gjzx+yXv4N/6D5sOWmg4ICHglUUkr7+Un/DM3MJmWUX6/Zzn2wJftyPfWim/8bCbOEvey1IcCvN+vBcmrPDTJX4Tk1FYk6wrxpRfzkDxrQSbQWnc+S93/A4g8k8DH6+uZfPPNcNVV4zJeryROeo3PSG/L8uV0b91KGpFXDrrAlUjzMtKLAU+gUj7WM89KBA2jm8gSRC3KM4XvqjwEIw7tFN5PCL6bulVoeQKfB1iCbi4rLbTQvSa7bbrcuZbib/I0cZ5hMe/lLm7hOrpHlZ4OCAg4kdBPEau4iO/xcfLoZSEbyGN4pLj9EvR8W/sgC1IZ5NDaXwe+0IX58axDTC7S1lKInArRxHkOPlE+7T73IDmWQdalGNL8rPD1GYhwO5Esynfn1ed+VyEizkxXF2f94Q+wdCnMmTOuY3e8cdIT35133slzy5fTs2XLiNZm5cLyUcrAFPe5GdnbH0IXEnzBaOuCnsQ3hS1H5cMmoZp4A/gbbjq6kc9FZtOk297yZwrwEZ1WcmgIT3CXIUK+HJk2G9BDIXNGnA7K+Aaf5lp+wi5mjuOIBQQETCSGyaORN/E7ruRsVlNNKzGyI6bPKkZXfrGguSJGV35pwOcCViDZ0I0nwyy+6L2R3Vx8/p9Fgg4juTUZHyxnqVjnIlnUimTmbCQvDyJ/44Db92aUs5ju7aXivvsoXbCA2IIF4zVkxx0ntamzsbGRn3zmM+zZsGGkPJiF+eehWdMs97kLVUb5jfts/ed68aRnVVMmIxK6FM2UHkAzsgy6EZcgYmtx285ARDrkjjcNn4ezGd/WyBrZng98KvI7LF8nTYwh8lnFOXyOm1hL6JgcEHAyI48B/pkv80luppwuYs4r342S2I828CUHmT43ouovQ2iCbuZKs0xZQF0+8tn14hWBHLdNKV5LnIH3J1oHiC2ou8M2NMlfhJSDA27bs4G6igoWfPzjLPu7v6O+3pq1nTw4KYlvxKf3i1+wcePGkQjJQff9FGTbXohugL3oxvglTmVHN0A+uuA96GYrRBd2htu2CLgTEVwWkd4ifMNHKxBbgG7EGuCN6AZ6wH3XjWZcDcj3dy4KirGZl6UnDJFDE7P5JtexnGtDLl5AwCmEs1jN1/kM57hC2AkyI+T3EEeX8/dHfO9Pc5lUuPUtIM+a29YiLW+vO751iB9yyychjTDHHacM+Ijb7wpEcvuQ6fSTiIQ3IvmZduvPrazkXV/5Cks/+cnxHazjgJOO+EZ8enfcQeeGDezCVy/oQaRnte2sz9UzKHqzE6/2J9FNYj3zCtENVAGc55avQTeONaSdh+zuZfikUSsqOxkR5eXIZr4OVWU4gAjzXEa3DopWZWilmh/wN3ybT4V6mgEBpyjyGOBDfJ+/57+YxXbyGKKH0YEvR5Pz9ydkJu1EMs9iEvLwpk1LaSjAd45Jue1Mw0wi+ZNBMvPteH/gQZRT2IBk4z5kRt3mvitGCsKMSZP4+N13U/OqV437eE0kTirii5Jez4YNgC6I5bQsRJGTSbzD+PfItzfsXtbkMYEIy4iwCJ/fV4IIcz26ETJ4v94ZeGLrRObTTqTtXYLXBvcjUqwHPozMo5ZtZ6QXojUDAk4/1NDC3/JNruX71NBKnOwxmT434tOkOpGssb595raxAvlJVIZsOpKDO/GFrq00o2mVls5l3eJ3INeP5RG+1p3fBkTI8912F9TU8BePPALz50/AaE0MTiriu+GGG9jw05/Ss349uNPuQoOfi3xySXSD9KLE9I34ho+WxB6LvArRTWImzn40+9mPLry1CImhm+f97pirESmW47XCuWh2Zq1LKpAGOMudvw10ihjrWczfcguruXA8hyggIOAkwRR28SU+z3u4g0IGRxLe/8SfN33uRTLqGWT+tOhx8/n1I2IEyZ+LkZbWhVKqDuKtVtZSLRfJsQIk66wcWjGSgcuQVrofxTHMwRfU/2RDA/PuuAOWLBnXMZoonBTEZ5reQ8uXc2DrVgqyWbIc2qeXRer4I0hjs0oqufhIKEt5KMAnfVrU0yC+dVAHowmyxr0GkRkgB5kC8tHNYKkIy9As6wL3Dj5FoYNivs11fJUvBD9eQEAA5/I4t/ERzuA5evGmz+fxlVbGmj6LEOksR9anNnzSei6asOe59wEk296NZGMbSqi3nL4U3k9o7Y1q0AS+B5GvrfOCW28uigZdjyJCXwN86tJLKfnOd2Dx4gkYpfHFCZ/OMGLevO8+9m/YQDqbHfGxDaILWoNmOvvRTOhJpI5bFRVr6lqIyM4qqyfxeXkxt611U7dQ4HJ80mgNfvZlSe6liCAH8GbSWUhrnOWWmWlzAwt5N/+PX3A1aZLjO1ABAQEnJZqp54f8DTW0sIRNzCTFEjRprkfa1QwUtzAFuXOW4Su8WDujbOR9EpI9OfhWZ034VAfz+1mh/XJ8BxrrLbrf7WuqO88sIssDSA6WISWh3R23e+dOZj/3HAVXXAFlJ3bO8Qmt8Y2Q3oMP0vPoo+xLpciiWc8gqi5gGpcVhX4IXYhet04O3mZejK+haehBRGg9rvLw9TqrkD/veXSjLEOEaDOlQbff7eimeRVS/2cg04L1z+slj5/yAT7Lt4KWFxAQcFj8JT/nBv6RqTSTIMt2fMeYOkRQWfe/VXSZhMyXO5AJcy+SYeVIrln7ojiSfbOxmr+Sj9OQudSKdMTRhL4fycx5SMbOQabYHkTIaXywjMVILIrFeOdFF3HZAw+QLD5UX5kTAye82pFsaaF2zRpqU6mR/JRJaOaSQdGaNkt5Bh+MYtqc9cYrxFdWOYAusNXvzOCDUqzMUC66QSrwyaAH3bLz3OedyGyQi8ys/4Q0UPDRUnuYwj/yde7ir8Z5ZAICAk413MX7WMs53MxHuYjHqWSIRajS1MOIqKKmz1o0AV+AZOPzSMbtRIRnwTHWTaYNkWENPqVqM7JaRatSDeBLMA6i2Ib1SE5aLMTTeFPoPHeMDdks6x97jMsuv5xzvvAFFi5cyPwTMOjlhDV1NjY2svL++9myfDl9XV30IrJJollGESKWTmSvtk7pljfXhy5gNfK15bjv+9BFjlZRtwtsiaK16ObqxxdsLUJmhlxkwmxCMyur2fkeGAlTyQAp4jzBebyLX7OCkyvUNyAg4JVDO5XcwfvIkuE81jGXwZFOMpPRZN0C+qyaVAmKczDrlHVYtw7tlkJl6Q7WV9TSsBJ4mVng1rWiIAeQzN2EVySa8UWyq1Df0LlIXm4DmlpaiLe1UX/eedTX1xOLmUpxYuCEJb6nn3iCJ2+6iZ2trexDA23tgcqR3dmqnLfi81qiNe2KgMXIXNmGD1qx9IVufCdjq9yShwhuCO/s7UEa4yx0I3UgB3QKOAf4W5SjB7oResnnR3yED/ILWqmdgNEJCAg4lZEmycO8hkZexwU8xkxa6UNuFZNveUgu5SAi7ERyMe6WpfH9+ybhm9pG07qsZVoJko0ZfJ3iXPfqRf6+NEppqHDHWuD2f9D9X+SO0ezWz925k56+PqYvW8akSeb4OTFwQvaVX7FiBb+4/nrW7dlDL94/NxsN9sV4rawJ+fcsGCUf3RAlSCUvQWr/MN4MatqfXfx8RHb1yE+3FF+rsxQ/e+pDTRvr3Pqvcy8L4M0Cu5nM3/BDruOW4M8LCAh4WVjLuVzCau7iw1yAyoW1I7fOJkSEzSiYbw8iorPwNYTnIdkXbcCdg+9C04wsV+brq8ErDXlI5lkgIW7/5issxVeMAVWkehuS1XGgKp1m2n33Ublz53gPy8vGCefja2xs5Bdf+AJ7t20bmY1YlYGZSDU3e3QnPmcvg28ZVIJmODYzGUY3RR+aIcXxTWWtuHQW1dBciOp59iJNMYsS4a06zB+R6l8NvNWtY1GjaziT93M3Wzm1uhUHBAS8chgkn4/zQ55hMf/CvzKJwZFKL90cutKLr47FpgAAIABJREFUTbnXIItYD/LtmTbX716W8N6JiDKG5OswIr98fO5zPyLcBvf/CnwwTDsq79iA3E5Wlaqvs5PUpz8N99wD06dPyPi8FJxQxNfU1MTPvvpVnn/8cYoQeS3Ad0SowwhGmt52dFGH3fIct9409z6MZjMtbv/V6EJa7c18dLGtH99OdHGb0MD0482jVsJsEkqUfx+KcrJUhd/yet7Dr4OWFxAQMCH4Np+lizK+yj9zBgdGKr0UIdlnEemd+Ijy7fgIdEtrKMP7/AYQKdYiWWrtiszCNd3t06xl+xDpmeZo1agS+KIfre4YTyESHHjmGV57zTWcfe+9kH9iyMcTxseXyWS48bOfZfVvfkN9Os0SRGA7UdLkLnQRn0KzFGubYXkreegiWVuOdnQDHHD7t7SGDL6+3ZnulUazpEp0Q5gZtASp/nsZXd3lbOQ7VE5fgtu5mg9wd8jNCwgImFA8w9nspp4LeIoGOkgj+ZjAR29aj79yfNEOq+bSj2Sg1TK2VAfryl6M5GgO0hCtR98Ut69uRHBDbh9tiATnuX1tdN+ZJDQf4dM7djCro4PaN795QsblWHHCEN/qe+/lx1/+MkMDA8xCLTW2uvcYIqvd7j0PmS4Puu+SSBu8Bl+A2nrfxfAhvaad5eK7rffiHbxmEkihm6YM336oDGl5lwJvRqTYTx7/xd/zGb47UcMSEBAQMAobWcwmzuBc1rGI/exDRTv6kTyzyPUmPKHVo4m69fRLIS3PkuBN87Oan2VIJg64ffXi+4pa3WNrtQYizLRbVomPuI8hS9uebJYtTz/NnPnzqTnzzAkZl2PBCZHAnunr41NLlvCHrVuxlMcsIp8KNANJI2duCxrU3egCJ9FFmoOcq21Izd+KZh8W/dTN6BDdJL6CSyW6kL34Aq3WkWEAaY/nAP+O7+N3gDL+nS/wbT47/gMSEBAQ8Gcwh+f5Hh9hBo9xL8qrM83PIj5NNnYha1kLMo/2ojSEmYj8tiNZV4LylMvxRbAfd+uXRZZbsIy1TLoSL5tNYTnozsd6/qWAhRUV/POTTzJt7twJGpWjwwmh8f3fv/gLfv7kk3Thg1bykKmzDpk59yBy2ofsyP1u20K3zlvwgShb3DrWZHYfPhUixy1LoAtu3/W7Y9aiC9qGzAUl6AZ5IzKjqhbodD7J9/k5H5qQ8QgICAj4c2inkv/m/SxmK+9gE2Vk6UEBgNVIbpXiq1U1IwKymAcr21js3k3bq0MkZ3EOnfiCHNbmyKI9C5DJ8yAiuzYkqzvwCkTSrZMCdgwM8Ng995A7cybxeJzKysoJGp0j4xVPZ7jpM5/h1vvvH3G6WnHVcryNOgcNYBPytw2hE7dWQQVoljEDXZB8RGwpdAGsFNlS5Jub7Y49gNT3IkScr0EX/gV08UpQ3so30IwmA6zkHN5CI/dz1UQMR0BAQMBRY5B8PsLP+Tpf4mySnIOvHWw5zVlEXpcg2VmBZGoKxVBsx1fEMpdSEkW4W6pCvtvPQSRvzXJm9ZGto7spEoWIgCuRjJ6MmnjnAs83NfHgd7/L/v37yWSsfP/xxSsajXHzN7/JD265heFMhmmItMxRuhTNPNqQWt2F7zqci2Yz1iV9J4z0s7Ii01aV3NIQzkAXwIpOD+JLmlleXzUiugMoqOVa5NOLI9L7H97MB7grRG4GBAScULiBz9NGJV/i07QwyEOI4Kwwv/p/KkhlK7KmbcMHvFjunqV/pZBrqRLJ2wSSmT1ufxciv2IXsrAlkbWsxn1e5M5rBUqBOIhyoBPIEtf04INsveIKLrzwQuLx469/vWLE98ADD/Djb3yDeH8/l6CBXuO+a0ZkZD2phvHqtnUbrscnUIK0QfO/WQRTHKnxs9AFsTI72936RfhOCmeh2cvv3HaX4kkvDfyQD/IJfjTu4xAQEBAwHvgBnwDgs3yOAXp5Ck3+oz6/EuA65LeLIxI8gJQHK0FmRbDPRT7AS4CfIxLcihSR55H87EKEavEUFUg77EME14WvGGOdbPqB4eFh9n7nO3S8//1U1ddP0IgcHq+Ij6+pqYnv3Hgju1aupDabJYmSMffhW2JYE0TzyW1Hg2kJ6tPQhbOQ2/1ogK10jyWdVyOSrMJflJ34/L9Ct91r3DZdqAvDe/H9+n7IhwLpBQQEnPB4mnNJU8I1PEolQ/Qhy9lYn998JDM78SUbQfIzD9/BvdIta8D77CwOI4bvT5rBt2LLQ5rh024fs5ACsh+5kZJIJtPZSXbnThJLllBVVTVBI3JoHHfiy2QyfPOb3+ThH/+YwYEBYmj20I7321WimYHl6m1HsxILl52L1GarDG7OVQu13YWv6TnZfU7jbdrN+Arn3YjsnkXFrichf14Duph38w4+ws8nbkACAgICxhFruIAhinknjzDMEPvwOczRPL/9bv0cfC6fycW0W6cTyeMqfLBgIZLXCSQnretNn3tZQe0d7lgz8XJ5AK+AZID0tm2kq6uZe/75JJPHzwB53I2rd911Fw/fcQfDXV3koMHoRYO1CAWemL3YGhyappePr59pHdUL8DOafqS1ZZAafg4isl5EcFbE2ho1zkbEaTl/eYhEaxFJ3sub+TA/m7jBCAgICJgA3Mx1fIWvMZXckWA/Iz/L81uMtLFF+GbcfUhWliB5ewCZRVtR8GANvouNxT5UITKMITLdjw9ONB9hHMnnQSRr97l97x8cZN3NN9N38OAEjsaLcVyJr6mpif++7TYObN5MDr4mXD3S4qaiwZqC7/6bQANnhaot7LYTDXgDulCD7sfkIi3vcnSh9uJzUMwxW+LWy8FXNz8TRXZeiwjx91zJ/+LmEMgSEBBwUuIHfILv8p/MJId9SP5ZSlcZIrbLkSwtg5EcamvtZontLW6dZrePDnyiOu69GMnrbhQVWonkea47Ti2+OlYSX1VrPbBmzx623nTThIzB4XDcTJ2ZTIYvf+ELPPqrX5EeHqYfDWIxIrBzUTSnNVHchb9QSXzCeRG+LZEFumxFtmPrs2frNuHrclojxqlI2ytx23Yi1fuTqDJLKfAgV/DP3MRWTrwGigEBAQFHi2e4kAoSzOFhVpNhgBdXd+nBV8CywtUD+FzpAeRu6nCvYiRDC/BNwHMRMZpW149kbBlSJNqREjLstu91+81zy7atW0fdkiVMnj37uER5Hjfiu+OOO/jejTfS29lJJRps6/abjwjHOh08jU8gt/Jilu1hBVmzaFAPInIzza4IXZAet98OdJFt5nE5qrVp2l4LUt8/7pat4Syu41Y2sniihiIgICDguGENlzGHThbyJLvJjqQlDCO5OAO5ivYhuTvstosGGlqZM2uEOwXf2SYPyeF2fF3QAnyRkGZ8o3BbZla6Wnf85uFhVj35JDv7+6mqqqK6unriBoTjlM7Q1NTELf/1X7S3tlKFfnASDUAh+uHDKCS2A++ATbrv89FAZfAmzTY0Y7AL8Tga8AX4ENsSNLAd+ARNqwSzw61TB3wQEeYLzOQD/CJoegEBAacUvsXX+SfivJGv8wLZkfZseWjCX4eIqxvJxQNIJhfgNTsjyiKkFVrASw+Sy5VI0ytEctiiO02W1yFrXS2S9bvxxa5TwI7t23l6xQoaGhpoaGiYUM1vwnXKTCbDrbfeyvNr15LMasCtukoeGsQ+NKhb8S2HrFP6VKSRVaMBLUWzjXmI9FLAA3hf4XxGd1owdd3MnS8gjbIJqdznAa8GdlPHO7knkF5AQMApiRv4Giv4wEhC+qGCXRbh/X8ZpKXlIPIbBDajqlZt+DZGe5EsHUbKSA6S1xlEpmZGLUMyuggRrOVat7r/BzIZ9j3xBFu3bp3wii4TrvGtWbOG3/3mNwz19lKGfiT4HJCUe21Hs4P9+BqZ05AqHUcXyqqBW+DLELoQFhU6H6nruOMMIB+emVHrkTbY5T5fDLwL6KSCT/I9NvHKVw0PCAgImCh8i2/zD2yniEcpxvfxs6jM6SjGohSfJ23lJPsQ4dW69brxZR8t+R23v2oUMdqPj7PIRZpkP5LPeUiBKULyPQMUtbVR19Mz4akNE6rxZTIZvvWtb7H7+edHStVYL7wkvsuvNYzdgwayCngnGrw+RF5m9rQ6cz1IazuIt1MbadrsxP5XNwVd0HYUQXod8AEgh3y+xJdC7c2AgIDTACX8F3eTZA4DePeR1fSchcyU4LvXWKHqFJLPu5A8tcDAWiS/rddfG/AnJM8XoO4QM5Bct+4N5rIaBtYBq9w+U8C+73+f3gPWSXViMKHE98tf/pLHH32U7MDASMRQDtL05qJZQwZPUFbF+yo0mP1oYHrwLe4LUZTQTDRDSeHrvw0Bbwc+guzJue48rB1RAQps+SfgAiBBnP/Dp7mF6yZsDAICAgJOJAxRza3cRC/lpPD5eFGTZxWSr5YvPYhPd2hFlrZSJFfNamcmTWsUvhbJ4Teh+IoOfOPaBLLmPYuINIU0wFZg1Z49/OTv/5729vYJG4MJI76mpiaWL19OT0vLSAkcKzdWiojN/HF9aODjaAZR4Jbl4Gcb3UhbK0Ok14V3mqbRgA0CjwK3oIGci8ylloln3dNnu+P9knfxZf73RA1BQEBAwAmJ3byaRj7FC+SwFlXI2ofkbitSQPIRAU7C99Ozrg4HkYa2120zE8n1HPcyheVZYCUiSpDCYSZWs/zlMtq02go8fs89PHDHHRP2+yfEkJrJZPje977H1s2byenrU2kaNJDmWO3Bd+lN4zv61uC7MGTxarhFIJ2BTKDT8K0vDqJK4y3o4lnZnVy85mjL52Mzlgv4WKi/GRAQcFqihDV8jjq2MoWfMoDMmHFESvVI/j6H719qnXHMPPoCUiamuu1iSNbmu+9NcYmhgMQYIksQ0cXccayUmeX/tQH09vLoLbew7NWvpqGhYdx//YRofGvWrOGPf/wj/U7b63PLS/GEFEMBLZsQy2eRLfj1KOjkPHdyQ2jArfL3DuBB5K/bhkyc5gMsQdpckVu/G9mZexA5drllz1LPtSwPVVkCAgJOY5TwG24mn4uZipSC+cgvNw8pDZbOUIWPAi2KfJ6H6iYvQCRmFVwS7tWJz7HegeQ2qOLWUrfc8v/sGFan+dlnnuH//fCHdHR0jPsvH3fiy2QyfP/732fv7t3k9/aOOFBN06tAmhtI8zKHaCWq3jKMZgXWTb0fEV8p8gmaSmxJ71ZWxwpOH3TrWwXwQbe8GrgIeBVFfI2bQ9pCQEBAACV8l/9LB9UvSnGoR8EuFllvFV0yeHNlMz7hPcvoqi+27ll4y5sVLqlGnNCCrxaTwWt+hUAqnea+5cu5/vrrWbVq1bj+6nEnvjVr1rBq1SoyBw6QQD8oH/2QNP6H96DBtYKnC3GtKtAs4Ek0O7DKAPXIUboHaXrP47v/zsDPQixvZDGahVhgy9uAfwBu5fMhgjMgICDAYZgGfsXnaSZ3JKDQ6nnWIPltwScZ930tIo/dyHJntY9BMr0PX8Ulibf0dbpluxDhmVtqCSLDSfi+gWcDPfv2sX71atavX8/QkHVfffkYV+LLZDL8+te/5kBLC/T1jZgwrTr4ENLy1rvXATSQdehH5iPi2gTcE/ne0hM2ItLbgS9ePYQGK2q0nA6cj0jS8gXfAzzI67iJ/288f3JAQEDASY7/n703D5Prqs/8P7V1dXVV73tLbe3WYkneZTAWXsCODTYBfmwhrDYZmJnAkISEkAR+JAyZZJIwCSGZDCEzJGQSeBJikkDAS4wBCRt5kS0vWqx9a7V637u6tvnjPd86LdnG1VK3LNvnfZ56arv31j3n1nPe+37XWg5wOwe4jQlOTXFYB2xGwsI6NuQRsVkFlx1oLTYVaC2OxvDxGl1ozZ52n0261zXI7LnG7WsuKfMVzhSL9O3ezaFDh8hms/M24nklvt7eXu677z7GT54s98DL4DsAG+tPoMFPIXPldWhiEojEDuKVYRIRmbUdOobP+B91vzvkPpvCVwg45H5jFfDbQJE27uBv53O4AQEBAS8T1PItfp0TLC1Hy5vJsxtfcLoR36Hd0hssB7sG79Kyfc0NZWlsphhb8eXRrKP7TrddF+KBo+74idFRWuNx0un0vI12Xonv61//OgeeeYZIPk8RTZ5l9bcgVdeJbwpbi8JbV6MJ6gHuRsQXQRO2ARHjFW4b0KSB79E3hGzFeVTB5Tp37AtQZ/UNxPmv/P/00T6fww0ICAh42SDHar7FJzhOihlONXleitbtBL5R7QkkTEz1ze6MA94veMBt3+G+s+oty1FEaBcygY6iNTyFyPVh93mmWKR22zYikci8jXXeiO/gwYPceeedZIeGyrkeVmml1b0fQErOOvZ2osielNv2KeTbG0ET3I6ayabRJFiT2QwitmsR0R1BkUAWTbTFHWsZ8CFgO6/ib7h9voYaEBAQ8DJELb1s5m7eDDxb9VlKmRHTOD7AsB+ZKWe3kLOKWUfc0aNo3bb0suvQGn3c7Rtx+0yhNIpRvIvsyR/9iO333TdvI50X4rO8vcP79xPP58sZ/FGk0poQqw+jAU+jybsGyd8ccA+6a7Ci0jXIvnwhMo3uQRNtduIcUorXInKN4SsM5JFS/BAA9fwyfxxSFwICAgJeEMt4nLfxEMueFejSidZ1s8aBTJxW1mwAn8ZgeX85nK8OkZqpwR603k8j1dfitsvgozzNn3gc2D4ywo6vf33eUhvmhfgeeeQRtmzZQml4mDTe12bC1Erf5BGLx1Gm/yr3/WGk0I6giYqh9Ib17vVeNEHNaPLHUSWAPwD+GU1SHBEsyFTai+5O/pxf5HEun49hBgQEBLzMUQus5F95I08T5Qhanw+gPGur2Wllx6bc6zxavyfQGh7Bd8Q5CtyP/ILW6WEKubUaZx3L1u+Ce1+NrxhzGNhy552MDQ7OyyjPmvgskvPEsWNEJyfLFVlqkRmyCh+hY0qtCpUTK6GBPYHIrICvDrAR2YQNDahlxvXoriCPr/02jsyh1hoDd9wHWcHv81tnO8SAgICAVxCWMcF1PMOacoL5EFrXW5ALqhEf6GLFpafxLeZyeFOptTAaxhNaEYmTQyio5RlEisuRdbAOX/2lyh1j+8AAT333u/MywrMmvt7eXrZs2cLYwACRUqnccWF2bbcalHNnobKt+Jy9h9Cgx9HERZCqu9p9bx2BzW/4EJpA0B1ILd6pancUa4BriPK7/HkwcQYEBATMCbXATTzFx2kgxSp8RZdb8H63iHs9iVRbE56orC5zM7482SA+9sNqZdag1LNuZAXsQDxhZlMjVytv+a0/+zOOHrV4zzPHWRPf1q1bOXzoEJHx8XKlb5Ozls+RRaRUQKTY7b47DmxF5GY5fxng9Xgz6SiyL1+MCO8omqwE3hFajya4GtmL3wtM8g4e56azHV5AQEDAKxC1DHMVO9h0SnrDECKmCFrL03gfXwSRnEV6mpCZxvv7wAczziAOWOYeluZ2EpFpgzuWucqqgB27d/OJj370rMnvrIivWCxyzz33MDEyQj6fJ49IyEjLInzG3SOPCKoRTdTjUA6bLeIjOSfRRO1yj2PIxtuLZ/9RdFcQQWTXjM8DbKCGT/EHZzO0gICAgFc4lrGV93GY+nJCexKtsebfsz7pwyhgpRq5qaxAteVad+C5YQqJoRm0rtciIhxFqrAGxX9YN/gcvmbz8WKRn2zZwpYtW85qZGdFfL29vTz66KNkXQcG65Kexlf6nkSkl3Xfr0BMHkPMPoAvS1bt9i2iHJE0lDPvzD68FO/7sy6+Xe73VgKvAbbwPnrKxtSAgICAgLmjllGu4AFuAHxxkKXI7JhF66+ZNq2heB6RYxElqefQem61O+vxjQROokDFD+Ej+PNI4Iy6/apR9D/uGIyMcOywlbs+M5wV8W3dupUTPT1Es9lyB4UUGlQSsb4lLGaRKrsIDWoIX4y6ym1v+wziJ+A4mjxLnpxAE2KRnK/GJ0auBC4mwx/xm2czrICAgIAAAJbxCO9kN11loqtFYmMFWns70boPIsReFMNhqWej+BS2LsQDFviYQ9GdAK9CQqcBkaNFicbx9Z4B6nM5NtTWntWozpj4zMw5OTZGoVQqt6uwJEczc465k4+hKi3LUFHpx9FkVKFBmc14HCm5Ehr4ITSZVfhuvyfcdkl37IOoSstGYBvvfl61N4+J/wEBAQGvANSSZy3f5W2UiBJHRHc5clm1oTU9hcRIHq35GeTvA63VY/j4j+Ks1xFElP8XEekNaC0/hFIosm772Z0hYsD2r3yFw2eh+s6Y+MzMWZiZKdfUjOAHn0SDtSosNqh2RGzWVD6Kl7aW1T+CBncY3S1kUAPaYfe9tbmfRJMzisj0OjL8Dz79vOdcKp3paAMCAgJeqVjGUa7lJ6x9lnnTGgVY1awqRFaTSN0lkaqzHnvH8cErRn5ZfPESq+uccs81+K7ww0hMpYCDTz3Fo9//PsWieRnnhjMmPjNzxrLZMgub3J1xB7aoyxbgI8BbUb3Og5zK/hmk+kbRJDXge+lZ7sgQ3iRai096jCHS2wh8m58Pvr2AgICAeUUtsIi7eTsQL0d4Wu/UI/j0BVv3iyh6vx2RVdZtdwyt5VG0hlui+wlEir1u+wZ8IWtLirfyaQeBiWyWfXfffcatis6I+E43c1oDQzNzmoozVl8E5dopXwOexDtDLYWhHvnxGoHb0B2AVQjoQ/l7/WgSptCdQBrdPTwBQDIkqwcEBAQsCFKMUsujbAT8Gj+C1u16JEzAE9RyFHeRdtuM4hsWWCyIccBJ4DH8mj6NFKNZ+16FgmqiiAN+ABzYupVk9My02xnt9VxmTovkNCmcc6/TKFqnCzH+VjQB1m3XKrV0uec0IsoMSpi8DNmKC/g8D0uSNJk9BXyfG4PaCwgICFgQLAOW8u+8inGizKD13eong4iwDim/QUR+N6O6yabsrLrLBN4NFnHvH0NkaTWXR5EQss49J90xcN9tP3aMk089dUajOSPiK5s5Z2bKZs4aRHjgq6yMI2a/1r3/ZyRnk7O2se68Q+44FyDTZQMyfw4jaVvntku73xpx+6SBFUT4O373TIYSEBAQEPCCUA3Pk1zIbjroQeRWRGQ0gVdn1Yi4TgBXAm9H6Q1xt12f+76E1J+5vAbR2r8KCZ1JfG7foNvHWtElgeP5PB//+MfZuXPnnEczZ+IzM+f09DQUi8TxLedjSOlNcqqZ8zLgQeBOxN7L8cEwNW6/ETegG/AJ8LXuOJbgaEEtFgFajezIS9hMLxvmOpSAgICAgIqxjBLreID1jOLNlUW0Pl+IxI4lrZ/E51ovxdfoTCLCq8NH9OO2fQaJH6vuYkVRXgO8FhGjlcTMAc/s3Mm2bdvmHOQyZ+Lr7e1l9+7dFLPZsmJL4StyV+Mz883MOQX8G7oDsBJmE+54VWhSYm77CdRP7wmU8mA128bQJFjdzg538q3At/jluQ4jICAgIGBOqAUSHKSLKElq8S6tJConab33apBF7n7knupwe0cQB1gkqBWijiNR9AgKiKnHR/9Pu2fjA0uHmAKGh4Z4cseOhSe+vXv3cvToUQqziM+6rOfxbebNzHkVcJ8bUBzfqmIab68dd/tdgLovtCJp248JbB8sE0d9+Nox52cz+7l5rsMICAgICDgDZFlMD4vKyq4BkZE1Ici4zyZRS7lRZPlrQ4QzjojM4jbMihd126aBt7h9UogEjSAbkZiqc9sn83mu3bCBeNzKXleGORPftm3bGBkZoZjLlYNMwJs5LerSzJyLUJPZ/W6wOURq1mZ+BoWwVqHIzy4U9WnVXiwJ3pyjzW67HCK/Id5NoexdDAgICAhYOHQC9eymsxzNOYXWdvPBlRDpWZf2RrTGW+RnFsVu2Bpv/XOq8Tl71tauxR3nAOKBMZQSkUMc0w60nEGPvjkRX7FY5Mc//jH5nOpspxC7W8HRgjsxM3MuA+5CTWZH3GBzs56tMHUEEdoq4AFEfHl0h1CNInr68QWtn3THv5g4j3DHnAcdEBAQEHAmWA20MU4nkGEUreeWcx3BNxBI4mM9LkAuLWtaW3CvrdxkCXFBPyLRZSg3+3J8IZQpd7x+995MpYe+9705j2JOxNfb26syMYVCeWeLsLFkcitPk3In/mN8Fe84sv1agqOZOhvRpCwBfoLK1TQichvFpzyUUITnwyjB/RrWcIB1cx1zQEBAQMAZ4yrgFoa5hBqkzLqB9ahCVzMiOgtanAb+P0Rs1moojVxaaUSEpgQHEK9chm9wUIMsgZasFsXX+jwI/OTRR5keG5vTCOZEfHv37mVkZIRcNls2aw5yamd1EFG1It/eTsTgltkfd99H8WVvLgBuRVFAP0BBMCOI2Yfd8RfjSTMBvA64k/eRL8eUBgQEBAQsPFYDyznIcpJEyqRkXRgi+DgPq+K1HpHaKL4TzwxeuVnj8tmtjqx7QwwZWEcQByxBwgi3z6PDwxx57LE5jWBOxLdv3z6GhoaYyeWIuh+3EzafXcINqgPZdc152YzSGLqRYzKFLztW5wazHanFGrzd16TtAXzLo2Hg74B/4/o5DTYgICAgYH4wRRpoog7fUMCi+800WULiCERiRbSOm5kyiq/ClUNkeQSt8cug3Nzcips046M8JxF5DhYKHHnkkTmd+5yIb8uWLUxOTgIiLcvFsBMfd4NNIclr6QtGhNXuB2P4YqTWQBa3T5s7roW3RvFRP5b0mAV6SbG/3K0vICAgIODcopnDXMIYvoC0lSebROt8FAmXE5xadWUU8QXIbGmxHgXk7roTeCcKjkyiNb8dnyduhGgZAk/PsTFtxcSXz+fZvn07pVKpfJJFxMQxd8LWOaEdEVi/G6yVMzuJV4Hm3FyEN5FuwHfinUSKsgndIVS5/WawRrZrKNI8p8EGBAQEBMwHOoFl9LCIMURc5q8bx7cPiiFS3Is8g3F8IGQMreVWf9O270O1mhtQ3eYknj+GkICyyl5GtDsef7wsyiqBrIp8AAAgAElEQVRBxcT39NNPMzAwQMRVa4m6AY3hCTDvtm3GJxientRu/j3zES4FLkVk+SMkc6fxvZfsBE0p1iD/4RDvwbcmDAgICAg4d5Cfb9o5razKygRe2VWj9b4PtZj7WSRmrDOPJaFZakKd29d694F8hw2I9I65Yy5zn1kbvCngx0eOcP/991d89hUT344dO5iZmaFUKJT759mPjuMT0ovIrGmK0ErLZN22Y5zqzBxFwS11blKMGGOcSpZmLo0BGSIcYnPFgwwICAgImH+UaGWKjmeRXhKt5yVEfGMowCXtvrcAmASy/Fk/9SziBYvgrAV+hlO5pQpZE+vwPr++bJZ9e/ZUfN4Vp7vv37+ffD5PPp8n7k7AyMtMkFPuZFa5wZjNtxHfv8l6K5XcAIZm/YblAZbc/mP4jg9VKAGyAWikhgEurniQAQEBAQHzDTN3dtDMgXJVrirkooqgNdxIphq5wCywMYECX6JonbeIzhiUozcakTusDm8F7MH7A3H7R4HhY8cqPvOKFV9vby/5fL48OEs4rEYGx6zbrsGd6AXudRuK5OxA07QUX56mwb23QT6OzKcxt30bmkTzH44jIu1kJYWyZzAgICAg4NxD5s5Rmso9WVNoPbc+quP4uswgN9ViRIwxpAxnmzxB/HAIFTPpQlbBVnyR6xZ3jDrED83ueIlRK6L2wqiY+IaGhpjJit6q3Ank3cNY2loMdSGmtlZF5oxsQMRVQmRmPffqUDDLXfh8P8vlU1lUsf2gm4A+bqh4gAEBAQEBC4cszVQTJYGsckl8GyHQWn/YvV6FiGoCX3KsnVNLX0aBR1HRaxDJxfF9/NL4gMkRfPnKRG9vxedcMfGNjIyQc/l7zxVSksOT2VKUd2eE+Fw/WkB3A0vcZ3eiXn2WxpDFFzu1dIcZxOy7uKXS0w4ICAgIWFCkiZAq52f341sTWQbAcWSivBwRYwmRZBof1ZlHQieFajtPu2MMIq6Iu32Poe49B93nJowGDh6s+Iwr8vEVi0UGBwcpFQplv13ktG2szUS7+/4xRIZWlHq2P9DquiWgXHDsfryt1hIZx/FOTavasgc4xKqKBxgQEBAQsJBoJkk71eynj1ODXKwjzyiwC3g1stodxau8CXxNz0Z8TU7z4VkxsgLiEqsAE0PkWUCm0ZMnTzI9PU11tZU/eX5URHy9vb1MTEyA63mUQnK1CpHUMXxpmjjwHZSHUY03i5bwnXpn9+57EN/PL4NvXT/hBjyML06dAkrEORn8ewEBAQHnATqBTUzwAEX2l0tXmjsL/Lp/ELWduwwpOhNAw/iAxggiPfDklEFcYU0RLG/cfsuCII+NjjI5OVkR8VVk6ty7dy/T09PEI5Fyq4kRxL5H8OxbjUjrGdSRwZyXlnx4FMldi+i0YJcEsuNm8fXdFrvPrS3FNEaANRTLUxoQEBAQ8OJhNXAJvbSXyavGPVJI3SXwKQ0g4luK1F0DlNPjEohLcshqaNbAm1APvsUoaLITlb9sw5c6mwQGslmqqioTRRUpvn379lEqlYiVSmXT4+y2EMqt87kYB9xgIkgNmuydnbFfg0htHSK1H+FTIpJoogbR5Fl3BlWJ6URxogEBAQEBLz72McgwdfjWRLNztUdO23o98Gak+u7HV/gy62AKEZvFksTwRal3uePXILKbxptFp4pFCoXKRFFFiu/o0aNEnNqL4rvi2sNgknQQDdiaDNp2NYjV7UdP3zeNb14bcZ8l8XkfXcAEKyoaWEBAQEDAuUGWZLmrAojEMniT5GxUA69HgYrH8IGMNYgbWpHCM1VmOeGHEInG8LnjVbO+HygW2bp1a0XnW5Him5iYYGZmBhBxxd3JlxDBzY7otFDVKXznhdn1PK2LA26AdsxH8eGvI0gWm5nU8gVjwBBXVDSwgICAgIBzhThxolRRpBaRmgWpnB4IifvuScQHNch8mUPc0o7Kks02WlrboyokjFoQJ0yjCM8DiDQP7d9f0dlWpPiy2SzZbJaoG9ASlH+3Ad9wcMadxHEkXc1x2YnY+yLgEnfSNvCdwJeB/4MPZe1wx1+CmLwN6/mrzwa5uqKBBQQEBAScK6SoIUYGrdtD+F565qaajV3APkR0VYgP+hEhtbltdiBeARFfC76J7SRSizsR16TcMe797nfJ5U7XmM9GxSXLstksSeRds7YSERSMYn2VmhF5FfC9+WpQMvo0vgLL7Mot0/jIzRTelNqLFGQGJT/WAauJ809cU+kpBwQEBAScE+SYxrcJGkfrfQHv75vt2noCWfWyiMRMHdYjwbQTEdwuYKP7vAmR5T4kqCy/z4JjkkDV9DTR6AvruYoT2IsulcEY2ErN5NwPm0Tdj0ydRTwr55EszaEcjBIivcuAlUjJ9QC73fZDbtspN8ghd6I3kKRQLnMaEBAQEHB+oJoo8XIOtvn7zB0W4VTiO4jWdSOugtvG+roOIsF01H3Xgy9bZgGPBXwATRoFzSwulYjFnqtsyqmoSPEVCgVisViZ4Jbgmw1G3eCsaKiFo4LvymBd80665xiamKfwnRxsEDPu+y58wnsSydxj5RouAQEBAQHnBzqBLnIkKDFVjuKfjSIyZRr6kAgqIMugpTBYV4YTSOFdjAjyQve5tS2y3G5reFCL6/BTYb3OihRfIpEgHo+TcAPocyczim9HVIUckkuQebIK35PPMu1HZp3wDOq4vsZt24gIbxrfc8+cpBnE+PvKYTEBAQEBAecHVgOLyPLcieNxxBs7Zn02iLghig9+7HBHWoz44RgSV+MonqQfn+xuFVvybtsJxEtN4+NUgooUX6lUIpVKkcVL1jq8/XbQbTeCCLGAVJpJ2BgixH68SrReTPtReTMzkcYQo9e645mKHAQeLtNmUH4BAQEB5xMmqHlWEIvV0cwjojNYtZUkvrJXFHGEWf7GEOnl8IIrgo8BGUP8UO+26QMmR07PGnxuVER8mUyGWCxGdTRKfbFYbihr3RksMqeElJt1xo0ioutzr2fwZFhC5NmE8jNMBkfdYHsRu0/g29ovKvd+DwgICAg4nzBJTTnRHLySm+3+MlglLgt0TCHusL58o/hmBrjPMognepCvzwpgzyDBNQKMT82m1+dHRcTX2tpKqVSiWCpRjQjLzJ4H8U1ma/EVtM2pCTJbLnHPve67SVTurIQcmF14Rs+776vcbxXd86Jyb/aAgICAgPMJBWLlqi1VyGVlieanr9pmojTh1ICv1NKPj+0wgisAm/B54K34qNAJvJWxUHouD+OzUZF8qquro6qqqpyobmkLpr/M1jrstm9ABFZ031thUUtvwH2/FUVtJpFd1+TuUpQfuALl/Q0hcgwevoCAgIDzDTPACFGyZSLL4EXM6SQzicitiHeFWWOCASinRZgatKpeE/g0ugS+I5A1Q6gFUhVEdEKFii+dThOJREhEIvSXSgzgScjKzVgfvnUoi/4APhP/IL4br3VYb0Vla16DnJ5/7AZc5wZnpGkBMgngcChOHRAQEHCeYS+wkwQjZUtfP1rP42i9n01H/YgPLBtgxr3vw6c1xPGCqhH4LhJKpgQP4vPHC7Me2fksUp11Va/zpRJTyB47O1HdMvMzKAx1EMrxPdaRPYXU3eyeeyZKrThpAt9mIoNSGIrut04SojoDAgICzj88A/SRIUctvoqXuaws9sMysKfw+eDGHRbdmUckOelet7j3T+G7ry/B53nHkcgy8+pYMlnRGVdk6pyYmKCmpqbc/n0YEZHlYZi07QMedyfcjAJXOtzJFvBFqy3QxWIzrVVRCyJJM5MeRyTa4b7vL8d4BgQEBAScH6gGVrKEKBnEB3XI5WWKLIVSFUCEZQnuSSjvk0QEmcD7AJe77SwJPoV4ZR0qmDKDrIcNyKK4s1icv5JlMzMzxOPxsrTsxBedHsE3l33YnVQTvqVQp3t/GF/YuohI8LA7zk/coNsQsXa4wZ9E+Rt73XkUKbpfq6vktAMCAgICFhwNQAPVzJRLT5oFz1IT6lD3dfC53yV84WnzCVpSujUqT8zax8yebchc2otkkPUBLAHN7e0VlSyriPii0SjRaJTxUol69yMJRF4W1GLpCatRMIoFsYyhwJVp994idEooqhNEelcA29EkpfEFTi0/o4D1ejpOic5KTjsgICAg4JygnyGmywRlvj6r2ZlHfrlL8MQXRQRn+eFGcrWIU2anw5svcAJ4GlkDLa0ujoIhu4H2Vavmr2RZU1MTk5OT5WKj40jFmcQcwqs+C11d6gbQik9Mt1ptRm7jqBip5WHYwIuIDJsRmebQHcFrgFFS9FZy0gEBAQEB5wyRUzry+XSF2R0X7HPTZLPzu4v4puZJJJ7M7TWIOGcGFbgG3+7I/IJVQOMFF1R0rhX5+FpaWigUCuUM+suBzYhlxxGDZ1GJmQPIV5dD0ZpbgG1uEHE3gBn3OOLet+LDVy1us9Ud9yG8cXMRsCzQXkBAQMB5ghkU7TFJyTmjbB03grNmsSBX1nZ8IGManxUQQzwy7r5rwgufGqTwhvC1nK2xOSiokkiEzpUrKzrrihRfd3c3APlIhFypRB8irQlEXBapCSLDq1H/ve8Ae/B1PSeRTzCC1GAaFSF9DQqK+a4bgHVyH3ADsyTIE0A7dwKvJSSyBwQEBLzYOISiOmMkKTKNr69ZxCeyz0YKWfOs/KV1WDBynEZ8cgHy6ZWQ68uanV/oPhvBdwkqAH2RCG1dXRWddcXEF4vFiEYiFEolDuP77yXxDkZj5UF38hfhzaDgHZYrEGM3IQpb4Qb0LXzEzwQyo65DinEcpUMs4kEUB2p1vAMCAgICXhwMAkXS5MsxGVN4E2cC30rIkETrf5pTYzjMZDmFuORKt+3jqGUdSDBN4ut0mok0h4qcdHR0VHTWFRFfKpWis7OTXZEIIyiM9FXI9LjDneSQO5l97r05N3vxuXgWfbPUnfxeZAp90J30CL4x4RrgJmTyHJh1/EH2uq0CAgICAs4HNNPPFL6/nhWftjzvllnbJtG6vhKZPbP4rj0x97oK+Bl3jD9CLrQaFPE/iie+HBJZ1UBXPE5b22xv4vOj4g7sa9eu5cf33svU1BQRpMYOuB/tRBosjwjrDkRwe5AIHsEnGV6I9/etdo96xPppRHBmI34QKb+Um6xDwBQjRJh+zp5PAQEBAQHnHo3sYxSt25aGYHU1U4iwDAXEAfvdNkkkdqxzD0hUdaO0hSF3jFWIa6JIBc6gSNE88gW+obGx7JZ7IVRMfEuWLKGuvp781BRVyPxo7d7NwWiENYyI8En3XQkfoLIK32B2AgXE7EbkOILv9GBNbKNucgbcxHRSpJmT9HNRpaceEBAQEDDvsMCWI+Q5yiTiAzNrWruheuQWMy22x+3Zg7jCUhVK+JrOi9y2TyCrYRXK+96HD4KZQSSbR3wxXFVFLpcjkXjhtnUV9/hZtmwZ6fr6MjvPIB9dxJ180T2O4J2a5oyMuZOzBPZxfNHRE+4YP4tMoNbKwrL3pxAhmpk0A7TzdKWnHRAQEBCwILDAlmPkmCrn5c1uW2dpBgUU9BhD63kfWv/NFGrEl0ACycgz4r5PoGyCS/Hdf6ymZxoR6yXLl1eUvA5zIL4VK1bQ1NpazruwFAT7Ycu9mELMPI2CVq7BJ7pHUHTOKPL57QAeQI7Oy4C3IUlszsq4G1Crm4wa99vd3F/paQcEBAQELAgU2FJFnpjL4bPgljF8PnYO32/VSGwcHxRpaXIx5EKzcmfgc/zG3TF7ObXySzWKObkE2PjqV1eUvA5zIL7u7m7aOjqoisfLZWgG8KkGjUgB5oBHELFtRgpwzP1QPT553UJUJ5ANdxe6fxhxg7RE+GosqEX7KYT1kUpPOyAgICBgAdFGrtyqLonWbXNZNSJSWuO23YfIywJgFuNdZZbXV48KUYNcYdNumx7k07MKLjm3v/1229VXV3zOFRNfKpVi1apVpGtqmHQDGMOnKLQg4rP8iixwD2olYe0pTLVd6PY19j/sJqOA7hYK+FpsEyif4yqU2jAD9HOM+LN6+gYEBAQEnBuY9BllMc+UIyyt9qa1FZod2DIB3I/WdQtkKeA7OOD26wDWI0W4x303jYRRHgW4VOHLXvYD3dEorZddVvHZVxzcAnD55Zfz7/X1HB4dpRXlWTThg092u8E1oYoruxCRRd0PZRHLdyC/4Dqk5I7gS5jF3DZNKOLzDW6/BDKJbgdyzNDMPnrL9xEBAQEBAecO8u9FiLGcE+zBmyCrObVMZcbt0YusgUcQbVrJMYsRKbnPzcVlSesFxA32uWUFgO/3V0qlmMxknpUs/3yYE/F1d3fT2NHBkSNHWIZ8bz1uIMfxKQvb8eoug1jf/Ht1wF3AY/iJ6kJ22hgivDxShAfcRFgrpDuRU3QKaOEv6eVjeFEcEBAQEHBuYInrJYqMlRXc6WkMFp05AzyKUtRGENFV4/P3SrM+W4xXfouQurP2RXX4YMcS4okSsKOmhuN9fayst4rQPx1zIr7a2lrq2trKIaS7kMzsQX4667pwENl2uxBDr0WxP8fdcRrxiYrm3AQFuETdtifc8fa6AW91vwGanI08zFMcQdkeFVtsAwICAgLOCt7M2cxTDFIggoROFh+sUo9SGKJu6234dT2KAlhmF6hOARvxUmYvalk3jE+AH3fPZlatRQzQvHw5yQqb0MIciS+dTpNuaaE+HudkPs9RfAhqO2Jlq592kRvIckRYe2YNuB2fdR9FBBlFvff6kWKscQO+B03mBCLAtNt/GduJcDelclvCgICAgICFh6UxJFjGTibxZSbzbosEvtJKDBHWAD5Hux4JIKv1bPWbrdLm91EJy8OzfjWCb3M0hXhmJar3fOVNN1WcvA5zJL7W1lbiiQSZTIbc8HC5N98SxMgP4WuqWSb+ND673u4KLkZK0Hr5DaOpvM9tO4j39RXdSXah9Igc6t23jjG6aeJweaoCAgICAhYeMnNGqGEpB9mLlFsW3zc1gdRexr23yHwjvfVoXbeiJRbkMoNE03FEpC1I1jzp3pu1MYbP5auuqmLp5ZfPaQRzshGmUina2tqYyWSIIlvsKpR/F0VqbRqfnmBNAw/hGw92u4HhBjmCQlZ3IXV3wE1EI75ETYf7rRk0IbtQH78buRPvOg0ICAgIWFh4M2cTh0kyUW4+m0RrfA4R1Br3+iTihD5EVEuQry6Dr+kcRSLKUiF2uX2GET9E8GIq646fREJpIpOhe+PGOY1izs6xTZs2UcxkGHQ/egz4KxR4cgLfmy+J0hCmEZtbUuLTwO+i5PUpfP+laXRXsAQR42o0IUcRce5GEziJbMUPAMvZRh3fwXfxCwgICAhYOHgz50b2M0mJCbywsWhNi/EYd3uYL68FkdZJRISjeIXYjkyh4FsNpZCwMoWYQ0KoE1kPq4Hla9eyaOnSOY1izsS3du1aOru7y9n4JxAB1iOiS+FTF/rdyccR+y91A+1BE1KNL3VWgyYliUyhjci+axn9SSR5G92+u4CtTLOGO90RAwICAgIWFt7MuZ4t5Yos4KM5qxE5ZRChpZClzmp29qHUhmfwaQ91yKpXhU+PA1kEv4GI0kybuOck4pVbP/hBIhH7pjLMmfi6u7tZvmYNsUSCIiI4KzVjHXejSKZayoLZcEc5tT/TlBuAJTIeRqkRr8LXcMu45wa3X87tW4VIN8WdRDkw12EEBAQEBMwJ3szZxn5mGCg3nDXfntVUNjNn1r2fQMQ3O1fburVH0Hq/BAWrHEXCJu2+24UCHo+74zUiBdgGtMfjtG/ePOeRzJn4UqkUV2zaRLq2lixibvDkdxGKzkwjcroYBaX0I1PmCLLbmqNyMcrVsAaESVSRe6fbxuzDpiyH3UlvRHcP04zSyX34eKKAgICAgPmHN3O+lkdIojXaIi3huc2cNWittm7qlrtn3XdSSBANIjU4gO/YMOSOY9Vgom67/Yg4VzU3k1m2bM4jOaMEuE2bNtG9cmWZrRPuhJP4LuzjeCdkHd6JCb6wtYW7ZhAhTqIJ+RacUgLH7jN24bvu7kKBMFXAVdxNMHcGBAQELBRmEOkNk2Ka9TzGMN7KZ+uymTmTSOwYByxBCq0LiZ0kWrtT7vtWVMpyOeIKM4dOI6VoBU7MLGrHbb/44jMqXnlGxNfd3c2Gq64i4QYw4Z7bkULrRYQ14QaQcQOIIT/eBfgM/358/kce1QA1f14TulNYhG9TYQO3pEdVDHiUOnacyVACAgICAl4Qe1FYYS8r+T7PMMmTSJDM4HO0zcw5iNReEUmSQUSb1i29BinAOiSe6tCaH0Nqbgzf0sjyuCPus2n3u4eB/IYN5HJzp74zIr5UKsWlr3kN6Zqacn7eZW5ww7MOnMNX1JZlWISVxWf216GJiLvjDKGBm813BZrYZmTeXInMqctRwmMROMk0S/jqmQwlICAgIOCnYgaR3h6ijHMVjxLB5+1ZikEMiZ8WFLU/hNboFL7HXgER4gm0djchrhhD1bl+BxU7Kbp96xDRPYM3eeaRcFoSj/OGD3yAmppKK3R6nHGtr9fddBPtXV1MI4W3zZ1cBjF5rTu5MTfIWjQxY0jlWcZ+FWL/OndcI8ZGRHzDyAxahyZjFE3oJCqSXY/INc09xMie6XACAgICAp4Th1Di2lKuY4SLmSy7rhJIsFihkWVofZ5AJs9L0FpuqQsW5DiEz82LIlPnALL2LUfmUAtoTLuzsCphzbhozgsuoHn16jMa0RkTX2NjIze/+c3l+mz73UmvcCdtNt2S+/xKRIpxxODmyOxz7y2VoehOqhZ4LyK2BCLAHrf9AUSiE3h/4hQjLOKLyNgaEBAQEHD2MN9enhqiXMN9jOALRZvLKYIIah0SOjX4/qyH8C2F+vE1l0HEV4MKm+xx240i95aJpyK+hV3e/c4yYNVNN0EiwZngrKo73/RzP8eidJpqd5JrERmNIDLL4SfGkhRr8Uyecdubj7DBnVAelS/rR4mKXWhCLwOuQ+3nl0O5Ia7l+t3IP6GEiICAgICAs4eP5HwdD9LCOCfQKmvFScy/txKR1jgSLBsQOR5zn8+4fYzIOtx+aUR0FrCyxu1fQoLHyK4FccYRYCQaZdEtt5zxqM6K+FauXcvajRuJuxOW8hLBWeCLtZkfQUEtTUgJWmf1GqQQG90xzU35DPBlNAFpd5wbkJI8jiq3PIz0nfXqW8o2avl74F9RNkhAQEBAwJnBR3JmGONKHiCOVFoeX5/T/HGX4H13oLV8yr2e3SzIcrkHELmlETlmESHuRXzRMuv4S90xLF1iaUsLF77+9Wc8srMivlQqxfU///NUISafQr44K15t4adZRFazC5HaBJbQAEfwEtgqeB9AdwXdSA3uQZNyGN+ksAYR6mJgMUXezd3uqK1nM7SAgICAVzi82ruZLTQwyUm0/mbxxBdHJSatYWwKiZsYUme1yEJnTcsL+CYFEff5MXe8WhTZvx/FjZTwnX+sJdHrgLe85z0kziCoxXDWjexuefe7WdzSwjQitz5ESjYBLfjkQ4vSsYjPcaTLHkQBMlVIubW47QaAbyNS3At8Fym9STSpzW77CaQedwI5nmA5P0BTFRAQEBAwd5yq9i7mYeJozR5Fa7KpuQbgGrzaSyExEsf3Wz2B1uc8kiRL3XdVWCESvd7jjm1CaMb9Rso92oHVqRQdb33rWY3urImvsbGRm9/+djLoJEfRIMB3Xk+hCakD3owG2o4vajrtHrXoziGNCK2ImN8y+SeR329q1j6WNN+HjwjdzFdJsPdshxYQEBDwCsXpam+aHFJwQ2gNto4Mm/AVXCwpPYYnrhJGofr+esQTceTb63S/aK2ILL0t4Z6XIyvihDvesosuonXTprMa3by0Lr/pQx9iSU1NuSanRVrmUfTNBvf5MRT1uRKxfi2U80Fq0CAPIsLLIPIzs+dipAStgWEB3UWMum0n0d1CLdDJILfwu/MxtICAgIBXGJ6t9sxlZSXFrNxYBwo67MVX8apBa7hVddkx6/tl+KT1pcDVSN3hts2gfO1FeHIaRaRYhWp0Lv/IR844mtMwL8S3cu1a1r3mNaTx3XTbkLIbRhJ3BBHT7yJitJBV6+HUgkhrDE3SEbfvNFJ5exABrkODr3Pb18963Y8KYx8EVvIPNLF1PoYXEBAQ8ArCc6u9R5AFzix6aURStXj/WxIJk4OIBPei9TyP1uqrkEhZhKx+Fh1qaWxZVI7yOFKKtUgBHnf7ZaurOb5hw1mPcF6IL5VKcfW73kUmHi8HrjQhhk66RxMa3ID70SsQ23egCTrBqWkQ1okhhtSd9Xyybu15RJK7UY+/nSg3cAZNaIZxruN33LcBAQEBAS+MU9XeFTxQVnsHkfkyj9b4FhTJ2Y/vzGB1mw8B/4YCFC19odvtM4mP2ziG79JT5R5HER+kEYlm8Va/i6+4gmSnGUfPHPNCfAA3vOUtLFu6tBy0cgDfi6kbFSBtRAN4EkXsfBsR3QwavPkBq90xzaFZhSbKCpdabbeC299yPUR4mrQscDn308SDnJoyGRAQEBDw3PBq74P8AxkK5IAf4Wpjuq0SKLVsCngUrfnWXf0kEiJH8ZVZapHLK4NMnNaKyCx969znaXxps3G8n3AVcGs0yi984Qt0d3ef9SjnjfgaGxu55Y47qHcHbcRXcbFuuhZn2YuI0UJhGxBpNSLzaBGf9GiE2IRIbhxfAaYNVYTZ6H4r7Y79NLo7KTHDO/lL9y4gICAg4PkxjipmPslyvscE+3gaWdWO4LvnWJWWNrxr6QSqrLUNpSMM4OM8EsiyF0N99b4/a7/jiGafcvuNoTXfCl6X8CUsb1y7lqpLLpmXkc4b8QH8zO23s6iujhIa5CJUaWUNvl18Oz7ctQ2ftT+D7jWO4CNDjfDa3P7j+HDaLhQBOoRswn3uHCxi1ORxHQ+yjG/P5zADAgICXmaYAe4G7iHNCV7PNhYhRbYfmTMLaF1PA69G6veRfLYAACAASURBVG4IWeRa0DofQWuxuakakNJ7MwpqrMebNC08pcVt0+2ObbU4rSvPYuDaaJSLPvGJsw5qMcwr8TW2tfGOd76TJGLyJ9CErUSRP5ei7upL0aStxjcxrMfLW2sx3wVciyq2XIXIMgblJMp97jGBSLLP7VOPJvxiNLlv5A9JMDafQw0ICAh4mWAG1cH6CRFaeCd72EiWJfjmAJP4ZrMtiKR2IgLrxLuyhvHKMI7W7E0o0vMgItFBfOyHpUbsROt6EhFdK76RwZuBD65YQeLWW+dtxPNKfAC3/eqvsrGmhiS+ltsoUnIDs5570cAW4zs0ZBDpgXI3atCEnECJ63ayVgk8hia4EfhZ4PXoQk0i4rXHKKP8Fm+b76EGBAQEvAxwCHgIqOdVnGQ9+yig9XkHEhdWbzmJ2sKdwCxqvpHAKD6+AmR5uwqJHCO8avfo5tQu608hU2kO1Xxegdb1i4Ab43GiH/gAtLTM24jnnfgaV63i/W98I+0oIGUcTdYiNChLZDyKz804jqZ+zG1jrSliaDIfxlcDt1ZGq5GKzCB53IQPq+1HpDuKVODjwDbuZjnXEqI8AwICAgw+irOGSW7i21RTII2Ibhdae3No7W1DKm4c35XBAhrNr2f1N9cg0sO9t22n0Lo8ieeCKbxZExQDYu9bly+HO+6Y11HPO/EBXPf5z3NTTQ0tiMwKaMCWlG6Rm+0oraHFnYjlAE4iIozhy9eYwrO7hK3ISZpwx38AhcfWo4tzFQp6wf1WFFjKj2nh28jFasVwAgICAl6p8FGc7+cfaGGcHFJnT+MVnHVIuAKtz2kkUKyBQA8+19rKkM1OOogicssgS95hfFnLJL7behxZBfsRwV4XizH5kY9Ae/u8jnpBiK961So++K53cQWaFGs7FHHPJndbEf3MtvWCb1d/HN99fQ2amGpElNPoTqEfTfZONPGdKDR2CvjfiAyb0AWKk+cNfJEYdwLfQlowICAg4JUIr/Y2cz8r2FMOXjG1N4aPvl+F/HUZtL4W3OcTaB22lAazwkXcrwwgQ+qQ+zyHeKHFHaMP33XBrHY9yB12YvFipt7xjnkf+YIQH0DdZz/L+xsbeQtKcuxETN+GJ8BWfBNaa1EPmvQCUnoW8PIEmqCSe7ShSY+441S71zE00Q+ju4rVwPuQZF4GfIRj3M6jbo+2hRp+QEBAwHkMC2h5mOU8zU3cTw0SF9ZV4QQ+cLANeAsKQHkQiZJaJEhi+GouVl8zilLSHgO+iXSlVdqKIQU5hUi1gK/PbGbTDLA+EuH1X/wizYsWzfvoF4z46O6m6v3vZz0KOrkU3S1sQhK23T2XkHRehYjJmtGCb1/RgW9nUY1I9P1o0tuQndgk9j6UIH8UEWcE3T1UoZDZNcCNfJc38D13FgEBAQGvJMwg59A3SfM4m/g3jpFjNwpCOYaCWqxKSxp4F1pnv4MERc4d5bB7tjztCL7p+AgivEF8Pt8ufCHrElrvrU7zhNumGdgMfODmm+l+05sWZAbiC3JUwyc/Cd/4BvT0cAhNyAQivE5EToP4HLylaOIOIPIbR1L7AnQHYBLYbMK4400iv2G7ez3mPq9Cdyj3oom3JPgpirybr5Li1XyT9y7U6AMCAgLOM5jS+2ciHOZ6trKEcdKIiKaBe1CUpTUV34zW5W8jUkyjoJXL3PuT7qim5KqRekuidRj3WR0+iOUoEjNWirLR/YalOnQmEqz80pcWbBYWTvEBdHTAL/0S42hAY4iY2lGdTitIHUUTPIafIOvVZMnsS9HEDLlj7UWTPY7kci0yedodykYkzS2S6AgKafkHVGz1EFNcxy9xXUhuDwgIeMXAUhdSvJEj3EgvXfiYixiymo26rZejmIkfItOnVcxqR2bLZrRWW1PaGL4NXT++CDUosn898Fp8YZJu93ktPnhxOXDzW99K1fLlCzIDsNDEB/DRj5JZu5bbECEVUJ7GYpSrUcRPmHVwb0ekl8cnpheQ1C6gSbbq3RYJarkgzchsegWa9Cy6QJvQxbVO8U8BDzHAbfwiK3hmQacgICAg4MWHD2bZxBNcy0Mk0LrcgJTbP6MYCRA5XY3Mk1Zs2rovXIHW3AEkLOwRReTYh4jSmobXIBfUbuDf3WcmVi5E63rJvX5bTQ3r//t/X6hJABba1AlQXQ3/838yfsMN7CkWSSB/Xz8ipBVo8urwdTsbkEnTypbNJrBmNLlW/NTSGUbQBZpGBHfUHXcSEeMiKJdSsyCag0CMQ3yS9/FRvk+2XB47ICAg4OUEH8yyhse4lbtIUyoHs9QB/4j0oMVLdCNT5MPIGpd0n9ch19MY8gnO7thQQGtuNT44cRlag/ehdT+FCM86sT+JSHIFcCuw+QMfIHrBBQs3FZwLxQdw7bXw+tezDtV4a0XE9Di6M6hDk229907izZ0FRHqjaEJXoIm01hbW4TeGN32eQKSXdN+ddJ8NogvZ43434T47zIP8Jh9ZyBkICAgIeJFgpHcvS9jNW7mPFmbKpJdG3Rf24IVCAzJF/gitvRFEFtWIrE6itfkw3o8XR+tt3h1nHN9ibtrtY0WoJ/HddgpImPwscEtjI9FPfWqB5sFj4RWfQ+YrX+G2detgfLycqW+KznJCTIklkJ13BBGYFS4toTuNNeguw0JtY4jkOhApduCVotWNM1vzIiThY+iOJobMppfyN9zBSv6K31q4SQgICAg45+gBHqWVET7E92hitNz3tBPlQO/ABxk2ofrIlpBuaQ41yI00jNbbHkQgKXwqWQlZ5i5Aa3oC0a4VnD7gjt+Aj/yMI6V3O8Dtt8Niq9+ycIh99rOf/eyC/wpAfT0MDsIDD/AEysubQcpuESKhbjQ51pF9I760Tcx9nnOHO4omLIlILoMPq21DsjqCiHMIqcYiKpi9Gk38BYgox9yxruXH1FPPw1y1YNMQEBAQcO7QAzxMLfdyLX/LEKOMIfK5Eq2RPwR+jIRENYqH2IUiNmfQuluDSNJiKvrQumnpDDm8Ba4arbUziCSjaP0+7F4vQesvyLRaQpa8pR0dNH3ta5DJLNRklHHOFB8An/gE4//4jwweOsQh99Fq/MBr8E0Nx5ESXIXk8FF8SsMBKFcOH8V3+B1324EmvQFdEAuxbXOfTbpjPomijAaQyTNGljv4DDNU8b/58AJNQkBAQMC5QA/wPRr5DhdzD9VM0IAvJWbtgbYhgooj8XEQrYkRtCbH8Gougwix4F7Xus/70Tpbh0i10W0zhG8qkHXfX+o+34mIcCVwaTxO8hd+Yd5Lkz0fzo2Pz9DRQeZXf5Ur43HqETlZgek0uuOwRMchpAp78Unt4+79CLqkFkFUQJNbQheq2j2bbdqcsnYRR/GO173u9XHgu8DvMUyJ/8KVfArK9BwQEBDwUsMDtPMYN7KN1zL6nOU6voyIzgpQW1qZBRom0Lq5BF9g2iLxLQ9v0L1PoKjMTkSgzW6/duRqSiMz6BByO8VRUZFfBG678kq6f+M35nf4PwXnlvgA7riDjiuv5JPAG9Ek1+OLU1e7910oCAZ0Qa7F15CLIoLMozsLC4QxU2cETXoe3ZG0I7/fCnyZnUvRRUnhfX7N6IL0keXn+BMu45vuFwMCAgJeSuihnid5Hd9hNUeYwfvwDP8XJauX0PqYQcoPRGJWYSXqtomj9bEeL1asy7r10bsWuY9MgMSQQhxEK2kHsrTtQ+v6euDSujr4X/9LGQDnCOfW1Aka3Be/SOaGG7h+TM1hq9CdRD8iv2O2KbrL6MAXPY2ji2IRRjF0ISbw6i6BtNpJfMqEXZRqt81DSD1aAEwJuNHtlwVGmOJaPk2JAbbzfnQvExAQEHC+o4cY/8qN/A3d7COBj40wPALcj4isExGZWdESaC1NuM8jaH2NIuuYRdAPIeGSwBed7kJrqW130n3XjtbwNnxQ46XA9UDmc5+DDRvmexJ+Ks5dcMtsdHXB4CBVDz7I6lKpnHc36B4DaKIPI6Ja7T4fQpNYi6I+L3eHM4OktSwyU2bCbdOBLmDCHdOKpx7GJ8JvAv4TMn02Az8H1JFjLTspchHPcPHCzEVAQEDAvOEx4Bts5n+Q4SDj+ICTEXTT3w/c5943orXxABIfcaTgrKfeSnz1rChah0/i4yIK7him1XrxpLgXX6IMd8w6tH5vAF4DXLV5M7G/+IsFmIefjkipVCq98GYLgOlpuOoq2LGDrSh0Nory7UbQHcGTyM7cgS7n7BJnVuj6KJpk89+V0ERbzbdlaLIn3HOf2++kO76ZV5ejC9gKXAzchqqQ1wGL6eZj/Dl3cevCzUdAQEDAWaEH+BMu5+ss5xBpfOR8Az6q8mG0bqbdYxgRWxKto1ZirAMpuDEkFiyXbxjvVqpy21SjtdTiLerxLeguRZWyRtD6eilwHXBtfT2J3bvPWUDLbJx7U6ehuhr+9m8Z37yZwZERRtAF6sC3GGpBdxy7kQobwFcG2AXlwtc5vG/PzKLW8mIa/R2O4WW9VX+xxMo6fDfgPCLSH6OL2wp0cYR38m4m+Wt+xFsWcFICAgICzgSK4LyRf2EDh8rR64dP2+pJfNWsZvc6gSeqKnzRkEm8Jc7KlcXx7eCsqwLufS0SGEP4WI1OfCf1WuAiJCqujESIfP7zLwrpwYsR3DIbGzaQ+dznuD4aZQW+9VADUm5rUCIleB/e7H5PCXzOSL/b/yQiMGuAexQRmCWyZ9GFtNySbuBDqBBrGinPbUjtHUUX9yBwP2Ncwbu4mb9emLkICAgIOCPIp/c2Ps8V7CznyJ2OA2idrHMPM0vW4yM4TaVVIVFg7iUrIDLm9rGgwXokNi5GJFfE19+sRWvxw2gNvQjFUawBIps3wx13zNsMzBUvLvGBili/6U1cD6eQn1Xvtt5OCbx92KKMcK+T+PDaYXRBTR0m3aMdKcg8KtO6G6nC5YgUZ9CFswLZ1tWhFl3EPHCQGa7nF/g4v78wcxEQEBBQMXqA71PPb3M9nwT2MfQ8Wx5Gbp44WlOPo4hMkACoRmtdLVoHq9C6ad1ujuO7rMfROlng1KBEEx+WLz3hjmHpDilEgCPNzfCFL5zTKM7T8eITH8Df/z2Z5ctPIb96lG5wFF2IFcgHtxZ4FbIr251NDl/FxXL+h/CSPeaOYdskkDLsR3dBj7jXWbev5RImEUGO4pPnHybHe/g0f8EHSJat4QEBAQHnGg9wAV/gGr5GK8NUPc9W4/gE8ykkDkxQmGlzFK2n3W6fKL4GpxFXEVnAViPBgNu/hA8SrEWiJeO+G3T7bEQBLVclk3T/yq/A5Raa+OLgxfPxzUZ1NXzjG2Re9zquH1UnqGqk0LajyWtCF6IByeoq4Ouoj/AUulCm1KbwhVAziCxnkBIESfhJdDF3IPnegS98bTXmrBaoFVTNIl/hp8lxM1/jV9nNV/kGR1nYSuIBAQEBp6KHzXyNjdxFHTlKPNufZ8ijNc16l9pNfRytjxYfYXWLlcssEpt2+5hlbQUyV2aQMNnPqZGcNUjxWVPbGtTI9j3A0kgE3vQm+KVfmr9pOEOcH4oP4Ior4GMfI5NKcRua3B406TNI+aXwwSmvA25GhFiF1OA6ZF+uRYRlUvwEUoUR9xnoonWiCzOBLp6VQMvhnbHWFXjSfTaJwnS/SZEBHuQObuVCvrcwcxIQEBDwLPTwdt7Pq/kX6siV0wWeC1NoTduHN21m0FpqRaKtzuaTKGhwj/tuHKnDKbefFa+2hrVZRJRHkHiwNIh9KE96DOXpXYei8Nm4Ef7wD19UE6fh/CE+gN/8TbjmGkgkAEVtDrqvcmiyu9Ak1iHS6sYHvdSgu5LZg8oj4rwQ1f1swge2WLm0JLpbeRLdpSQQkbYgSd+ELnYb+tNYxNQhoIsn+GU+yiVsm9epCAgICDgV8undwo1EuIf9FMuVVp4LY4iUHkI3//lZ31lxf8vbm20OTaE1cwSRWw1aC2tQ/t+jiPDMFJpG62MDclENu2NsAN6KmtlG29rgK1+BBe6zVynOL+KrroYvfQkuuohxRHojSHZnENlVI+LL44mxGt2VbEdqbBCv7oaRebIHpUZkkFxfhbdjj+Pl/ag7FSuQvcEd6xHgLig7j2Pu+EqV2Mt/4A1cxtfmfUoCAgICLHLzJt5LNU8RRWTT+TxbF1G/033oJr0KWcosbWHSbdOG1rImdJOfRmRo3XCqUCDLKrTeHUWpXo8gAiwhn9861GWniETDm4AvIsERraqCP/kTWfXOE5wfPr7ZuPBC+L3fI/PhD3P9oUNMISVmwS5t6G5iGN+uvhtdyIP4xrRWYXMGEeQQMl2m0AUace/NFJBFJGgJ8N3oDiaGjypN4asQjLhHr/vtPAN8mf/IHwF/z3vne1YCAgJeseihlb/mNr5ALX3UwvMGsvSjdemAe8zgLVgptGb2uec6tP41IzExjfcTmgCodg/bNuceg2h9tF58EbcNiOw+iNZs4nH48IfhXe86uymYZ7w4JcteCCtXQlsbVffey5JsliS6UJ0oFySDlN1eRFZdKGrILmwBf7eSQhcoji5mDEn25UjpNaA7oAn0JwFd6E7UHPEG4AcourPR/eYoushL3Xb7EOkeIsdH+Wcu5ij/zk0UzsP7ioCAgJcSeric3+YWvkQTw9ShNex05NAaNIZEwRF80F8a36KtD98+yGIcEsgqdsJtP4ZPP+hCRLYaxTgcQWtlOyow3YgPgKlDJtGrUa8/YjG49lr46ldFgOcRzk/iAxUtzWSouvdeFhWLjODtxiUktR9CEn0Nurgb8W2KxhGhdaE7D1N2OXSHU8CTqNWis35TjW77g+7z+1H6Qx/6UzW5Y1qIbwf68+wFTlLiHTzKe/kW29jECboWZn4CAgJe1kjwE27hPXTxbcaYOSV5/HQ8jvKTBxGBjeEtYjH3vg+tV+lZx5hAKvEEWussIr4arWlmDsUdYzdeGBxF6+FxZPnqQKlmbwBSkQisXw9/8zcvWnWWn4bzl/hAtTyPH6fqscdYXSyyAknq2UWma5DysrQDS8Lsx8v5cXSBrUL5hHvYXdFS9AfJ452eRXQxH0B/CmvGWIXs3X3ueyuqnUJ/gB5EgBfQx4f4Oo1M8hOuDuovICCgYixhKz/DW4nwDKB1zgpynE58PXiym8JHrlsSukVtWvugenSDX3LfDbjnktunHomJMbd9BK2h1ifV0iCG3Ocz7vxWonV1A1C/bBl88YuwadM8zcj84sUrUl0ppqfh1lvhhz+EnKhrK/AddJGb3KMOkZ2F1R7Bm0KtVBnuO2tsG0NO2WuAe5FN3JrdJpHzeAiRm3WFaMQT3zF00avwVWYW482yVwHrSDDAq/lP/CX7QmujgICAF8Ab+XNey2/Qw0i57ZrV3ayDU2xIRnqDSIFZM25r0TaJ1rUsWqcWo1iJOkSGR/DCIOqOfSlaI4fRmtqA1kRTgovcMR5Aa94lwDtQtwUAWlrgT//0vPPrzcb5T3wAe/bAz/88bN/OeKHAvwF3oz/EBnQBrbW9KcISvov7SXySJu51Dl3Ejchebf2h9qI6ndYTMIcv09OJr3bQ4Y5rVWMWI7/heneMaqQAW5G9eyOL+DX+gG/yc/M8OQEBAS8HVHGAj/CfiXMXxyhSwEdunk58PcgVM4QIaQS/TtXj62Se4FTSs0Id48jFY6kM5ve7Gp9DvRfYiaxZFvhXi29AmwVuAn4GiYcqgExGEZy3374AMzR/eGkQH8Bdd8EnPgFPPsk48H0UVGLO3kF0IavQn8BIbpf7rg9dvCwiK7N117h9GvB16ZpQakQOKbosvj9VCd/hvRZJ/qJ7b1FOA4gUrSdgK6oes4wE+7iVP+RvmaFm3qcoICDgpYmL+B5v53aO0sMAUmuN+HSF2cQXQYRmEZxGQkW0/tSgtdB8ggl8G23rwl7AN4sdx8c33OZ+c8x9vscdbwZf7nHI7bcY+BSKlUgA1NSoBueHPzx/E7NAOL99fLPhIj156CGqhofLKQknEHHlkFQfQhexCxFVFl1k601ltmyrQm5+PwvztcTNHJ4QzSxag/4MFjmVRF3dp/Ddi2tRGZ/97rgRRKSNwCRFSuziDXyNI1xAP+vmeZICAgJeSkgzxif5z1zHpxhlBPDKbLYvz/x0Y+6RQzfyw/i1L4PWtFG8ikugNWwdWoss0M9EgN3QW0rCMCLZKaT46pBrZxIf7BJH0e6fRn69GEAqBZ//PPziL87j7CwcXloRF297GyST8MEPkhkY4Hr38T68nDfHbiO6IzLz5El08SJ4R28MHxQzjP40VrnAgl3MRGpdHmrdZxPIlLkVkewqfNfhY2hiX4XP/7N6Bf1AmiP8C+/lTn7I7/BfmXjOOK2AgICXK+oZ5vX8FW/m93iCfrZA2bT5fDl6oLUkhnxzg/iKVg0ozsBIsYgv0J9BZtHZjbrNzBlHxNeJT/eytm9GckmU17wUrW81wAeQiwhQ4ZFf//XzogZnpTi/KrdUgttug7/7O2hsJINqwXWhizyM/jRWc9Ns0mvRn8KS3+Pu2UyZE+gOx0KFLf+vHk1QFJ80P+X2tz+nVY9JIcX4NCLANCK9PqRKC2673Sghfw9TfJQv8jgX8V6+Qv1PLT4UEBDwckCSad7KN/g/XM/r+CQ/oJ9n0M31T6vEAiKiE8gNsw+lH1gx/3ZEepNovSrg/Xcn8WbPBnz7NpA16lWo0op1Vci555XuGCNo7WxGavRiFNwCQFWVXFCf+cyZT8qLgJeOj+90/NM/wUc+An2yWo8Df4Yu8gZEPpZo3oK3g2/Fq8OT+FyUanwyuzVqXIH+YLvwStEa4UZQdJORZzWywc+gP+FK/B+1HXiXe70X3/X4DciRnKaK7VzCH/MrfIc3keXFL+IaEBAwv7iEh/kYn6SBB3iAKQ6hG+l2+Kn9XUrIinQPunku4TucmxulCR+VblWorLpVCt3QN6B1z6I1o+53V7pjPYaIbjFaB8fcMVPIj7ceuBx4LVJ9xONSep/73FnOzLnHS8fHdzrWrlUE0YMPwtRUWbkVONXnF8c3lJ3E952qR2qsH11kU4Mg8oq594dRsIoVeLXjVaE/wnr3+VPuWLj9qvF1QwvuOI+47y9ByabPIF/gegpcyHFu4i6u5EF2szokvgcEvEyQZJpP8xl+jY9zmJ38gBzH0M1vOyKa50MBWZF+iFIKjMjq8Y252/CpXBZwZ8U6Ioik0m67OFKKi9AaVkTrZqM73kl8bETvrO9WuX3jSBDURKPwqU+9JEkPXsqKz/Cnf6quDuPjjJdK5WjPLF7FDePTHayE2QOoIss4+vOl0R/HevXZvpOI+Cy6c9Idow39AUDk9iT6o3WgP0oX+lMNutcj6M+53L0/6vaNoXSHm9GfK0acQRr5Sz7E7/OZoP4CAl7CuISH+Qv+A63s4F4KHEZrzk+rt2kooHVqN76xdhpYhtasZnwtTcvls0A7C9AzwrsIeB9KTziM1p09iNzyuLqa7rhrUCWYo2g9+xhqLVRGNAq/9mvw3/7bnOfjfMFLK7jlufDRj6qN0a/8CpnJSa5HF38XPtilAf0pxpBtejNy9lYjIutBZGYtjPKIAK0lUge+y3DafdaP/sD1+BqfVv084bafwZtFx/F+xUEUXrzXnetORK5LgTeRp4s+PsXv8xa+xS/zx/yI68krYDggIOAlgDRjfIbP8Aa+zHEmuZP/196ZR9l1VWf+9169mucqlWTNkmVZlmVbtuXZkEBsbBNM0iEJ0GkDaTBDh4SMkKwEslbmteiQYYVOaKaEIU2ANGk3dAzGJoBnSwbJgyRblq3JmkpDVanmqlfVf3zn076vVJJlWbY13L1WqVTv3Xfvfe+dc76z9/72t7XmHA+BZRKtEfciDsFw5rlqoqm2hTXcKNtRrgMI2FwOYRCckV63N53LzbUtPu1WRd0IaC9EgHdd9uaKRfjgB09r0IPTOdSZtSuugM5OuPdeakZGWIFi0X1UljuUUP5vlFBccZ8pt+1oRkDXhgZce3rdoXSpFmKADKb/DxGDcygd6+a21sg7kF5TRzBER9Fu7DI0UPcD307HDzPJFezjF/k613If25lPL+25B5hbbqewddLNf+bL/DNv42Lu4l7GuAt5TyW0xhyLwOLQ5v3IMzOYudzAjE2nZxahTf4+BJigtagW5eLmopSK+Qn7CJWrMQRuTuO0ICAcT/f504ibUOubKxbhPe+Bv//7E/hkTi07/T0+2wc+IEmzP/1T6O6maXLyiHKHOYiWu44IQc5FAPQYUYtXj5LJjchDNCNqBhpoW9OPd1cGVXcgLiAg8y6qFwHpJQhg96fHRhE4X4/CC4+k825HYU+AOkZ4A3ezike5i5v5V97GXdySA2BuuZ1CNpPd3MaXuInP0cEmHmOCH6IoUQmFGqd6eaNobSqjDTpEmHIQrSVZ2v0E0ZlhEK1RvSiV4zVnNL2mLf08lp4fQYpUM9Oxm9Lzc9Aa5rZDBbQRfztKwRy+vkHv058+sQ/oFLPTP8c31T71KbnhO3bAxAT9wL+jXdE1qLThR4hUkmV97kQMpkL6/zMI9NoQccU5wqHM7xqihYdLIJqJXoCz0EDchDy9eUTNzBwU4iwjD7MFhRdG07kNykVEhlkKLKHEXmbxANfy3/ld1nLqNHbMLbez0WoZ5p18jvfxGRp4mvsZYh/Ky5lscjWa91szrxtFObbdaM0ZIMh31hV2Pd1w+u1cnMWnQZvo4XQOL+QdSELMf68jusgcQJvs2nQtR6AWETV6HyFTrgAqWXj/+yU6fYbYmQd8AN/8JvzWb8HmzTDl7d2PBlwfEe4sEV0clgBfQcB4Exoo69DA20kotxRR6MBSQZPpHG9FcmpDwBtRnP5xNLicaL4RkVy+j3ZpI2in10SonZ+f7sUyRGVUP3MxMJNaDtLG/+KX+FP+KC+Azy23V9hKjPE6vsuf8/u0s5HNjPAfRB6vCW1029Gc3YWAz4C3E4GWiXRjRO2xVnpaYAAAIABJREFUu5+XECC60NxNsS2k0YPWIusHV6Xn6lEI06pVw2htmkAe4tx0rEF3PgLFPcgzrdBeaWyE3/gNRdLOIDszgQ9g0yYJW69effihrMbndKzPWWgQfBUN0m1ocDShgVWdju0nlA1q0MDy7mke0d2hLZ1jEO3UnM9bggb+ajQxelHCeQYasBek++ggJIT2pvP/JwSaUGCSAjuYw1/wUe7gLeyn6+R8drnlltu01koP13EvH+HPuZJH2ZdyeI/AtBqbjQTwbSJAz+FHi2wcRCBHemyCyOfVEF0XqggvrSc9b0lFawVPoLVkEK1r56M1ZitR02zy3hhaj5aitecXUX4PkETkRz8qAuEZZmcu8IFaGr3znfD1rx9+KBv6bEaAVIUG0RIETg8hkLsTDVTL9lSjwTKOdloTaMdkNqg170wjtjaoZdDmoTxfNxqIm9FgHkz3cB4SiTVpZll63dOoXMIapIuBn0/3oxBIFZtZzFf5JT7H7exk/sn5/HLLLTdAIc03cQe38xmu4mH66ecp4HvIyxtk+kL0RgQ830EF4vuJemPQeuFuB2aBjxJkFnMHLB/mkoZmtNluRhtk5/8WEeS5p6kEyjG0ZliX2OHPN6R7vxRi5XA/vVtvPeHP7FS2Mxv4bL/zO2qVMT5++KGpHR6q0a6nBw20GcA3kKe1hQC5vQTLM6uLZw+wgyC0uMdVbXpNHdH80QM6G8JwO5F9aNf1VgSM29JrFyBQ3olA+qdQztKaeaOU6KOVf+dm/obfYQMX5WUQueX2EswhzY/yZ1zCOnoZYBNwDwKyXkLlaSpbcxTN3WG0zlgE3wuuG79WpfNAaAP7dyEdUyBCouOEt9hOdEu/GAHX08iDHEv31pM5diBzzsuBX2NKPg9g5Ur42tfg/DO3f+jZAXwgl/0Tn5AXmCwLfueiQbMD1dnsB+5Duyu3/9hJ1M70p/87jOm+WA0I7Fwb4zCEvTMrLvQQdYLuhlxNlDU0A1eke2lD4FZEg3oHmkzVwFUoef6TuEtzkUkKDNLAg1zL/+CDPMBP0EvbSfgQc8vt7LBOurmJO/kNPsEKNlDNGLtQw+qH0BzVcUeXGzPwWbpwO9ENpobokFAk5r+FMuz9daRj3TqoPZ3vufS7AQHu9Wgj/Cxat9yZ4WLE7HyeKFJfjtaLt6djKuymm+COOyQ8fQbb2QN8AH/0RwK/Q4cOP2Twq0NhxYMoJLkNeXrO/bWisMaPiM4Pe4iOxD8mGuIOoh2ba0VKCNyGEKDVItDzoJ+Vrt2CgPYxglbcTlCYPSHMGh1AnmkBSae9Be08y6kb4SQF+mhlNav4PO/NdUBzy+0YVmKMpTzFu/gcP883mM3zHKDMBrTpfQgBWB1iiHdQydScaqPIK9xHqD81EoL4Zm/WovVikJAZq0ZpjXoCQAtEZwWXLXQiQY4b0uP/gtaqhYRovwvWL0Gb6cXAtUzpUFAswm23wRe+cPwf2GlsZxfwgSTOUq1flvE5FQA3oYHh3N85aMdmoenvIRCbjwbzvUSfK4cWrG3ncEY5Pe56GTOwatLPKJpM+whptKr02DlowF6IwO4xtOtrI8S26xEA3piucwnajU5QZJB61rKSv+AP+A/ekIdAc8stWSfdXM+93M7/5GoepoV+ipTZi3JzDxH97dqQx5Rlah7NnkL6vMNo7rtrQrZTelM69nm0dpjF7QiPhTS2EAxw0jFWipqB0jQTqPi9A23ed6G16Tm0vrwDCXssYQro1dWpDvqv//p4P7LT3s6cAvbjtV/7NVi0CH7912HrVpiQ/+TuwwZAx+470MCymvkyAhTrUKihDgFTCQ3QcSKk4bBnHVGTAxHH9ySYgXZ5TmTXIvDyLrA3/f1axOzsRjnGmen/k+ke1xBdl9egUOkCJjifAa7nQf4PP8vjrOBv+C2+yxtzJmhuZ6XZu7uNL3Ar32QhW6hPsLIbpRTu5tjElaPZLgRALk53DbD7e1YRClBl5KlNEKkOd1EfR8BloXuXULk20CVYc5BHWo+iT77GIrQxN1v97dPdbFsbfOxjKv86i+zs8/hsmzbBO94Ba9ZAuVzx1NFyf7PQgLwHeX9u0FiHwPAg2gHuTL8LREjDBJcRKkHRCWyXVsxEE60XgbGZXbUofDEn3cckmqCtRBcIg/U16dyjaBJehcKgpinrvkrs5hy+xZv5LO/NiTC5nRWW9e6u5FGa6aOaMfagcORzwA+QuMQYmmvZ8gSoLFGwx7cLRXp2ow2pRei9BjhF4RmW9Tjs5Tkd0kowwUvpnrx5drH5z6X73Ye8RYhuCybgzWOaVkJZW7gQPvnJM5a5eSw7e4EPRHT50IfgS1+qIL3A9Lm/K1AubzUCmPko/LgTeV3b0SDsQWGOYaI5ZIHoiDxO6OPZTH5pIYStl6T7GEzPzya8v040UerSMYV03S4U6tyPEt2b0/2/DngblRNYhfdFBqljAxfyed7N97mRrSzKQTC3M8ZKjLGcJ/ivfJ6b+DYL2EZtiq3sQXP1boKAUkARmKPl8bLA9xAqjdqLgMVlCRalHyGUWbzR9cbX6Y4qQkzaovpWY3ERejVabwrp2HK6t/OBfyVyft7wdqHyqDa0Rv0smfq8qiq46ir4p386o5mbx7KzG/hsX/kKfPjD8PzzRzyVBcAL0ATZh3ZXzv3tIhLZLQioXKC6DpFgnJA28GXZXC5idX3PeDpuCSK4bERgeAmaAFvTNfemv530tgDutelczyAA3JvuaS6q/1uFJkUjoToDBcapZi9drOZKvsQ7eIDX5qHQ3E5bMzPzvXyKS3iSegYoUT5cepTN4bn43CIUUxVXxgh5sCoU9tyFcu370PzrR56Zc/lZD68PzbcWIp/fS/TKGyI6LNSl4zqJovSdhGJLJ9FLdB5R9zeB5v9rUNpmWjHs+npFuv72b8945uaxLAc+26ZNEmF96CGJXU9j9yOqcB3abWXr/qYCoHdfe9FOchOaECNoEowTorVWgnGNjr3BKhT2aAfehHJ7dyGG11Nod+oieXumMxGoLUIe3ziaXE+l669Iz+1HQLoYeYiuUwSRYUYpsY8u7uc1/DO3sY7L6GZm7gnmdkqbvbvb+TRv5pvMZC/VlJlk4rBMVzfK3f2AEJFeQqWIdCPK569Dc8evq0Lz6TwUzrS+7wTRlaVApXlutaTXerO6jyh16k8/bqQ9D21g56Z7GEcRnup0XxvSfbWk1yxBBeg/j+b3tDZ7NvzVX8Hbp832nVWWA1/WhofF+Py7v4O+voqnpiq+LCFyf96xTSIA3EzU/O1HA7OHELg+QAhkz0GgaM3PPek4W4lIdi9P11qCvNB+BIomzTyLJsaC9HgBtRapAb6b7mUJkfsbRRPrIgSACxGgr0jX1oQuMkYNu5nJWi7nW9zKffxEHg7N7ZSxEmMsZAuv4x7ew2dZznrqGaHARIV3t5EjQ5rzOFJE2rqZ40hn13PXeTiLVDxNRGh2ElGcRmIT6w4LXWhT2pmufZDY8Jr0UkTzfCFib1vVaQsC2gYEvtVEXu+YJQq2qiq1bvviF8/a0OZUy4FvOluzBt71Lli//oinpsv9jaNd2Tw04C1mbcZmLcqxVaGJt4GYENbhc/7ufjSoze60RqhDLG1o8hXT9doIodoNBMvLZRMXAbeinem6dJ1daGJvRxPydek8xXSuS1FYdymxmwUoU2KIevbTzlouz8Ohub0qVmKMdg6yiM3cwF38NHeylKdpo4fi4cC9NpFZ7y5LWJka0tyGanT7ETCaIOLIjeUITYObRBvJSTSPXGBen44dIHJ57gBTIjzFp6jM9bWifF1DOv4KoixqH5r/Tk00IAnDY5Yo2PLQ5rSWA9/RbHgYfvVX4ctfhpGRI57OAqBbDzUg0NpKNHM8Fw3QOem47xAd2+uJJrX2sjYhYNpN9OSqI7RBPRHr0zmty3cumkw70CQzIDYiVmcZ7V4NuK3p2rNQqNMss/1oMXhLOo/NO1LlBMVFHafEfjp5iKv5Iu/iR1yZh0Nze9msk26u4mFu5Dtcy4OcyzM0008pwZEX/t1M791NR1ipQRvJhxGYlAmhicVos2m5Cwva70JzdhgBnCM03uQWiLIlF6J785oVurcsWQeKzFyJ8vI70TwbTffmjusqTVKUZgPTdFLIWqEAc+bAX/5lHtqcxnLgeyE7BvEFpi9834QGaguRd3MYcz0KiVjb72k0EZaind1mNEHcpHYsHW81mKy4rUHQ4U3Lnfmabcgj3UuEUmvRxLGmXzPw/nQ/j6MO8IfSMZegybWCKKFw3y9PWhAztEyJg7SxlpX8O7dyDzeymaU5COZ2wlZijC72chmreSdf4EoepYN91DFGcQovei/BZH4h724rArk+oh72IJofrYSoxAw0FwfRPBlPr9tOAJujIZYmdDshiE7os4lUSFW6Vk26ryZUm7sinbsdzd07iSiPQXYOCoPOQutFRSeFrNXUwDXXwGc+k4c2j2I58B2Pmfjy8MNBfJnysWVr/xy/9+6uCw3gfjQZd6Ndo1sjdaIJtIuo1wHtJuuIjsz7qQyPZM0g2IImr5lhhwi2mRPv1YTm30zkEV6MgG87UYDv/MUlKJxyEcEGhZj0WVHdSQqUqWaIOp5nLndyM3fxRjawIvcGc3tBKzHGEjbxU9zNm/gWK1lLOz1UUabABIUUdzgR787al/egeTqKxvdcBFBmU9cTgvFrCQLbPkL82R3PXaIAAWytRNsgi0lUo7nWiOZxNRxOEMxF82odmqPLEAjWEe3Ozkf6mlbcreikAJIcKxahtRV+5Vfg938/D20ew3LgO14z8eVTn4KDBwV804Df1JZH1WiwThBd363W8GM0keYSzK42NDnGCa9xGOUgzOLMqru7HMGPVROEmPmoHGIAsVHdv6uQuUY1CtHao3NTSpdK9KLJPCPdZ5YN+oYpH5GBMHs/UGCMGg7RwlYW8CDX8m3eyOOszIEwt8Ne3XKe5Cbu5I3cyVx2Us8wVYwdDq87wjEJh4vNX4x314vSEIfSay0F2IIAySDlTgiuuX06Xc8RF6cK6ggNXQMh6fHmdJ1y5rlWtHlsAJ5Ec2lZur/5aN4/guZ6K1ofZiHAuxqxuqfN4YEAr64OLr9csmNXXHEcn/zZbTnwvVhbswZ+8zfh0UcFhtN8fFOVX0xdntr1fTcinXSg2P4eNGlq0I5xhAibbEEA5r/NDp1AE8yLg++mHk3+JjTJ1qfrlwg69QBBpy6hSdmBwqbZUM4wsTu14O18xDKdTYhsdyCvMFufmAVpNc8tUqaKARrZykLu4zU5EJ5FZqC7iHXcwp1cy4MsZCst9FLNWEUkw+PI6ka7OHHvzlqbnUT/OgPSuSg3voWQFhtD4LWPyOG54NykF89liHFel543GaY3HduMQGwB0QHminRvc9Fcvi9dtyPdlzsuHJW4AgprLl4sKcb3vCf38o7TcuA7ERsehs99TmUPzz47bd2fwc8f7rNUdn13ScM5aDJ9H02CfWjwt6HJMYQm4HB6fXP6GUMTtYwmoAtnHXK03FE5XdP1gSbFtKEJuI/IG9YT+qMz0331p+eKaNIWkPBuM6JP1xE5yCEEhlchpmhWlikLgOMUKDKZgFGh0UEa2cZ8VnM1D3A9a7iC/XRxkPYcDE9DM8DNZxvns5GVrOVC1nMeT9PJgcPlBtoKTRx+XSHzAwF2O1Ch+VNoDC1DGzuH7e3d/RiNaZPD3DGlFc21mek5g5YLyJ8kdDldhgABkFVoE1lP9OusRyHKh9M5zeh0BGUcbS5rEHGlHs2belSPaybn8vQ+/1+61uuBn0GbyKkpjfigCtDVJbD70IfgnHOO/mXkdoTlwPdSbPdudSn+7GfV7eEoNl0I9BCaGNcjEFyNAGgHAsAhKnN0Bq8mNLGcd3BH5R4UNh1I12xLr+8m6ouqiaR9kegpOJrO0YwWhyY0Ua0TOI4WhTnpsacyx1q27ZJ0jLtZWEHC4aCsVYZCK73DSQpMUGKUWvbTyV662MhyHuZq1nI521mQe4anoGVDljfwXV7LfcxiDw0cookBahijmLpFksnTZaMUEDm7HSgKsg4BmXNudajG7QoEFj9Cc6Qh/f85NEbb0dg0K7OMwOh16ZxrEMhVEXNnjPAw3VnFBeu9xKbRefmFCIA/k+7xHKK9mEkxc4h8utMLDcAP06dwQTrGxe+XAx9Em86jWl0dvPWtSr3Mn3+sI3M7iuXAdzJs+3Ylk7/+9WlLH2zZEOgIGvAXET27SkQOsBuB3zDKTXhiuo2JW5aMpOca0OTanh6rRpNzP5GvgEjAV1GZt5hAE3Zueq42c0wfQbRx93mz1W5ADDN3il+Ujn+MYLq2osXK7FBbliVK5v/Z3OUERSapYoICw9TTT8vhYvrcM3zlzTV0nXSznPWs4HFWsYYL2EgnB2hgmCLl5ClNprbIE4dzdP6Os793cWTOzvWjE1SGMreg8WxFk9UIlCwiARqXK9DcegbNiUEEqjMJTV29H13D5QUuFXIfvHK6lueni9Cr0zXPQd7bKAHOK9PrtiBPtAl4kIj4DKR7aEnHtqJoym3pWke1YhFWrIBPf1qszdxO2HLgO5n20EPwvvfBk08ebnc01bLe3zUIMO4nmJwOwTQhb2xbenwLmuD9aIcKmuADRNuUIprcA+m89elv9wasIXIXVotw+6MxNGkNrBAeolUsLIRrtmcDqgG0YsQWtBAcRODufOW2dM/twBup1AqdalNB0I8pKOb/hx85Rg29tNJHM7uZyybO4xmWsoXFbGB5DoonaFMBTiD3BMt4mhZ6aaKPBgapZoJCan2snNzE4VBlNi+V/S79+Avl7KrT4w5lrkVenTuj7ECg1YEApB/loJvS8T+BCCN3ofFnEWgXl3tzZylBCHLKSDqfC8pNfqlLry0RHl53Ovec9NgF6XU/IIrWN6Zji5nrvAZtCAGu4yilCcWi1pKWFtUVf+xjeR7vJFgOfCfbhofhj/8Y/uEfoL8fxqf2YQibGgL1xOogmKBbiC7wixHL7O9RrWAJTcqpZRFmhDWjvIhDOhbHHSC0Qt33a5QI7TjnV0Ww0iyE25J+D6XnG4mwzEw06cvErngsvXYLWuhaiPZObYR24aUoXLpwymc0nTdopp8fNTCWk08xSg1likxQYphaBmlkJ3N4jnPZyIVsZkkOihwJbot5jiVp6zCHnTTRT0MKU1alTxQm0//Db5vqyUHl92Yz0D2BNkPrkSfVxPQ5u7UoZ2eCyCZC6u8iNAYH0bi7EnmAG9F4rk3H7Uuvd3eEJgRiw0SjZ0cYZhBzpEikDdwTsyFzzUY0T+eme6pHm8Dn03FLgX9O9zIHzfVzUd5+KfLwXkOlPuiRX1Cp0svL2ZonzXLge7nM7M+1a2Fo6Iief1mbTgfUTNBsF/gbkCf1bbRobEIT0vqec9AE7CXyIKtR6GgYgaP1QvuIsKfp2dlQpsHRCvPuLFFPhIRq0UIyQcgr1afHL0QTe1F67GEU/mxFnurq9JpOtIgsIAg9C9P7X4aS/0cz5wT979TnJg6rNU4eDraVKTJOiXFKDFLPKLUcpIMeWumhk53MYTvzT2uPcSqx5AI2MptdtLOfFg5Rywh1DNDGIUqMU8cwJcYpUqYm/S4mmYIiheRrB4iFmHnlY1ngg6MDHcTmZ2rObj0iqYym/+9Px88nNjyLkUfo89agcbWWCGE2obFTSI/tIxq7mvDi2j2TX85D0YpDxLwbTOdzaLMPzYOfTOe3luZ+1OjVJRP7UC68k9DOnYWiHXPQfD7miKqrg44O+OVfzr28l8Fy4Hs5zezPz35WRfADA8c8fDomaAuhyH4FAr370W7T3defQqxQe36NqH5vMaFQsRNNPtO1n0fgNooWAFO8XWphtflDhNdmVQrnBV0Mb0B1WcTsdA/NhAive5TVovzLY8Su+wK0Yzel3DnIq9LjFuc+XrNnOJU4Mf2xBcpUJUZsNeOHG9dIz3+UakapZohGhqllghKTFBhPHmWBSYapZ5Qa+mk+acA5nTc2j23MYRdNHKKGEeoYppy+rVLi8zbSRwe9yUsTxBcoMAFUUaZEmSrGU0F4fF5ZwJqah4MI0U091pbN021Am7EdKFx/iEqgW4i+2+eRR+fec8+lx4bQ2G5BY3QEhRB7Cbm/jnRsNxqPFoYeQaDXRoTxnyd6WjqsWUUQWDrTz3ICJBcA96Zj3cD1vHSOFuC/AV9P93MwHfc6Qqy6kD6PhagGbwkvUJZgK5Vg1iy47jr4yEdyL+9lshz4XgnbvVvNbr/4Rdiw4Zjen21qLeDVaGJ/Fy0svWgi1qLJvyM9Pk7soqvQRHSYpxqFc1zIvgktTK3p8R0IJFuJbtCDaCJb5cLU7jEqqd5eZFwQP4Z26e46b0HfSQSus9DiNJR+L0vvo0h4BaX0/FwE+jOR4PZRKd7HsOmAMPyYFzcFJimkH3uV1YxSdfjx8DJLDFPDaCrgH6IWKCYQFFiVqUoANZHYhGPUMkIDQ+n/o1RRZoICVYcVSyaShzZBkXLFVY/1vivDkAUmk6c8HfP2hWwq0D2KxsggGrutRGF2PUcC3TDasGVDkMPptU0IhEaIgvNhwtvqIMZJP9pgzUbj069vS9dz6YI3U85PW2KsnahdbUv359rVgwhQryRKGOahceqUgev86tCYLqG5tyrzOUwVejiqtbfDzTfD294Gt9ySe3kvo+XA90padzf827/Bn/yJwPAY+T+o1AG9EXgAFbmamFKFJuBONFFdKjETgdo+Ip/hYuAuYnf8HJrEnQSA7iFA0qHLRjShnyFyga5c9LFeuCaIekBQ3m4Ehby8sE0QbZPGkXf6kwgwH0bhK+/MvXtuRYvnm9ACY9B/qTYdIB6fpxgASGIvGgyLhyFluhCsOY4cDiHGGcMjyOY0fabJw6/xOSQWV6TSk5iOKXsim4WsGeh6US7th8TifzAdcxHRZWQGCnP/EG2oTH4azNxTVvmkiwBO56rNxvQYG0i/TcIaJJSPLkTjd2s6dyca39ked54D/nsWlRJlu9L7G0NeXRMC7xXp8cfS6zrRPHG3dYNnE6GRu5gpXc+ns0IhVFeuu07M8Esvhba2Y70qt5NgOfC9GnboEPzhH8JXvwoHDhyzBMJ2rDxgVhFmCSqA/U762Z9e67ojd1wYQLvbEbQj3YbybtsRWDrvMYQWiIuB7xGMUJNdHO7sIFoxWRnGIDiRrmP9TytgNCIw9EJhVp4fc3+yIiHs/V/QQrozvV/v5HcQdY3noIVwNlpQl7/gp/vi7HjDqC/mfF6UYXrgmvp/Mo9l7+nlsh+gcbYLhdV3EV7+bvS9zkMg1I7Gzma0WSojkOohxo/JVtXoe1qExvYgEXYfQoDjsXKAEGS4AI2BbQhw5hH5uTr0eWZrWBvTjwG3M52/h1Asqkbjv4wiC+em85+H5soaNJ6Xojm4J51nGWJlXkBluc4xrViExka47DL4gz+A178eqk+fPPLpbjnwvZq2Zg18/ONw//2wd+8LeoBwbEWYDjQJ56DQ01NoAdqGJvsKtFjsI3qVtSOiwK70mu3pcauu9KXzDqGFxXm+KmJxmiSknEbQgjZBFBJPpvOYUOMyCedczDr1wu4ehRb8bU733AXcTOiWjhBdJobS6xajsKjZrs+nz2M+kWeaT5Alcju2bUMlAU+gcfEI0UDVVP8JIg9sFvAIGjcjCBjdaWAoPd9MhBQbEav36XR8J9rkdBAF6jUI+JyfW4rGwK70d0u6XxOvLMln786anFYw6iDycbPTNYtoAwXwPqKZLMC/pPNdigCxCo1H17EeF2QVCpI4bG6Gq6+G974XfuZn8pDmq2ClFz4kt5fN3BX5zjvha1+D731PAtjTSKDZmoA3p//3o4VjI8FsW4h22QcQeOxC4DMHTe7H0UJk8elL0YK1lQhDzkbA8AxRDD9IdH7oIRY664oq6KYB5R19E/Iwm5AkVE366aLSO6whJNcMRtZD7CJqB7eihXcEhXJHEbBNEB6w+58V0vG70zHriZxlMb2/D6T3f7bbHrTpWU2IJXhsuV7TBeL9hPfegz5fs3+9KapOf88l5PYKaOPhfLFrVS2w4Bo+0LidRxBQagmQNEnlWTSGfV1/r2691Zmu0UiwND3OrENroWnnm2uJcbgfRR9cZtSJWJy/QJQxHBdZxVZVJeLKqlXwe78Hr31tHtJ8FS33+E4V6+lR6cMnPwn33vuCAJi1bC7wGkL3swvt0gfQLtU9xXpR6GYILfwDiCl6AHlWjWgX68acXWiBs3LFSDrvTqIrfB1aQN0hepgomp9M5y6kc1n6bCfRwX6IWMRaM+d0E14X5rvdkhdTywSsIog4nek5E3tqCVC1Z1CNCu/b0z3uQ4t+U7o/y1eByA3ncfJDpifbDGDPoM+sgN7fBeh7bk/HbET54keQR3cAvd86Qti8H30e9qYm0HdlMPTmhvScQ5dt6fehdK556XoT6PMzy9K5MjMu3V18DH3/b0qPf4/IUx+iMmLg411aY9JKVzrGHmKZ6FZisOxI9z6Zrn1+On5ner/N6fnz0FwoIwLMcRNVbMWiuqCvXKmQ5hvekIc0TwHLge9Us7ExeOop+Pzn4RvfgB07josFmrWp4VAX1xZQ+PMOBHxlRCzZjgrdTTpZjhaQ59Nrd6IdsPU/J9Fi1oM8ujnEDtmengGrEYHWAUIOymSVElpMhwivoZ4gK/yI6CjhImI38DShw55nPSFZ1UIw/Owx2GswUcILZrZ56Dha8JyHtFeyPJ27j6iD9AI7N/3+BRRmfblsDyJrHCKEDryJ2Y1A7EkE9JNEGUwDATJt6fgNRD9ICE+5RORuzyMYiwYydyI3uakafY8OU/vzrUXjoR2BxtPpvtyJfAchY/cE+m4b0rWzuep9BHHLnpi1M02GsRCD88cGuUY0Hk0+cZeS+USnk0aCHFZLCLRbNuxFEVVsxaLCmYWLKfsZAAAQFUlEQVSCeuOtWpWHNE9By4HvVLbubvj2t+ETn4D164/bAzyW7UeKEt8ngMJMuxloMXLvQPcrewAtvK7xG0Ue3zgivVgazYtTQzqfwa4HgVAVlaBVIFh3Do+V0CLlnmZe3Jw78j3YE7HQtlszOfc0SdDYXUtYQ2UXe4derQnpsG7Wk3BR/2jm9xih+DGZOU8zAbA16fMz6LoJMelz7iKk3brR4j5J6Ju672I2/OdaSoM9mfeTbU3l9+Y8ht+PgdyfpUWT7cW5PrOEQMotqLxcjxP55L70vuwZNRHaloMIWBYh0HkWfZfeVIyhcPoSBHz+bM2S9OapipDLs+dmFaNLENnE77WH6H9ZSve4K73PqxCQL0nv+Y70ubsofWZ6v7PR+HeO8biJKraq5AM3NIil+cEP5iHNU9Ry4DsdbGwMvvtd+LM/g3XrYHCQ6foAHo9l2aEXIg/gYWIhtPC1w5I1aIHZjRaXPqIXmcWBuwjQ6U2vsaeYDY05TOW/vYgblCxM7fIJaypWZZ6zTFoTCuFtQeE6A+koAsQWtHA7JOfwmAGoPp1jMnP+bmLh9+vqqCzYd4jVuovePJA5zyhH9kc0WNtjHSNq2LI9FX18LcGI9cakPr2PIaKO0vdVyFzHIe2GzGdIer+r0uu2oe9tBOVCHRI2AM5D32UTwar0sfb2DOrZ9+33NYkABQTwvveedK9tCGD2Ep65NzgWQG9Bm5rh9LiJMM2IqPUI4S2OI8+sNd3HJWiM70bRjqvSa+9Gm75q4LXAW1Ao+yUTnaqrpad5yy3w278NF12UhzRPYcuB73Sy4WG44w4pwaxZIyWYcvmogtgvZEcrkVhPpWYo6fejaNE4QITK7KFUEwtcJ9phb0LA5IXXeTMv3CYXDFJZs2awyQZ4DQ72CBegurEfoAW6jgDZJrTIDRCeZkN6bYGgvM8kSDv1KDdmb81Nes8nvNs2tJAW0mezGC3cpuiDNgZZ+TYTKdqozINtITYLEMDtjhn2egxwVek9O+zYSORvzXQ0C/ZCQmzguXR+A/lN6bXrCU/3yXTOc1B4u4y8NXuG9nqd9/P9uATD77VAZUhyJuFBt6bvaWd63oDqHK8B1JqYV6bj9qTP3EQobxQcdhwgQuSrMo/VZD6fhQQz2de9FHVDsCrLi7ZC2mrU1cF558HP/RzcfnveJug0sZzVeTpZXZ1UHW6+Ge67T41wH3lEADg2Fkrux2lNwFupzAmaIGKWaPbYJYRM2kZCJ3Rb+m3vyR7AXDTADqTHHM4zo3MmlZqgJqDUZI63B2Kvayzd4yhasJ37KxHeko/36y2r1ky0fhpN79MLrsHKTX3JvA+TamYTHqXlq9zhop7omuE81zK0QahBIb++9LxDe81o4b0Q+E3UiPRu5FWZOWnPyv3k+hAorQS+mbl3E00GEFCYoWgvuz09vzd9L8Ppe9mDwKeaoPe7hrOR6F5uTy5bcWrgs2fsz7o5nd8dQp5N9z2TiBb0pWOdH3QhucPGIKA0eDkSUJ/uoSVdc2d6blH6ey3hPZfTNT1umpAneBvHma+bzgoFsTObm8XK/sAH4DWvUVPY3E4by4HvdLS2Nrj1VgHgE0/AZz4D3/wm7Nmj51+kB5gtkYBKIPRu3iqjvWhhnokW9hlosf5xet5ahvuJPN8QoQ7TSHgEBbQommTSQXTJ7kEeWLYDvN/VBFrAzdB0uNReSC/yYLK1gQZBkzCsOWoq/H4id+aFuJzO4w7zj6fnmhBBo5ZYvJsQ4BiEDQT+TOzlWk/SbNd25JEsQmCTVaMx6Pnz2Zz+tqc5kPleHCacRN6kCR/2ZkeIfJzVVrYSWqwT6Z6b0HfVgEDFYVuHpn3fBjwDkEOqdcTGpD8d7xY/EBsN5+J8PouhO0y7NZ2jC20MehBRxo1eFyDd2iaiXKcPhWjnpc/z+vTaF52rm2qFgvJ3tbXy6N78ZnjnO2HZsjyceZpaHuo8U8xEmE99SmDoMOhJsCwQHkJsy3bkAW5HC/4IWqRuRGy854B/RQuY1TbaiJKIcxB47CE8ls50LeesutFi5nxZXfrturIS8qj8GnsKJn54V+fw4nDmfF6w7VE2EA13TYbxOa0cY8BsQIDvnFgdwVZ0k16TMMoI9A5lzpsV+l6AFueG9JntQKDmsFwjQQAyYJrx6Lo2K+nYQzSjE0Laa4wo4B9J1ziQ+XxmpO/FdXkXpNc59NlE1HC6L6QL2PuJ+rhsbtNhbzNh3esxu2FpTp/5cPrcLHVWQvq0A4gA4yay9vzKiJ06AfwiCl2ew4soJj8eKxQEbF1dcOWVArvcuzsjLPf4zhTr6oJ3vAPe/nYB3z/+o4Bw61YpwpxgHhCOLJo3aLhm0EXpM6nc9deinbjzZj1EF4d6gvxgLdFaQkXmMWLBtzJMMyGHNokW51q0ALrI3XlAewZuK2P2pUE120PO568hSCEGeRNIDCqlzDl8bjNPfW3nzkaJ/m/WPvVrO9Pjpt9vILwvlxBkPS1fyyxZe1QmC2WVci5F3pofN8CPIW/IGqjeaAxn3ts5hLLJhekzfJYI55q8ZGauv1N7bRAbDXdjmIWAq5rIGdsjd13mchR270MbnrtR/nYg8zm5RrQ53edlaONwosLlRzV7dytWwLvfDTfcAIsW5d7dGWQ58J1pVl0t/b/LLpMXeN998gIffVSNcV0ScYJAODUs+vr020LaXoAmiPzRDLSAPYsWMHdbKKJuE8671SD1EHtIWTKDC8t3Ep7TesLbc04HKvNMFu+uJaTRxoguFFDZkNRLmyW2mjL34l6EDlcuSOfpJZiwV6FFfn96H3vSuTqQiEA3Idvl+rTm9LpDwP8mvEuHBR0GbkRgDxJ/NlO2iADP4Dee3qtl4Qzstel+3dVgGepS/kMCjDcS3qm1VA06Fin3+/cIshfrzcNK4PdQyHUDyrs5nGpC1FIEiOek9/ae9H3sITQ+9yCgOw9tkjrT8UVEOlrGSTITVaqrYeZM1dzdfnvOzDyDLQe+M9m6usQ2u/VWFcV/8Ytw112wfbtCoaOjJ1wWYZsKhBDklWYEeiuJfJ5bLM1CYb11KBzqBrhW6HfxNUS9lRX1nXtyGHGqULOJHi7e9jmrMsc5H+ji7jJaVO1N2ZMy8EwiCv0AWpCbiE4EO4nQZDUCRBNlBtJ7a0nn34LApCv9uDi/nQArS3XZUzOoXIIKsH+Qrr2fqPerJ1iYDtdaKKA7c95dmeuYbFJO77sl81n1IMDam553ONJg2kTUG/oeJ9L3swAVrZeJsPP8dM/2Li2UcBEC3/p03Kz0cx0voxUy/mGpJGWViy+G979f5Qh5KPOMtxz4zgarrtbu9eMfhw9/WEzQ73wHHngAnnlGIHgcAtnHa2aLvjXzmD0zlxGAFmX34fPj48hL2IYW9NnpmN0IcEyUsHdjj8cMw0Lmp57IhTn3lA2rDhOKHnsz5+gnwoQGP9eWPYMAvYXI/bkOzR0BTOl3eLUpPe8auAmieLwWkVtKBFiYzOISjUYEFvPR5sENfVvS+3O4cFfmfCbq+DMpEgQU5ykb02f7SHr/u5CHbmZoVoB6kmCh9hJ1eFsJ4HUT4iGUB/b7m5XucRUi5izhJOfijtfcBsi5u3nzRBB797tz7+4ss5zccrba2Jj0QDdvVnH8nXfKKzx0SKSYV2hY9AFfQSSZVrToFogWNm1E2PTJdJxlreqImjLnpvahBdXelsOB9gStyOKQoOvxniJCgiNocSdd31T4lYiAMoLAsoiA8GB6nRmpuwjSR086xwxC/LmMwrbOSV6V/v6/6T7cmd7elcsw5qf3P5zeu1s4WTDA9YRmRlq9ZZzQN20gai6b04+JLiMEE7UFeWXl9BnNJtRa+tNz/YjN65q9C9M9Zstg5qAc3Kta3Wagq6mBjg64/PKcqHKWWw58ucnGxmDLFnWI+PKXBYJ9fQqHup3KKzhULJv2IxQedA++XYQUWgshbjxBaGmOpmMMkKb2NyIActG5PSErsBj4nCNz7gyijq6XIOhk2Z916dxWWrHqzdMIlPza5nSfBaLW7rz0/GoErJZwm0kU8w8hQDE41RKtmZqJEKfr7EwCcljSr2lM9+dCeefp/B6qETlmOWp67MddMzdCKPyYWbsc+F1eQjH4y2H27urrYdYshTLf/GZJiOVElbPecuDL7UgbG4NNm+Cee+Bb31LXiIMHwxMsFF4SS/SlWBYQtyEAM8XejMuDVAJkA5VF8CauGCDNnMzKhmW1L53nKxChyqxMmBmRlmKzkkgLAjkXuTcSUmAOpxrkqlHOzp9qHVEkv5nIy9USdYtmnWYf8/0a9LJto/y3i/tdN9iCQo81aFPQi8BvfboHbwRMaHGO1I2BTxrJ5KWaPbu6ughj3nKL2JkzZ+Zgl9thy4Evt2Pb2Jia5K5eDV/4gtihBw+qa/z4eADhKTaMpgPIXrRodxLhwANEMfcO5FlVEUXrZjGaLGKiy9QO9AbU7HbAYGoSTTMKG1pM2XlLsyQdki0R6imuj/M1LHtmco9znIX0dyMKuVrzMqslCgrRno8AuB6BresYq5DXugyB2lxOYSsU4qeqSqIOl14qItcNN8DSpTnQ5XZUy4Evtxdn3d0ix9xzDzz2mLpG9PQoJHqSCuZPFZsgwqzPIOKKa+As6daFwGIT8szcw+4pooehyy0WIg3K81GY8pF0fuc0IVoeuTNEbbrOIpQvG0Gg1YHCozNQ2HEhai1Vxwn0jDtVbeqGykBXKilf196utj/vepcKzHOvLrfjtBz4cjsxMzmmu1ve4AMPCBC3bQvt0MnJ6E+WD7Pcjtecn3M4vapKQNfWBsuXK193441w1VU5OSW3E7Ic+HI7eeaw6Lp1Yonee6+AsL+/Mj9oe5XyhLmdYja1rs6PNTbCggUCuOuvlyh0V5c8vdyzy+0lWA58ub18NhUIH3xQEmr9/coPmiTjIZgD4dllxWL81Ner48Hs2XDNNSKlrFyZhy9ze1ksB77cXjkzED75pGoH779fHSX6+qKI/hRgjub2Ei3rwWXzdNkuB52d8t6WL4err1Zt3YIFOdDl9opYDny5vXpmINy2DTZuVNnEhg1Skzl4EIaGBH5eSA2Er2CBfW7HYVlwq6qqZFxWV4cnd8UVecgyt1PCcuDL7dSzo3mGhw5FGUW5HN3nX4g8cwqWW5zS5o3GdJ+ZiSeg3+Wyfrt+rrk5SCirVilsee65uSeX2yllOfDldurbVM9w40b9f+tWPT48HKFSt2CaCojThU2PtcCf6Xas925g84bBeTh7cAa51la44AJ1ArnwQoFd7snldhpYDny5nd6WLatYv15h0o0b9XvPHhgc1I/zhvYSDZJwpMc4NS/ln9OlTtH3a7A/msfrnJs9NgMc6PG6Ov3Mnq3O4wsXCuiWLcvzcbmd1pYDX2655ZZbbmeVFV/tG8gtt9xyyy23V9Jy4Mstt9xyy+2sshz4csstt9xyO6ssB77ccsstt9zOKsuBL7fccsstt7PK/j+Cv5Tlu5ArZQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEMcCle2ntbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "af702b1e-6540-445b-9134-e6d0b90cf93f"
      },
      "source": [
        "Adj = nx.adjacency_matrix(G)\n",
        "Adj.todense()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 0, 0, ..., 0, 0, 0],\n",
              "        [1, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmP3oiQTOJso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2f311eef-cfbd-4256-9412-c1dbc6a54403"
      },
      "source": [
        "Adj"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<837x837 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 34828 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nX6t_ML2dJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fe834a0-5a20-462f-abe1-8ce813e5b875"
      },
      "source": [
        "type(node_features)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUCPEuDYdh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.sparse as sp\n",
        "def load_data(adj,node_features,node_labels):\n",
        "\n",
        "    features = sp.csr_matrix(node_features, dtype=np.float32)  # 储存为csr型稀疏矩阵\n",
        "    # build symmetric adjacency matrix   论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "    # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))   # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
        "    # 对应公式A~=A+IN\n",
        "    # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
        "    idx_train = range(500)\n",
        "    idx_val = range(500, 660)\n",
        "    idx_test = range(660, 836)  \n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))  # tensor为pytorch常用的数据结构\n",
        "    labels = torch.LongTensor(np.where(node_labels)[0])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)   # 邻接矩阵转为tensor处理\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test  \n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))  # 对每一行求和\n",
        "    r_inv = np.power(rowsum, -1).flatten()  # 求倒数\n",
        "    r_inv[np.isinf(r_inv)] = 0.  # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
        "    r_mat_inv = sp.diags(r_inv)  # 构建对角元素为r_inv的对角矩阵\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
        "    return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels) # 使用type_as(tesnor)将张量转换为给定类型的张量。\n",
        "    correct = preds.eq(labels).double()  # 记录等于preds的label eq:equal\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):    # 把一个sparse matrix转为torch稀疏张量\n",
        "    \"\"\"\n",
        "    numpy中的ndarray转化成pytorch中的tensor : torch.from_numpy()\n",
        "    pytorch中的tensor转化成numpy中的ndarray : numpy()\n",
        "    \"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    # 不懂的可以去看看COO性稀疏矩阵的结构\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEF3l9vGxWhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "\n",
        "    # 初始化层：输入feature，输出feature，权重，偏移\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # FloatTensor建立tensor\n",
        "        # 常见用法self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))：\n",
        "        # 首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter\n",
        "        # 绑定到这个module里面，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。\n",
        "        # 使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "            # Parameters与register_parameter都会向parameters写入参数，但是后者可以支持字符串命名\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # 初始化权重\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        # size()函数主要是用来统计矩阵元素个数，或矩阵某一维上的元素个数的函数  size（1）为行\n",
        "        self.weight.data.uniform_(-stdv, stdv)  # uniform() 方法将随机生成下一个实数，它在 [x, y] 范围内\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    '''\n",
        "    前馈运算 即计算A~ X W(0)\n",
        "    input X与权重W相乘，然后adj矩阵与他们的积稀疏乘\n",
        "    直接输入与权重之间进行torch.mm操作，得到support，即XW\n",
        "    support与adj进行torch.spmm操作，得到output，即AXW选择是否加bias\n",
        "    '''\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # torch.mm(a, b)是矩阵a和b矩阵相乘，torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NWtCnVPB4nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):  # 底层节点的参数，feature的个数；隐层节点个数；最终的分类数\n",
        "        super(GCN, self).__init__()  #  super()._init_()在利用父类里的对象构造函数\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)   # gc1输入尺寸nfeat，输出尺寸nhid\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)  # gc2输入尺寸nhid，输出尺寸ncalss\n",
        "        self.dropout = dropout\n",
        "\n",
        "    # 输入分别是特征和邻接矩阵。最后输出为输出层做log_softmax变换的结果\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))    # adj即公式Z=softmax(A~Relu(A~XW(0))W(1))中的A~\n",
        "        x = F.dropout(x, self.dropout, training=self.training)  # x要dropout\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCBnN6dR8mEV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4fff228b-d2f8-4183-99a6-450b2288b2b2"
      },
      "source": [
        "features.shape[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9JVSKDhxw00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8278bc3f-14d7-4a10-c12c-f76b3f4f2963"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse  # argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "# Training settings\n",
        "learning_rate = 0.01\n",
        "weight_decay = 5e-4\n",
        "epoch_num = 200\n",
        "dropout = 0.5\n",
        "\n",
        "#in_size = node_features  #设置输入层的维数\n",
        "hi_size = 16 #设置隐藏层的维数\n",
        "#out_size = node_label #设置输入层的维数\n",
        "\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(Adj,features,labels)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=hi_size,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# 数据写入cuda，便于后续加速\n",
        "\n",
        "# if args.cuda:\n",
        "#     model.cuda()   # . cuda()会分配到显存里（如果gpu可用）\n",
        "#     features = features.cuda()\n",
        "#     adj = adj.cuda()\n",
        "#     labels = labels.cuda()\n",
        "#     idx_train = idx_train.cuda()\n",
        "#     idx_val = idx_val.cuda()\n",
        "#     idx_test = idx_test.cuda()\n",
        "#global node_vec\n",
        "\n",
        "def train(epoch_num):\n",
        "    t = time.time()  # 返回当前时间\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # optimizer.zero_grad()意思是把梯度置零，也就是把loss关于weight的导数变成0.\n",
        "    # pytorch中每一轮batch需要设置optimizer.zero_gra\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    # 由于在算output时已经使用了log_softmax，这里使用的损失函数就是NLLloss，如果前面没有加log运算，\n",
        "    # 这里就要使用CrossEntropyLoss了\n",
        "    # 损失函数NLLLoss() 的输入是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率，\n",
        "    # 适合最后一层是log_softmax()的网络. 损失函数 CrossEntropyLoss() 与 NLLLoss() 类似,\n",
        "    # 唯一的不同是它为我们去做 softmax.可以理解为：CrossEntropyLoss()=log_softmax() + NLLLoss()\n",
        "    # https://blog.csdn.net/hao5335156/article/details/80607732\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])  #计算准确率\n",
        "    loss_train.backward()  # 反向求导  Back Propagation\n",
        "    optimizer.step()  # 更新所有的参数  Gradient Descent\n",
        "\n",
        "    #if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "    model.eval()  # eval() 函数用来执行一个字符串表达式，并返回表达式的值\n",
        "    output = model(features, adj)\n",
        "    node_vec = output\n",
        "    #print(node_vec.shape)\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])    # 验证集的损失函数\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n",
        "# Train model  逐个epoch进行train，最后test\n",
        "t_total = time.time()\n",
        "for epoch in range(epoch_num):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "test()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 6.7301 acc_train: 0.0000 loss_val: 6.7629 acc_val: 0.0000 time: 0.3127s\n",
            "Epoch: 0002 loss_train: 6.7024 acc_train: 0.0020 loss_val: 6.8108 acc_val: 0.0000 time: 0.0373s\n",
            "Epoch: 0003 loss_train: 6.6632 acc_train: 0.0060 loss_val: 6.8810 acc_val: 0.0000 time: 0.0382s\n",
            "Epoch: 0004 loss_train: 6.6126 acc_train: 0.0020 loss_val: 6.9796 acc_val: 0.0000 time: 0.0377s\n",
            "Epoch: 0005 loss_train: 6.5518 acc_train: 0.0020 loss_val: 7.1115 acc_val: 0.0000 time: 0.0392s\n",
            "Epoch: 0006 loss_train: 6.4843 acc_train: 0.0020 loss_val: 7.2831 acc_val: 0.0000 time: 0.0464s\n",
            "Epoch: 0007 loss_train: 6.4192 acc_train: 0.0020 loss_val: 7.4984 acc_val: 0.0000 time: 0.0407s\n",
            "Epoch: 0008 loss_train: 6.3428 acc_train: 0.0020 loss_val: 7.7553 acc_val: 0.0000 time: 0.0401s\n",
            "Epoch: 0009 loss_train: 6.2855 acc_train: 0.0020 loss_val: 8.0468 acc_val: 0.0000 time: 0.0374s\n",
            "Epoch: 0010 loss_train: 6.2461 acc_train: 0.0040 loss_val: 8.3646 acc_val: 0.0000 time: 0.0395s\n",
            "Epoch: 0011 loss_train: 6.2069 acc_train: 0.0020 loss_val: 8.6948 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0012 loss_train: 6.1826 acc_train: 0.0040 loss_val: 9.0264 acc_val: 0.0000 time: 0.0424s\n",
            "Epoch: 0013 loss_train: 6.1599 acc_train: 0.0020 loss_val: 9.3499 acc_val: 0.0000 time: 0.0387s\n",
            "Epoch: 0014 loss_train: 6.1498 acc_train: 0.0040 loss_val: 9.6597 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0015 loss_train: 6.1358 acc_train: 0.0020 loss_val: 9.9500 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0016 loss_train: 6.1226 acc_train: 0.0020 loss_val: 10.2179 acc_val: 0.0000 time: 0.0391s\n",
            "Epoch: 0017 loss_train: 6.1154 acc_train: 0.0020 loss_val: 10.4612 acc_val: 0.0000 time: 0.0384s\n",
            "Epoch: 0018 loss_train: 6.0913 acc_train: 0.0040 loss_val: 10.6800 acc_val: 0.0000 time: 0.0413s\n",
            "Epoch: 0019 loss_train: 6.0620 acc_train: 0.0020 loss_val: 10.8758 acc_val: 0.0000 time: 0.0425s\n",
            "Epoch: 0020 loss_train: 6.0668 acc_train: 0.0040 loss_val: 11.0450 acc_val: 0.0000 time: 0.0416s\n",
            "Epoch: 0021 loss_train: 6.0504 acc_train: 0.0040 loss_val: 11.1876 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0022 loss_train: 6.0360 acc_train: 0.0040 loss_val: 11.3072 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0023 loss_train: 6.0040 acc_train: 0.0040 loss_val: 11.4059 acc_val: 0.0000 time: 0.0419s\n",
            "Epoch: 0024 loss_train: 6.0065 acc_train: 0.0080 loss_val: 11.4850 acc_val: 0.0000 time: 0.0394s\n",
            "Epoch: 0025 loss_train: 5.9673 acc_train: 0.0060 loss_val: 11.5472 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0026 loss_train: 5.9632 acc_train: 0.0100 loss_val: 11.5936 acc_val: 0.0000 time: 0.0423s\n",
            "Epoch: 0027 loss_train: 5.9497 acc_train: 0.0100 loss_val: 11.6281 acc_val: 0.0000 time: 0.0449s\n",
            "Epoch: 0028 loss_train: 5.9518 acc_train: 0.0100 loss_val: 11.6467 acc_val: 0.0000 time: 0.0501s\n",
            "Epoch: 0029 loss_train: 5.9190 acc_train: 0.0100 loss_val: 11.6524 acc_val: 0.0000 time: 0.0466s\n",
            "Epoch: 0030 loss_train: 5.9009 acc_train: 0.0180 loss_val: 11.6493 acc_val: 0.0000 time: 0.0515s\n",
            "Epoch: 0031 loss_train: 5.8440 acc_train: 0.0140 loss_val: 11.6378 acc_val: 0.0000 time: 0.0453s\n",
            "Epoch: 0032 loss_train: 5.8486 acc_train: 0.0200 loss_val: 11.6193 acc_val: 0.0000 time: 0.0511s\n",
            "Epoch: 0033 loss_train: 5.8508 acc_train: 0.0120 loss_val: 11.5923 acc_val: 0.0000 time: 0.0576s\n",
            "Epoch: 0034 loss_train: 5.8284 acc_train: 0.0180 loss_val: 11.5618 acc_val: 0.0000 time: 0.0444s\n",
            "Epoch: 0035 loss_train: 5.8091 acc_train: 0.0220 loss_val: 11.5285 acc_val: 0.0000 time: 0.0453s\n",
            "Epoch: 0036 loss_train: 5.7754 acc_train: 0.0260 loss_val: 11.4922 acc_val: 0.0000 time: 0.0439s\n",
            "Epoch: 0037 loss_train: 5.7315 acc_train: 0.0280 loss_val: 11.4540 acc_val: 0.0000 time: 0.0442s\n",
            "Epoch: 0038 loss_train: 5.6669 acc_train: 0.0240 loss_val: 11.4153 acc_val: 0.0000 time: 0.0471s\n",
            "Epoch: 0039 loss_train: 5.6650 acc_train: 0.0180 loss_val: 11.3776 acc_val: 0.0000 time: 0.0459s\n",
            "Epoch: 0040 loss_train: 5.6672 acc_train: 0.0260 loss_val: 11.3384 acc_val: 0.0000 time: 0.0442s\n",
            "Epoch: 0041 loss_train: 5.6661 acc_train: 0.0240 loss_val: 11.2993 acc_val: 0.0000 time: 0.0488s\n",
            "Epoch: 0042 loss_train: 5.5841 acc_train: 0.0180 loss_val: 11.2592 acc_val: 0.0000 time: 0.0458s\n",
            "Epoch: 0043 loss_train: 5.5906 acc_train: 0.0420 loss_val: 11.2225 acc_val: 0.0000 time: 0.0494s\n",
            "Epoch: 0044 loss_train: 5.5975 acc_train: 0.0340 loss_val: 11.1896 acc_val: 0.0000 time: 0.0440s\n",
            "Epoch: 0045 loss_train: 5.5143 acc_train: 0.0380 loss_val: 11.1603 acc_val: 0.0000 time: 0.0476s\n",
            "Epoch: 0046 loss_train: 5.5714 acc_train: 0.0300 loss_val: 11.1325 acc_val: 0.0000 time: 0.0504s\n",
            "Epoch: 0047 loss_train: 5.4638 acc_train: 0.0400 loss_val: 11.1125 acc_val: 0.0000 time: 0.0427s\n",
            "Epoch: 0048 loss_train: 5.4976 acc_train: 0.0320 loss_val: 11.0949 acc_val: 0.0000 time: 0.0401s\n",
            "Epoch: 0049 loss_train: 5.4715 acc_train: 0.0520 loss_val: 11.0777 acc_val: 0.0000 time: 0.0369s\n",
            "Epoch: 0050 loss_train: 5.4716 acc_train: 0.0540 loss_val: 11.0603 acc_val: 0.0000 time: 0.0384s\n",
            "Epoch: 0051 loss_train: 5.4706 acc_train: 0.0520 loss_val: 11.0373 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0052 loss_train: 5.3874 acc_train: 0.0540 loss_val: 11.0192 acc_val: 0.0000 time: 0.0427s\n",
            "Epoch: 0053 loss_train: 5.3890 acc_train: 0.0520 loss_val: 11.0076 acc_val: 0.0000 time: 0.0389s\n",
            "Epoch: 0054 loss_train: 5.3770 acc_train: 0.0480 loss_val: 11.0000 acc_val: 0.0000 time: 0.0397s\n",
            "Epoch: 0055 loss_train: 5.3421 acc_train: 0.0640 loss_val: 10.9953 acc_val: 0.0000 time: 0.0387s\n",
            "Epoch: 0056 loss_train: 5.3279 acc_train: 0.0560 loss_val: 11.0066 acc_val: 0.0000 time: 0.0389s\n",
            "Epoch: 0057 loss_train: 5.3247 acc_train: 0.0580 loss_val: 11.0286 acc_val: 0.0000 time: 0.0398s\n",
            "Epoch: 0058 loss_train: 5.3082 acc_train: 0.0540 loss_val: 11.0592 acc_val: 0.0000 time: 0.0402s\n",
            "Epoch: 0059 loss_train: 5.2859 acc_train: 0.0780 loss_val: 11.0799 acc_val: 0.0000 time: 0.0387s\n",
            "Epoch: 0060 loss_train: 5.2499 acc_train: 0.0640 loss_val: 11.0972 acc_val: 0.0000 time: 0.0407s\n",
            "Epoch: 0061 loss_train: 5.2398 acc_train: 0.0580 loss_val: 11.1186 acc_val: 0.0000 time: 0.0373s\n",
            "Epoch: 0062 loss_train: 5.2358 acc_train: 0.0740 loss_val: 11.1287 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0063 loss_train: 5.1216 acc_train: 0.0820 loss_val: 11.1492 acc_val: 0.0000 time: 0.0376s\n",
            "Epoch: 0064 loss_train: 5.1969 acc_train: 0.0760 loss_val: 11.1718 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0065 loss_train: 5.1464 acc_train: 0.0880 loss_val: 11.2047 acc_val: 0.0000 time: 0.0390s\n",
            "Epoch: 0066 loss_train: 5.0638 acc_train: 0.0820 loss_val: 11.2515 acc_val: 0.0000 time: 0.0404s\n",
            "Epoch: 0067 loss_train: 5.1246 acc_train: 0.0780 loss_val: 11.3018 acc_val: 0.0000 time: 0.0377s\n",
            "Epoch: 0068 loss_train: 5.0763 acc_train: 0.0920 loss_val: 11.3579 acc_val: 0.0000 time: 0.0377s\n",
            "Epoch: 0069 loss_train: 4.9887 acc_train: 0.1040 loss_val: 11.4301 acc_val: 0.0000 time: 0.0391s\n",
            "Epoch: 0070 loss_train: 4.9773 acc_train: 0.0820 loss_val: 11.5009 acc_val: 0.0000 time: 0.0398s\n",
            "Epoch: 0071 loss_train: 4.8668 acc_train: 0.0960 loss_val: 11.5659 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0072 loss_train: 5.0057 acc_train: 0.0900 loss_val: 11.6239 acc_val: 0.0000 time: 0.0408s\n",
            "Epoch: 0073 loss_train: 4.9793 acc_train: 0.0920 loss_val: 11.6570 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0074 loss_train: 4.9332 acc_train: 0.1040 loss_val: 11.6750 acc_val: 0.0000 time: 0.0382s\n",
            "Epoch: 0075 loss_train: 4.8941 acc_train: 0.0980 loss_val: 11.6920 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0076 loss_train: 4.8255 acc_train: 0.1040 loss_val: 11.7080 acc_val: 0.0000 time: 0.0377s\n",
            "Epoch: 0077 loss_train: 4.9486 acc_train: 0.1040 loss_val: 11.7291 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0078 loss_train: 4.8632 acc_train: 0.0880 loss_val: 11.7506 acc_val: 0.0000 time: 0.0451s\n",
            "Epoch: 0079 loss_train: 4.8525 acc_train: 0.1000 loss_val: 11.7720 acc_val: 0.0000 time: 0.0374s\n",
            "Epoch: 0080 loss_train: 4.8106 acc_train: 0.0880 loss_val: 11.7961 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0081 loss_train: 4.8351 acc_train: 0.1100 loss_val: 11.8238 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0082 loss_train: 4.8386 acc_train: 0.0860 loss_val: 11.8494 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0083 loss_train: 4.7021 acc_train: 0.1000 loss_val: 11.8791 acc_val: 0.0000 time: 0.0387s\n",
            "Epoch: 0084 loss_train: 4.6995 acc_train: 0.0980 loss_val: 11.9029 acc_val: 0.0000 time: 0.0430s\n",
            "Epoch: 0085 loss_train: 4.7009 acc_train: 0.1120 loss_val: 11.9315 acc_val: 0.0000 time: 0.0389s\n",
            "Epoch: 0086 loss_train: 4.8136 acc_train: 0.0820 loss_val: 11.9700 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0087 loss_train: 4.6610 acc_train: 0.1140 loss_val: 11.9933 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0088 loss_train: 4.6616 acc_train: 0.1000 loss_val: 12.0060 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0089 loss_train: 4.7371 acc_train: 0.1040 loss_val: 12.0320 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0090 loss_train: 4.4668 acc_train: 0.1700 loss_val: 12.0701 acc_val: 0.0000 time: 0.0411s\n",
            "Epoch: 0091 loss_train: 4.6763 acc_train: 0.1240 loss_val: 12.1011 acc_val: 0.0000 time: 0.0395s\n",
            "Epoch: 0092 loss_train: 4.5928 acc_train: 0.1060 loss_val: 12.1343 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0093 loss_train: 4.5901 acc_train: 0.1240 loss_val: 12.1585 acc_val: 0.0000 time: 0.0382s\n",
            "Epoch: 0094 loss_train: 4.6788 acc_train: 0.0900 loss_val: 12.1482 acc_val: 0.0000 time: 0.0377s\n",
            "Epoch: 0095 loss_train: 4.5874 acc_train: 0.1240 loss_val: 12.1393 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0096 loss_train: 4.5531 acc_train: 0.1180 loss_val: 12.1301 acc_val: 0.0000 time: 0.0402s\n",
            "Epoch: 0097 loss_train: 4.7131 acc_train: 0.1100 loss_val: 12.1260 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0098 loss_train: 4.5137 acc_train: 0.1340 loss_val: 12.1524 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0099 loss_train: 4.4628 acc_train: 0.1300 loss_val: 12.1781 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0100 loss_train: 4.6774 acc_train: 0.1040 loss_val: 12.1893 acc_val: 0.0000 time: 0.0373s\n",
            "Epoch: 0101 loss_train: 4.4172 acc_train: 0.1440 loss_val: 12.2130 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0102 loss_train: 4.4943 acc_train: 0.1300 loss_val: 12.2458 acc_val: 0.0000 time: 0.0404s\n",
            "Epoch: 0103 loss_train: 4.3988 acc_train: 0.1380 loss_val: 12.2843 acc_val: 0.0000 time: 0.0382s\n",
            "Epoch: 0104 loss_train: 4.3852 acc_train: 0.1420 loss_val: 12.3224 acc_val: 0.0000 time: 0.0423s\n",
            "Epoch: 0105 loss_train: 4.3587 acc_train: 0.1300 loss_val: 12.3683 acc_val: 0.0000 time: 0.0387s\n",
            "Epoch: 0106 loss_train: 4.4602 acc_train: 0.1080 loss_val: 12.4042 acc_val: 0.0000 time: 0.0386s\n",
            "Epoch: 0107 loss_train: 4.5598 acc_train: 0.1280 loss_val: 12.4104 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0108 loss_train: 4.4116 acc_train: 0.1620 loss_val: 12.4250 acc_val: 0.0000 time: 0.0445s\n",
            "Epoch: 0109 loss_train: 4.3900 acc_train: 0.1480 loss_val: 12.4254 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0110 loss_train: 4.3096 acc_train: 0.1560 loss_val: 12.4170 acc_val: 0.0000 time: 0.0402s\n",
            "Epoch: 0111 loss_train: 4.5250 acc_train: 0.1160 loss_val: 12.4283 acc_val: 0.0000 time: 0.0390s\n",
            "Epoch: 0112 loss_train: 4.3085 acc_train: 0.1400 loss_val: 12.4390 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0113 loss_train: 4.3912 acc_train: 0.1360 loss_val: 12.4616 acc_val: 0.0000 time: 0.0390s\n",
            "Epoch: 0114 loss_train: 4.4057 acc_train: 0.1100 loss_val: 12.4810 acc_val: 0.0000 time: 0.0402s\n",
            "Epoch: 0115 loss_train: 4.3907 acc_train: 0.1180 loss_val: 12.5274 acc_val: 0.0000 time: 0.0393s\n",
            "Epoch: 0116 loss_train: 4.3094 acc_train: 0.1200 loss_val: 12.5818 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0117 loss_train: 4.3352 acc_train: 0.1240 loss_val: 12.6311 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0118 loss_train: 4.3383 acc_train: 0.1420 loss_val: 12.6784 acc_val: 0.0000 time: 0.0375s\n",
            "Epoch: 0119 loss_train: 4.1876 acc_train: 0.1820 loss_val: 12.7060 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0120 loss_train: 4.2314 acc_train: 0.1580 loss_val: 12.7481 acc_val: 0.0000 time: 0.0406s\n",
            "Epoch: 0121 loss_train: 4.3016 acc_train: 0.1280 loss_val: 12.7428 acc_val: 0.0000 time: 0.0385s\n",
            "Epoch: 0122 loss_train: 4.3340 acc_train: 0.1140 loss_val: 12.7086 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0123 loss_train: 4.2179 acc_train: 0.1500 loss_val: 12.6699 acc_val: 0.0000 time: 0.0424s\n",
            "Epoch: 0124 loss_train: 4.1743 acc_train: 0.1780 loss_val: 12.6485 acc_val: 0.0000 time: 0.0386s\n",
            "Epoch: 0125 loss_train: 4.1838 acc_train: 0.1620 loss_val: 12.6221 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0126 loss_train: 4.2690 acc_train: 0.1640 loss_val: 12.6041 acc_val: 0.0000 time: 0.0416s\n",
            "Epoch: 0127 loss_train: 4.4022 acc_train: 0.1080 loss_val: 12.6104 acc_val: 0.0000 time: 0.0384s\n",
            "Epoch: 0128 loss_train: 4.3244 acc_train: 0.1340 loss_val: 12.6367 acc_val: 0.0000 time: 0.0392s\n",
            "Epoch: 0129 loss_train: 4.1246 acc_train: 0.1820 loss_val: 12.6759 acc_val: 0.0000 time: 0.0433s\n",
            "Epoch: 0130 loss_train: 4.2858 acc_train: 0.1380 loss_val: 12.7038 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0131 loss_train: 4.2065 acc_train: 0.1520 loss_val: 12.7463 acc_val: 0.0000 time: 0.0388s\n",
            "Epoch: 0132 loss_train: 4.1745 acc_train: 0.1480 loss_val: 12.7857 acc_val: 0.0000 time: 0.0423s\n",
            "Epoch: 0133 loss_train: 4.1118 acc_train: 0.1720 loss_val: 12.8303 acc_val: 0.0000 time: 0.0388s\n",
            "Epoch: 0134 loss_train: 4.2891 acc_train: 0.1300 loss_val: 12.8541 acc_val: 0.0000 time: 0.0402s\n",
            "Epoch: 0135 loss_train: 4.1964 acc_train: 0.1860 loss_val: 12.8770 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0136 loss_train: 4.2400 acc_train: 0.1540 loss_val: 12.8994 acc_val: 0.0000 time: 0.0394s\n",
            "Epoch: 0137 loss_train: 4.1720 acc_train: 0.1660 loss_val: 12.9344 acc_val: 0.0000 time: 0.0406s\n",
            "Epoch: 0138 loss_train: 4.0435 acc_train: 0.1880 loss_val: 12.9480 acc_val: 0.0000 time: 0.0451s\n",
            "Epoch: 0139 loss_train: 3.9989 acc_train: 0.1840 loss_val: 12.9313 acc_val: 0.0000 time: 0.0420s\n",
            "Epoch: 0140 loss_train: 4.1424 acc_train: 0.1700 loss_val: 12.9191 acc_val: 0.0000 time: 0.0392s\n",
            "Epoch: 0141 loss_train: 4.2488 acc_train: 0.1160 loss_val: 12.9028 acc_val: 0.0000 time: 0.0395s\n",
            "Epoch: 0142 loss_train: 4.1023 acc_train: 0.1740 loss_val: 12.8909 acc_val: 0.0000 time: 0.0392s\n",
            "Epoch: 0143 loss_train: 4.0320 acc_train: 0.1640 loss_val: 12.8773 acc_val: 0.0000 time: 0.0395s\n",
            "Epoch: 0144 loss_train: 4.0852 acc_train: 0.1680 loss_val: 12.8810 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0145 loss_train: 4.0326 acc_train: 0.1680 loss_val: 12.9029 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0146 loss_train: 4.0731 acc_train: 0.1800 loss_val: 12.9225 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0147 loss_train: 4.2706 acc_train: 0.1400 loss_val: 12.9338 acc_val: 0.0000 time: 0.0373s\n",
            "Epoch: 0148 loss_train: 4.0683 acc_train: 0.1480 loss_val: 12.9398 acc_val: 0.0000 time: 0.0395s\n",
            "Epoch: 0149 loss_train: 4.0474 acc_train: 0.1760 loss_val: 12.9522 acc_val: 0.0000 time: 0.0389s\n",
            "Epoch: 0150 loss_train: 3.9407 acc_train: 0.2100 loss_val: 12.9646 acc_val: 0.0000 time: 0.0384s\n",
            "Epoch: 0151 loss_train: 4.2552 acc_train: 0.1480 loss_val: 12.9710 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0152 loss_train: 4.1345 acc_train: 0.1400 loss_val: 12.9940 acc_val: 0.0000 time: 0.0390s\n",
            "Epoch: 0153 loss_train: 4.0533 acc_train: 0.2040 loss_val: 13.0233 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0154 loss_train: 3.9549 acc_train: 0.1840 loss_val: 13.0302 acc_val: 0.0000 time: 0.0460s\n",
            "Epoch: 0155 loss_train: 3.9508 acc_train: 0.2000 loss_val: 13.0608 acc_val: 0.0000 time: 0.0393s\n",
            "Epoch: 0156 loss_train: 4.2457 acc_train: 0.1340 loss_val: 13.1076 acc_val: 0.0000 time: 0.0396s\n",
            "Epoch: 0157 loss_train: 3.8744 acc_train: 0.1900 loss_val: 13.1246 acc_val: 0.0000 time: 0.0391s\n",
            "Epoch: 0158 loss_train: 4.0797 acc_train: 0.1620 loss_val: 13.1443 acc_val: 0.0000 time: 0.0386s\n",
            "Epoch: 0159 loss_train: 3.8354 acc_train: 0.1840 loss_val: 13.1408 acc_val: 0.0000 time: 0.0406s\n",
            "Epoch: 0160 loss_train: 4.0581 acc_train: 0.1680 loss_val: 13.1590 acc_val: 0.0000 time: 0.0396s\n",
            "Epoch: 0161 loss_train: 4.0280 acc_train: 0.1720 loss_val: 13.1801 acc_val: 0.0000 time: 0.0376s\n",
            "Epoch: 0162 loss_train: 4.0585 acc_train: 0.1760 loss_val: 13.1899 acc_val: 0.0000 time: 0.0374s\n",
            "Epoch: 0163 loss_train: 3.8054 acc_train: 0.2160 loss_val: 13.2120 acc_val: 0.0000 time: 0.0376s\n",
            "Epoch: 0164 loss_train: 3.9295 acc_train: 0.1920 loss_val: 13.2376 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0165 loss_train: 3.8780 acc_train: 0.2140 loss_val: 13.2450 acc_val: 0.0000 time: 0.0409s\n",
            "Epoch: 0166 loss_train: 3.9347 acc_train: 0.1860 loss_val: 13.2638 acc_val: 0.0000 time: 0.0384s\n",
            "Epoch: 0167 loss_train: 3.7983 acc_train: 0.2080 loss_val: 13.2835 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0168 loss_train: 3.8082 acc_train: 0.2200 loss_val: 13.3236 acc_val: 0.0000 time: 0.0384s\n",
            "Epoch: 0169 loss_train: 4.1427 acc_train: 0.1680 loss_val: 13.3358 acc_val: 0.0000 time: 0.0387s\n",
            "Epoch: 0170 loss_train: 3.9437 acc_train: 0.1580 loss_val: 13.3336 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0171 loss_train: 3.8531 acc_train: 0.2060 loss_val: 13.3259 acc_val: 0.0000 time: 0.0426s\n",
            "Epoch: 0172 loss_train: 3.7056 acc_train: 0.2320 loss_val: 13.3157 acc_val: 0.0000 time: 0.0379s\n",
            "Epoch: 0173 loss_train: 4.0254 acc_train: 0.1700 loss_val: 13.2887 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0174 loss_train: 3.8645 acc_train: 0.1880 loss_val: 13.2634 acc_val: 0.0000 time: 0.0389s\n",
            "Epoch: 0175 loss_train: 3.8707 acc_train: 0.1860 loss_val: 13.2622 acc_val: 0.0000 time: 0.0382s\n",
            "Epoch: 0176 loss_train: 3.8741 acc_train: 0.2020 loss_val: 13.2782 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0177 loss_train: 3.9001 acc_train: 0.1940 loss_val: 13.2949 acc_val: 0.0000 time: 0.0399s\n",
            "Epoch: 0178 loss_train: 3.8973 acc_train: 0.1940 loss_val: 13.3068 acc_val: 0.0000 time: 0.0372s\n",
            "Epoch: 0179 loss_train: 3.8954 acc_train: 0.2160 loss_val: 13.3166 acc_val: 0.0000 time: 0.0374s\n",
            "Epoch: 0180 loss_train: 3.7053 acc_train: 0.1980 loss_val: 13.3242 acc_val: 0.0000 time: 0.0425s\n",
            "Epoch: 0181 loss_train: 3.9198 acc_train: 0.1820 loss_val: 13.3104 acc_val: 0.0000 time: 0.0370s\n",
            "Epoch: 0182 loss_train: 3.9488 acc_train: 0.1600 loss_val: 13.3078 acc_val: 0.0000 time: 0.0410s\n",
            "Epoch: 0183 loss_train: 3.8715 acc_train: 0.1700 loss_val: 13.3130 acc_val: 0.0000 time: 0.0422s\n",
            "Epoch: 0184 loss_train: 3.9284 acc_train: 0.1820 loss_val: 13.3175 acc_val: 0.0000 time: 0.0381s\n",
            "Epoch: 0185 loss_train: 3.9434 acc_train: 0.1560 loss_val: 13.3417 acc_val: 0.0000 time: 0.0377s\n",
            "Epoch: 0186 loss_train: 3.9048 acc_train: 0.1840 loss_val: 13.3501 acc_val: 0.0000 time: 0.0383s\n",
            "Epoch: 0187 loss_train: 3.8706 acc_train: 0.1680 loss_val: 13.3446 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0188 loss_train: 3.7119 acc_train: 0.2100 loss_val: 13.3308 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0189 loss_train: 3.9262 acc_train: 0.1540 loss_val: 13.3238 acc_val: 0.0000 time: 0.0431s\n",
            "Epoch: 0190 loss_train: 3.7141 acc_train: 0.2340 loss_val: 13.3358 acc_val: 0.0000 time: 0.0374s\n",
            "Epoch: 0191 loss_train: 3.9362 acc_train: 0.2280 loss_val: 13.3522 acc_val: 0.0000 time: 0.0378s\n",
            "Epoch: 0192 loss_train: 3.6194 acc_train: 0.2180 loss_val: 13.3795 acc_val: 0.0000 time: 0.0372s\n",
            "Epoch: 0193 loss_train: 3.7213 acc_train: 0.2580 loss_val: 13.4129 acc_val: 0.0000 time: 0.0370s\n",
            "Epoch: 0194 loss_train: 3.8970 acc_train: 0.2100 loss_val: 13.4504 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0195 loss_train: 3.7422 acc_train: 0.2360 loss_val: 13.4927 acc_val: 0.0000 time: 0.0391s\n",
            "Epoch: 0196 loss_train: 3.7171 acc_train: 0.2060 loss_val: 13.5117 acc_val: 0.0000 time: 0.0380s\n",
            "Epoch: 0197 loss_train: 3.7708 acc_train: 0.1920 loss_val: 13.4957 acc_val: 0.0000 time: 0.0395s\n",
            "Epoch: 0198 loss_train: 3.6305 acc_train: 0.2420 loss_val: 13.4884 acc_val: 0.0000 time: 0.0386s\n",
            "Epoch: 0199 loss_train: 3.9327 acc_train: 0.1720 loss_val: 13.4793 acc_val: 0.0000 time: 0.0372s\n",
            "Epoch: 0200 loss_train: 3.7139 acc_train: 0.2180 loss_val: 13.4601 acc_val: 0.0000 time: 0.0376s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.4275s\n",
            "Test set results: loss= 16.3648 accuracy= 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDn4EgFT71zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model(features, adj)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F__0775CTul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "eb7c5a5a-a656-4020-9a54-cc7874027571"
      },
      "source": [
        "output"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -0.6518, -14.9067,  -5.9982,  ..., -14.4112, -14.4119, -14.4115],\n",
              "        [-14.3754,  -4.1419,  -9.9019,  ..., -12.2404, -12.2414, -12.2426],\n",
              "        [ -6.0218,  -9.4285,  -1.5601,  ..., -15.8995, -15.9006, -15.8991],\n",
              "        ...,\n",
              "        [ -6.0880, -22.9709, -17.1834,  ..., -17.3383, -17.3392, -17.3388],\n",
              "        [ -5.1564, -21.1178, -14.3515,  ..., -15.1317, -15.1327, -15.1326],\n",
              "        [ -6.6963, -22.1689, -17.6316,  ..., -15.8482, -15.8490, -15.8511]],\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uJG5MkkVNjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbb0ccac-bbda-4ebd-a80b-fe6283ffcd5d"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([837, 837])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohRRLkRXag_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output1 = output.detach().numpy()\n",
        "np.savetxt(\"/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/output.txt\",output1) "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-VtJDSKWuHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(output,'/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/output.csv')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXCK2S6CZZpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c4448079-6153-45ef-ca06-611c46e6c162"
      },
      "source": [
        "data_vec=torch.load('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/output.pt')\n",
        "data_vec"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -0.6518, -14.9067,  -5.9982,  ..., -14.4112, -14.4119, -14.4115],\n",
              "        [-14.3754,  -4.1419,  -9.9019,  ..., -12.2404, -12.2414, -12.2426],\n",
              "        [ -6.0218,  -9.4285,  -1.5601,  ..., -15.8995, -15.9006, -15.8991],\n",
              "        ...,\n",
              "        [ -6.0880, -22.9709, -17.1834,  ..., -17.3383, -17.3392, -17.3388],\n",
              "        [ -5.1564, -21.1178, -14.3515,  ..., -15.1317, -15.1327, -15.1326],\n",
              "        [ -6.6963, -22.1689, -17.6316,  ..., -15.8482, -15.8490, -15.8511]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWn0lhW7CgEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e79b6b4d-1b59-4b4c-982b-f269575f84d7"
      },
      "source": [
        "output[idx_test].shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([176, 837])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxcVVdTsCExR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = le-2\n",
        "weight_decay = 5e-4\n",
        "epoch_num = 200\n",
        "\n",
        "in_size = node_features  #设置输入层的维数\n",
        "hi_size = 16 #设置隐藏层的维数\n",
        "out_size = node_label #设置输入层的维数\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 判断是否可以使用GPU来训练\n",
        "# model = GCN().to(device)\n",
        "# data = dataset[0].to(device) \n",
        "model = GCN(in_size, hi_size, out_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # 使用Adam来优化参数\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epoch_num):\n",
        "  optimizer.zero_grad()\n",
        "  pred = model()\n",
        "  loss = F.nll_loss(pred[data.train_mask], data.y[data.train_mask])\n",
        "  print('epoch %s, loss %.3f'%(epoch, loss))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "_, pred = model(data.x, data.edge_index).max(dim=1)\n",
        "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / int(data.test_mask.sum())\n",
        "print('Accracy %.3f'%(acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}