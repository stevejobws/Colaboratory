{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_pridiction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XkvkckV6Lc5VVRFDhXiPCTrNYzg7vQ0W",
      "authorship_tag": "ABX9TyNxiYDG4WtwOhONQdopjicW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevejobws/Colaboratory/blob/master/gcn_pridiction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA0EeTTq6Gi-"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install torchvision\n",
        "!pip install torch_sparse -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_scatter -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "!pip install torch_geometric # 下载安装pytorch_geometric\n",
        "!pip install networkx # 画图\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric \n",
        "from torch_geometric.nn import GCNConv, ChebConv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5TCDF488n7"
      },
      "source": [
        "! python -c \"import torch_geometric; print(torch_geometric.__version__)\" # 检查是否安装成功"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57sAyKd7EgIc",
        "outputId": "3faba9a1-84bd-4ff2-95d8-d5cd39cb8b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "! uname -a  # 查看系统  \n",
        "! python --version  # 查看python版本 \n",
        "! python -c 'import torch; print(torch.version.cuda)' # 查看cuda的版本，检查是否和cuda的一致\n",
        "! nvcc --version # 查看nvcc版本 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux e4e606ebf6e4 4.19.112+ #1 SMP Thu Jul 23 08:00:38 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Python 3.6.9\n",
            "10.1\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki24HI_7E2mL",
        "outputId": "fa4e9f0d-0cea-4594-b605-ce32e4a136e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(torch.version.cuda) # torch的cuda版本"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZMKOaKvfRmz",
        "outputId": "80138fe3-64c8-4634-fa4d-ca872eac2431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "print(\"hello torch{}\".format(torch.__version__))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello torch1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnLtfimHc64d"
      },
      "source": [
        "# 1. 预处理数据集的格式，转化为GCN所需要的格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8QW2jP7JV2",
        "outputId": "14a2bb69-3803-40b2-9d59-0e97e79acb54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "import tensorflow as tf # Orange 1.14.0\n",
        "print(tf.__version__)\n",
        "import keras # 2.2.5\n",
        "print(keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "2.2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TkV_5eTmCFU",
        "outputId": "3bc283d6-a66e-4427-8762-8370f486953f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==1.14.0\n",
        "!pip install keras==2.2.5"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 51kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 38.4MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.35.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqo1Ho-djcrH"
      },
      "source": [
        "### 1.1 import file is  attribute of node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScqvErTlc5iO",
        "outputId": "56ab6802-1340-45a8-9373-9ef0311cbb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "node_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNodeAttribute.csv',header = None) \n",
        "num = node_features.shape[0] # Number of nodes\n",
        "node_features  \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.770633</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.582718</td>\n",
              "      <td>-0.062620</td>\n",
              "      <td>-0.238564</td>\n",
              "      <td>-0.715823</td>\n",
              "      <td>1.022981</td>\n",
              "      <td>0.181646</td>\n",
              "      <td>-0.260391</td>\n",
              "      <td>-0.542033</td>\n",
              "      <td>0.233139</td>\n",
              "      <td>-0.283432</td>\n",
              "      <td>0.117144</td>\n",
              "      <td>0.531449</td>\n",
              "      <td>-0.061221</td>\n",
              "      <td>-0.229529</td>\n",
              "      <td>-0.710597</td>\n",
              "      <td>0.288485</td>\n",
              "      <td>0.098411</td>\n",
              "      <td>0.189655</td>\n",
              "      <td>-0.511962</td>\n",
              "      <td>-0.263284</td>\n",
              "      <td>0.078329</td>\n",
              "      <td>0.198719</td>\n",
              "      <td>1.086848</td>\n",
              "      <td>-0.256804</td>\n",
              "      <td>0.026246</td>\n",
              "      <td>0.093899</td>\n",
              "      <td>-0.142497</td>\n",
              "      <td>0.018819</td>\n",
              "      <td>-0.234937</td>\n",
              "      <td>-0.153423</td>\n",
              "      <td>0.425772</td>\n",
              "      <td>0.673969</td>\n",
              "      <td>0.025525</td>\n",
              "      <td>-0.294841</td>\n",
              "      <td>-0.315873</td>\n",
              "      <td>0.552618</td>\n",
              "      <td>0.437355</td>\n",
              "      <td>-0.087442</td>\n",
              "      <td>0.536133</td>\n",
              "      <td>-0.366451</td>\n",
              "      <td>0.656990</td>\n",
              "      <td>0.802005</td>\n",
              "      <td>-0.016102</td>\n",
              "      <td>-0.669421</td>\n",
              "      <td>0.856563</td>\n",
              "      <td>-0.101192</td>\n",
              "      <td>-0.165583</td>\n",
              "      <td>-0.405513</td>\n",
              "      <td>-0.410948</td>\n",
              "      <td>-0.027975</td>\n",
              "      <td>-0.397765</td>\n",
              "      <td>-0.509387</td>\n",
              "      <td>0.280567</td>\n",
              "      <td>0.208776</td>\n",
              "      <td>-0.099218</td>\n",
              "      <td>-0.229354</td>\n",
              "      <td>0.232348</td>\n",
              "      <td>-1.035032</td>\n",
              "      <td>1.209129</td>\n",
              "      <td>0.368444</td>\n",
              "      <td>0.414352</td>\n",
              "      <td>-1.065435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.882159</td>\n",
              "      <td>1.241338</td>\n",
              "      <td>2.232549</td>\n",
              "      <td>-1.932836</td>\n",
              "      <td>-0.574318</td>\n",
              "      <td>0.168684</td>\n",
              "      <td>2.937954</td>\n",
              "      <td>1.398610</td>\n",
              "      <td>-0.260359</td>\n",
              "      <td>-1.253366</td>\n",
              "      <td>-0.997117</td>\n",
              "      <td>-1.355796</td>\n",
              "      <td>0.847303</td>\n",
              "      <td>2.447680</td>\n",
              "      <td>-0.573743</td>\n",
              "      <td>0.211870</td>\n",
              "      <td>-0.960837</td>\n",
              "      <td>1.536916</td>\n",
              "      <td>2.928266</td>\n",
              "      <td>-0.955619</td>\n",
              "      <td>-1.152885</td>\n",
              "      <td>0.548847</td>\n",
              "      <td>0.525967</td>\n",
              "      <td>-0.445511</td>\n",
              "      <td>2.999283</td>\n",
              "      <td>0.190644</td>\n",
              "      <td>-0.916012</td>\n",
              "      <td>0.877579</td>\n",
              "      <td>1.501392</td>\n",
              "      <td>-2.191907</td>\n",
              "      <td>-0.344929</td>\n",
              "      <td>-0.643345</td>\n",
              "      <td>1.808293</td>\n",
              "      <td>0.922850</td>\n",
              "      <td>0.823318</td>\n",
              "      <td>0.399970</td>\n",
              "      <td>0.415868</td>\n",
              "      <td>0.005898</td>\n",
              "      <td>-2.257816</td>\n",
              "      <td>-0.475472</td>\n",
              "      <td>0.610879</td>\n",
              "      <td>-1.345582</td>\n",
              "      <td>1.256365</td>\n",
              "      <td>0.962199</td>\n",
              "      <td>-1.093309</td>\n",
              "      <td>-1.647974</td>\n",
              "      <td>1.594285</td>\n",
              "      <td>0.584256</td>\n",
              "      <td>-1.293162</td>\n",
              "      <td>-1.844852</td>\n",
              "      <td>-0.580833</td>\n",
              "      <td>2.215265</td>\n",
              "      <td>-1.795820</td>\n",
              "      <td>-2.811482</td>\n",
              "      <td>0.229828</td>\n",
              "      <td>0.073673</td>\n",
              "      <td>0.256524</td>\n",
              "      <td>-0.438187</td>\n",
              "      <td>-0.097229</td>\n",
              "      <td>-0.740352</td>\n",
              "      <td>1.529813</td>\n",
              "      <td>-0.640376</td>\n",
              "      <td>1.717616</td>\n",
              "      <td>-1.703426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.688133</td>\n",
              "      <td>-0.318813</td>\n",
              "      <td>0.999343</td>\n",
              "      <td>-0.651598</td>\n",
              "      <td>-0.407260</td>\n",
              "      <td>-0.314173</td>\n",
              "      <td>0.781863</td>\n",
              "      <td>0.242623</td>\n",
              "      <td>-0.107326</td>\n",
              "      <td>-0.972608</td>\n",
              "      <td>-0.235344</td>\n",
              "      <td>-0.511505</td>\n",
              "      <td>-0.138459</td>\n",
              "      <td>0.775548</td>\n",
              "      <td>-0.218838</td>\n",
              "      <td>0.545011</td>\n",
              "      <td>-0.670150</td>\n",
              "      <td>0.699975</td>\n",
              "      <td>0.817735</td>\n",
              "      <td>-0.134486</td>\n",
              "      <td>0.054736</td>\n",
              "      <td>0.374199</td>\n",
              "      <td>0.183914</td>\n",
              "      <td>-0.104466</td>\n",
              "      <td>1.410521</td>\n",
              "      <td>-0.258784</td>\n",
              "      <td>0.201976</td>\n",
              "      <td>0.270087</td>\n",
              "      <td>0.458823</td>\n",
              "      <td>-0.055942</td>\n",
              "      <td>0.035193</td>\n",
              "      <td>0.027436</td>\n",
              "      <td>1.150917</td>\n",
              "      <td>0.318230</td>\n",
              "      <td>0.150467</td>\n",
              "      <td>0.657710</td>\n",
              "      <td>-0.184495</td>\n",
              "      <td>-0.571191</td>\n",
              "      <td>-0.604887</td>\n",
              "      <td>-0.231566</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>-0.379828</td>\n",
              "      <td>0.181649</td>\n",
              "      <td>0.533105</td>\n",
              "      <td>-0.220025</td>\n",
              "      <td>-0.499370</td>\n",
              "      <td>0.676739</td>\n",
              "      <td>-0.635464</td>\n",
              "      <td>-0.520307</td>\n",
              "      <td>-0.607030</td>\n",
              "      <td>0.455186</td>\n",
              "      <td>0.421789</td>\n",
              "      <td>-0.637401</td>\n",
              "      <td>-1.185999</td>\n",
              "      <td>-0.661417</td>\n",
              "      <td>0.244160</td>\n",
              "      <td>0.266237</td>\n",
              "      <td>-0.213120</td>\n",
              "      <td>0.249394</td>\n",
              "      <td>-0.382235</td>\n",
              "      <td>0.886608</td>\n",
              "      <td>-0.040592</td>\n",
              "      <td>0.387883</td>\n",
              "      <td>-0.961931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.721908</td>\n",
              "      <td>0.388303</td>\n",
              "      <td>0.947409</td>\n",
              "      <td>-0.787672</td>\n",
              "      <td>-0.362458</td>\n",
              "      <td>-0.536777</td>\n",
              "      <td>1.290188</td>\n",
              "      <td>0.130461</td>\n",
              "      <td>-0.550028</td>\n",
              "      <td>-0.882603</td>\n",
              "      <td>-0.284483</td>\n",
              "      <td>0.195723</td>\n",
              "      <td>0.287096</td>\n",
              "      <td>0.741486</td>\n",
              "      <td>-0.613376</td>\n",
              "      <td>0.241035</td>\n",
              "      <td>-1.183460</td>\n",
              "      <td>0.651605</td>\n",
              "      <td>1.076140</td>\n",
              "      <td>0.284106</td>\n",
              "      <td>-1.281615</td>\n",
              "      <td>-0.727382</td>\n",
              "      <td>0.383454</td>\n",
              "      <td>-0.202577</td>\n",
              "      <td>1.619946</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>-0.379740</td>\n",
              "      <td>0.251197</td>\n",
              "      <td>0.567685</td>\n",
              "      <td>-0.998128</td>\n",
              "      <td>-0.046036</td>\n",
              "      <td>-0.882844</td>\n",
              "      <td>0.740021</td>\n",
              "      <td>0.870464</td>\n",
              "      <td>0.363524</td>\n",
              "      <td>-0.268316</td>\n",
              "      <td>0.146103</td>\n",
              "      <td>0.422400</td>\n",
              "      <td>0.236998</td>\n",
              "      <td>-0.150375</td>\n",
              "      <td>0.789005</td>\n",
              "      <td>-1.078728</td>\n",
              "      <td>0.767913</td>\n",
              "      <td>1.031454</td>\n",
              "      <td>-1.020215</td>\n",
              "      <td>-0.265524</td>\n",
              "      <td>1.290493</td>\n",
              "      <td>-0.687332</td>\n",
              "      <td>-0.580685</td>\n",
              "      <td>-0.275520</td>\n",
              "      <td>0.351817</td>\n",
              "      <td>0.330816</td>\n",
              "      <td>-0.828862</td>\n",
              "      <td>-1.068953</td>\n",
              "      <td>-0.534331</td>\n",
              "      <td>-0.844828</td>\n",
              "      <td>0.119608</td>\n",
              "      <td>0.585099</td>\n",
              "      <td>0.437665</td>\n",
              "      <td>-1.466863</td>\n",
              "      <td>1.006176</td>\n",
              "      <td>-0.079589</td>\n",
              "      <td>0.593564</td>\n",
              "      <td>-1.152426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.650453</td>\n",
              "      <td>-0.146959</td>\n",
              "      <td>0.802256</td>\n",
              "      <td>-0.280101</td>\n",
              "      <td>-0.731831</td>\n",
              "      <td>-0.456279</td>\n",
              "      <td>0.775432</td>\n",
              "      <td>-0.465780</td>\n",
              "      <td>0.143515</td>\n",
              "      <td>-0.736436</td>\n",
              "      <td>0.108304</td>\n",
              "      <td>-0.252066</td>\n",
              "      <td>0.123692</td>\n",
              "      <td>0.359086</td>\n",
              "      <td>-0.440906</td>\n",
              "      <td>-0.286168</td>\n",
              "      <td>-0.400844</td>\n",
              "      <td>0.719755</td>\n",
              "      <td>0.256401</td>\n",
              "      <td>0.432786</td>\n",
              "      <td>-0.412044</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.045897</td>\n",
              "      <td>-0.033026</td>\n",
              "      <td>0.904498</td>\n",
              "      <td>0.185858</td>\n",
              "      <td>-0.173960</td>\n",
              "      <td>-0.415105</td>\n",
              "      <td>-0.231409</td>\n",
              "      <td>0.234141</td>\n",
              "      <td>0.299792</td>\n",
              "      <td>-0.222521</td>\n",
              "      <td>0.539466</td>\n",
              "      <td>0.284991</td>\n",
              "      <td>0.444050</td>\n",
              "      <td>0.151308</td>\n",
              "      <td>-0.550617</td>\n",
              "      <td>0.110464</td>\n",
              "      <td>-0.113436</td>\n",
              "      <td>-0.128214</td>\n",
              "      <td>-0.150840</td>\n",
              "      <td>-0.427944</td>\n",
              "      <td>0.444110</td>\n",
              "      <td>0.421779</td>\n",
              "      <td>-0.314363</td>\n",
              "      <td>-0.818235</td>\n",
              "      <td>0.472876</td>\n",
              "      <td>-0.264762</td>\n",
              "      <td>0.096127</td>\n",
              "      <td>-0.077573</td>\n",
              "      <td>0.012599</td>\n",
              "      <td>-0.271649</td>\n",
              "      <td>-0.296263</td>\n",
              "      <td>-0.772448</td>\n",
              "      <td>-0.508725</td>\n",
              "      <td>-0.160995</td>\n",
              "      <td>0.444162</td>\n",
              "      <td>0.061997</td>\n",
              "      <td>-0.419543</td>\n",
              "      <td>-0.450393</td>\n",
              "      <td>0.573340</td>\n",
              "      <td>-0.265548</td>\n",
              "      <td>0.279305</td>\n",
              "      <td>-0.208944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>832</td>\n",
              "      <td>0.092474</td>\n",
              "      <td>0.106150</td>\n",
              "      <td>0.033633</td>\n",
              "      <td>0.018442</td>\n",
              "      <td>0.365770</td>\n",
              "      <td>0.026157</td>\n",
              "      <td>0.192186</td>\n",
              "      <td>-0.005236</td>\n",
              "      <td>-0.217018</td>\n",
              "      <td>0.257617</td>\n",
              "      <td>0.053886</td>\n",
              "      <td>0.152891</td>\n",
              "      <td>-0.122927</td>\n",
              "      <td>0.130594</td>\n",
              "      <td>-0.034565</td>\n",
              "      <td>-0.123680</td>\n",
              "      <td>-0.024969</td>\n",
              "      <td>0.082651</td>\n",
              "      <td>-0.083302</td>\n",
              "      <td>-0.207902</td>\n",
              "      <td>-0.276132</td>\n",
              "      <td>0.104665</td>\n",
              "      <td>-0.138261</td>\n",
              "      <td>0.175597</td>\n",
              "      <td>0.038199</td>\n",
              "      <td>-0.013455</td>\n",
              "      <td>-0.133683</td>\n",
              "      <td>-0.142527</td>\n",
              "      <td>-0.042970</td>\n",
              "      <td>-0.077926</td>\n",
              "      <td>-0.069572</td>\n",
              "      <td>0.217869</td>\n",
              "      <td>-0.284175</td>\n",
              "      <td>0.278465</td>\n",
              "      <td>-0.007701</td>\n",
              "      <td>0.073461</td>\n",
              "      <td>-0.324276</td>\n",
              "      <td>-0.116313</td>\n",
              "      <td>0.068643</td>\n",
              "      <td>-0.304055</td>\n",
              "      <td>0.147916</td>\n",
              "      <td>-0.450253</td>\n",
              "      <td>0.243640</td>\n",
              "      <td>0.015325</td>\n",
              "      <td>0.139896</td>\n",
              "      <td>0.207937</td>\n",
              "      <td>-0.348907</td>\n",
              "      <td>-0.123072</td>\n",
              "      <td>0.070664</td>\n",
              "      <td>0.271669</td>\n",
              "      <td>0.138426</td>\n",
              "      <td>0.055811</td>\n",
              "      <td>0.010685</td>\n",
              "      <td>0.009590</td>\n",
              "      <td>-0.026956</td>\n",
              "      <td>0.143835</td>\n",
              "      <td>0.134296</td>\n",
              "      <td>-0.018494</td>\n",
              "      <td>-0.094349</td>\n",
              "      <td>0.005062</td>\n",
              "      <td>0.147475</td>\n",
              "      <td>0.069725</td>\n",
              "      <td>0.208504</td>\n",
              "      <td>-0.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>833</td>\n",
              "      <td>-0.072233</td>\n",
              "      <td>0.119163</td>\n",
              "      <td>0.096811</td>\n",
              "      <td>-0.164411</td>\n",
              "      <td>0.278347</td>\n",
              "      <td>-0.201982</td>\n",
              "      <td>0.081316</td>\n",
              "      <td>-0.061770</td>\n",
              "      <td>-0.086994</td>\n",
              "      <td>0.235601</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.266871</td>\n",
              "      <td>-0.139913</td>\n",
              "      <td>-0.100954</td>\n",
              "      <td>-0.204267</td>\n",
              "      <td>-0.009871</td>\n",
              "      <td>-0.132975</td>\n",
              "      <td>-0.115135</td>\n",
              "      <td>0.074661</td>\n",
              "      <td>-0.066648</td>\n",
              "      <td>-0.195009</td>\n",
              "      <td>0.198070</td>\n",
              "      <td>0.041986</td>\n",
              "      <td>0.103717</td>\n",
              "      <td>0.212957</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.222006</td>\n",
              "      <td>0.069433</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.136644</td>\n",
              "      <td>-0.192403</td>\n",
              "      <td>-0.032700</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>-0.030231</td>\n",
              "      <td>0.076892</td>\n",
              "      <td>-0.572047</td>\n",
              "      <td>0.032351</td>\n",
              "      <td>0.019072</td>\n",
              "      <td>-0.175355</td>\n",
              "      <td>0.103921</td>\n",
              "      <td>-0.086620</td>\n",
              "      <td>-0.005460</td>\n",
              "      <td>0.071132</td>\n",
              "      <td>0.409461</td>\n",
              "      <td>0.037381</td>\n",
              "      <td>-0.081257</td>\n",
              "      <td>0.180506</td>\n",
              "      <td>-0.170608</td>\n",
              "      <td>0.123282</td>\n",
              "      <td>0.220902</td>\n",
              "      <td>-0.175002</td>\n",
              "      <td>-0.025833</td>\n",
              "      <td>0.079203</td>\n",
              "      <td>-0.021000</td>\n",
              "      <td>-0.098711</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>0.020249</td>\n",
              "      <td>0.162249</td>\n",
              "      <td>0.026010</td>\n",
              "      <td>0.033548</td>\n",
              "      <td>0.223271</td>\n",
              "      <td>-0.203688</td>\n",
              "      <td>-0.038956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>834</td>\n",
              "      <td>-0.132909</td>\n",
              "      <td>0.104212</td>\n",
              "      <td>0.050959</td>\n",
              "      <td>-0.111834</td>\n",
              "      <td>0.357196</td>\n",
              "      <td>-0.133188</td>\n",
              "      <td>0.143785</td>\n",
              "      <td>-0.049507</td>\n",
              "      <td>-0.147305</td>\n",
              "      <td>0.251885</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.199905</td>\n",
              "      <td>-0.070194</td>\n",
              "      <td>-0.153948</td>\n",
              "      <td>-0.143667</td>\n",
              "      <td>-0.036536</td>\n",
              "      <td>-0.038589</td>\n",
              "      <td>-0.034700</td>\n",
              "      <td>0.060599</td>\n",
              "      <td>-0.025647</td>\n",
              "      <td>-0.243201</td>\n",
              "      <td>0.148838</td>\n",
              "      <td>-0.037812</td>\n",
              "      <td>0.111142</td>\n",
              "      <td>0.232710</td>\n",
              "      <td>-0.081121</td>\n",
              "      <td>-0.017971</td>\n",
              "      <td>0.097984</td>\n",
              "      <td>-0.012490</td>\n",
              "      <td>-0.099915</td>\n",
              "      <td>0.092442</td>\n",
              "      <td>-0.163616</td>\n",
              "      <td>-0.015880</td>\n",
              "      <td>0.066135</td>\n",
              "      <td>-0.025328</td>\n",
              "      <td>0.081988</td>\n",
              "      <td>-0.507710</td>\n",
              "      <td>0.107737</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>-0.200301</td>\n",
              "      <td>0.093443</td>\n",
              "      <td>-0.091668</td>\n",
              "      <td>0.068706</td>\n",
              "      <td>-0.022480</td>\n",
              "      <td>0.297613</td>\n",
              "      <td>0.091121</td>\n",
              "      <td>-0.158395</td>\n",
              "      <td>0.178886</td>\n",
              "      <td>-0.117903</td>\n",
              "      <td>0.128856</td>\n",
              "      <td>0.285126</td>\n",
              "      <td>-0.158185</td>\n",
              "      <td>-0.052871</td>\n",
              "      <td>0.033910</td>\n",
              "      <td>-0.026669</td>\n",
              "      <td>-0.054522</td>\n",
              "      <td>0.224450</td>\n",
              "      <td>0.091497</td>\n",
              "      <td>0.057863</td>\n",
              "      <td>0.031415</td>\n",
              "      <td>-0.005544</td>\n",
              "      <td>0.270046</td>\n",
              "      <td>-0.138641</td>\n",
              "      <td>-0.049895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>835</td>\n",
              "      <td>-0.079618</td>\n",
              "      <td>-0.154153</td>\n",
              "      <td>0.295142</td>\n",
              "      <td>-0.150358</td>\n",
              "      <td>0.500962</td>\n",
              "      <td>-0.328006</td>\n",
              "      <td>-0.015192</td>\n",
              "      <td>-0.219420</td>\n",
              "      <td>-0.377795</td>\n",
              "      <td>0.257173</td>\n",
              "      <td>-0.064142</td>\n",
              "      <td>0.492528</td>\n",
              "      <td>-0.080609</td>\n",
              "      <td>-0.044967</td>\n",
              "      <td>-0.154126</td>\n",
              "      <td>-0.143692</td>\n",
              "      <td>0.100103</td>\n",
              "      <td>0.163071</td>\n",
              "      <td>-0.012129</td>\n",
              "      <td>-0.236396</td>\n",
              "      <td>-0.380849</td>\n",
              "      <td>0.415483</td>\n",
              "      <td>0.029281</td>\n",
              "      <td>0.239447</td>\n",
              "      <td>0.465136</td>\n",
              "      <td>-0.272850</td>\n",
              "      <td>0.155511</td>\n",
              "      <td>0.075649</td>\n",
              "      <td>0.088600</td>\n",
              "      <td>-0.196622</td>\n",
              "      <td>0.121215</td>\n",
              "      <td>-0.192279</td>\n",
              "      <td>0.005945</td>\n",
              "      <td>0.065828</td>\n",
              "      <td>-0.015154</td>\n",
              "      <td>-0.131507</td>\n",
              "      <td>-0.721625</td>\n",
              "      <td>0.131845</td>\n",
              "      <td>-0.092926</td>\n",
              "      <td>-0.340395</td>\n",
              "      <td>0.505998</td>\n",
              "      <td>-0.165514</td>\n",
              "      <td>0.192104</td>\n",
              "      <td>-0.060656</td>\n",
              "      <td>0.373459</td>\n",
              "      <td>0.290300</td>\n",
              "      <td>-0.260470</td>\n",
              "      <td>0.298324</td>\n",
              "      <td>-0.158419</td>\n",
              "      <td>0.281835</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>0.106287</td>\n",
              "      <td>0.018379</td>\n",
              "      <td>-0.148565</td>\n",
              "      <td>-0.095972</td>\n",
              "      <td>0.352697</td>\n",
              "      <td>0.146865</td>\n",
              "      <td>0.086388</td>\n",
              "      <td>0.010307</td>\n",
              "      <td>0.194430</td>\n",
              "      <td>0.168985</td>\n",
              "      <td>-0.259337</td>\n",
              "      <td>-0.073086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>836</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.008422</td>\n",
              "      <td>0.717436</td>\n",
              "      <td>0.134245</td>\n",
              "      <td>0.373170</td>\n",
              "      <td>-0.059227</td>\n",
              "      <td>0.102996</td>\n",
              "      <td>0.044156</td>\n",
              "      <td>-0.102515</td>\n",
              "      <td>0.433158</td>\n",
              "      <td>0.158489</td>\n",
              "      <td>0.265962</td>\n",
              "      <td>-0.011416</td>\n",
              "      <td>-0.031199</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>-0.496083</td>\n",
              "      <td>-0.352736</td>\n",
              "      <td>-0.257647</td>\n",
              "      <td>0.161620</td>\n",
              "      <td>-0.230452</td>\n",
              "      <td>-0.116733</td>\n",
              "      <td>0.159655</td>\n",
              "      <td>-0.278142</td>\n",
              "      <td>0.125518</td>\n",
              "      <td>0.496362</td>\n",
              "      <td>0.058816</td>\n",
              "      <td>-0.318031</td>\n",
              "      <td>-0.112670</td>\n",
              "      <td>-0.150040</td>\n",
              "      <td>-0.268167</td>\n",
              "      <td>-0.436891</td>\n",
              "      <td>0.335615</td>\n",
              "      <td>-0.349596</td>\n",
              "      <td>0.179373</td>\n",
              "      <td>-0.294503</td>\n",
              "      <td>0.210135</td>\n",
              "      <td>-0.615341</td>\n",
              "      <td>-0.007536</td>\n",
              "      <td>0.045706</td>\n",
              "      <td>-0.705685</td>\n",
              "      <td>-0.061241</td>\n",
              "      <td>-0.744448</td>\n",
              "      <td>0.617036</td>\n",
              "      <td>-0.066599</td>\n",
              "      <td>0.155325</td>\n",
              "      <td>0.487323</td>\n",
              "      <td>-0.912633</td>\n",
              "      <td>0.122922</td>\n",
              "      <td>0.049172</td>\n",
              "      <td>0.452203</td>\n",
              "      <td>-0.023311</td>\n",
              "      <td>-0.192136</td>\n",
              "      <td>0.286585</td>\n",
              "      <td>0.194225</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.318570</td>\n",
              "      <td>-0.017260</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>-0.195706</td>\n",
              "      <td>-0.051707</td>\n",
              "      <td>0.267640</td>\n",
              "      <td>-0.421967</td>\n",
              "      <td>0.063392</td>\n",
              "      <td>0.015846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>837 rows × 65 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0         1         2         3   ...        61        62        63        64\n",
              "0      0  0.770633  0.010733  0.582718  ...  1.209129  0.368444  0.414352 -1.065435\n",
              "1      1  2.882159  1.241338  2.232549  ...  1.529813 -0.640376  1.717616 -1.703426\n",
              "2      2  0.688133 -0.318813  0.999343  ...  0.886608 -0.040592  0.387883 -0.961931\n",
              "3      3  1.721908  0.388303  0.947409  ...  1.006176 -0.079589  0.593564 -1.152426\n",
              "4      4  0.650453 -0.146959  0.802256  ...  0.573340 -0.265548  0.279305 -0.208944\n",
              "..   ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
              "832  832  0.092474  0.106150  0.033633  ...  0.147475  0.069725  0.208504 -0.096100\n",
              "833  833 -0.072233  0.119163  0.096811  ...  0.033548  0.223271 -0.203688 -0.038956\n",
              "834  834 -0.132909  0.104212  0.050959  ... -0.005544  0.270046 -0.138641 -0.049895\n",
              "835  835 -0.079618 -0.154153  0.295142  ...  0.194430  0.168985 -0.259337 -0.073086\n",
              "836  836  0.239000  0.008422  0.717436  ...  0.267640 -0.421967  0.063392  0.015846\n",
              "\n",
              "[837 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ozu5hHltHR",
        "outputId": "dee25bda-10eb-4467-dd4d-778694b9c233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 将词向量提取为特征,第二列到倒数第一列\n",
        "features =node_features.iloc[:,1:]\n",
        " # 检查特征：共64个特征，837个样本点\n",
        "print(features.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(837, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gypPFc0lb_c",
        "outputId": "27a0798b-f7ad-4890-b875-e97df370b86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 提取节点标签\n",
        "node_label = pd.read_csv('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/AllNode_label.csv',header = None)\n",
        "labels = node_label[1] # 提取节点标签列\n",
        "labels[:5]\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yiryWH9Npol"
      },
      "source": [
        "filename1 = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv'\n",
        "def load_file_as_Adj_matrix(filename):\n",
        "  '''\n",
        "  Read .rating file and Return dok matrix.\n",
        "  The first line of .rating file is: num_users\\t num_items\n",
        "  '''\n",
        "  # # Get number of users and items\n",
        "  # num_users, num_items = 0, 0\n",
        "  # with open(filename, \"r\") as f:\n",
        "  #   line = f.readline()\n",
        "  #   while line != None and line != \"\":\n",
        "  #     arr = line.split(\",\")\n",
        "  #     u, i = int(arr[0]), int(arr[1])\n",
        "  #     num_users = max(num_users, u)\n",
        "  #     num_items = max(num_items, i)\n",
        "  #     line = f.readline()\n",
        "  # # Construct matrix\n",
        "  # print(num_users)\n",
        "  # print(num_items)\n",
        "  relation_matrix = np.zeros((837,837))\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      # user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "      # if (rating > 0):\n",
        "      user, item = int(arr[0]), int(arr[1])\n",
        "      relation_matrix[user, item] = 1\n",
        "      line = f.readline()    \n",
        "  return relation_matrix\n",
        "Adj = load_file_as_Adj_matrix(filename1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s8tkwn1Q1H1",
        "outputId": "1c2cf3df-e145-4a2f-d087-7a868f7dc454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Adj[0,268]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEMcCle2ntbd",
        "outputId": "a1b0cec2-c88c-40fa-85c7-84d7e072c255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import scipy.sparse as sp\n",
        "Adj = sp.csr_matrix(Adj, dtype=np.float32)\n",
        "Adj.todense()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUCPEuDYdh4"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "def load_data(adj,node_features,node_labels):\n",
        "  features = sp.csr_matrix(node_features, dtype=np.float32)  # 储存为csr型稀疏矩阵\n",
        "  # build symmetric adjacency matrix   论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
        "  # adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "  # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
        "  features = normalize(features)\n",
        "  adj = normalize(adj + sp.eye(adj.shape[0]))   # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
        "  # 对应公式A~=A+IN\n",
        "  # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
        "  idx_train = range(500)\n",
        "  idx_val = range(500, 660)\n",
        "  idx_test = range(660, 836)  \n",
        "  features = torch.FloatTensor(np.array(features.todense()))  # tensor为pytorch常用的数据结构\n",
        "  labels = torch.LongTensor(np.array(node_labels))\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj)   # 邻接矩阵转为tensor处理\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "  idx_val = torch.LongTensor(idx_val)\n",
        "  idx_test = torch.LongTensor(idx_test)\n",
        "  return adj, features, labels, idx_train, idx_val, idx_test  \n",
        "def normalize(mx):\n",
        "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))  # 对每一行求和\n",
        "  r_inv = np.power(rowsum, -1).flatten()  # 求倒数\n",
        "  r_inv[np.isinf(r_inv)] = 0.  # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
        "  r_mat_inv = sp.diags(r_inv)  # 构建对角元素为r_inv的对角矩阵\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
        "  return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels) # 使用type_as(tesnor)将张量转换为给定类型的张量。\n",
        "  correct = preds.eq(labels).double()  # 记录等于preds的label eq:equal\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):    # 把一个sparse matrix转为torch稀疏张量\n",
        "  \"\"\"\n",
        "  numpy中的ndarray转化成pytorch中的tensor : torch.from_numpy()\n",
        "  pytorch中的tensor转化成numpy中的ndarray : numpy()\n",
        "  \"\"\"\n",
        "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "  indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "  # 不懂的可以去看看COO性稀疏矩阵的结构\n",
        "  values = torch.from_numpy(sparse_mx.data)\n",
        "  shape = torch.Size(sparse_mx.shape)\n",
        "  return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEF3l9vGxWhX"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "\n",
        "    # 初始化层：输入feature，输出feature，权重，偏移\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # FloatTensor建立tensor\n",
        "        # 常见用法self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))：\n",
        "        # 首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter\n",
        "        # 绑定到这个module里面，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。\n",
        "        # 使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "            # Parameters与register_parameter都会向parameters写入参数，但是后者可以支持字符串命名\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # 初始化权重\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        # size()函数主要是用来统计矩阵元素个数，或矩阵某一维上的元素个数的函数  size（1）为行\n",
        "        self.weight.data.uniform_(-stdv, stdv)  # uniform() 方法将随机生成下一个实数，它在 [x, y] 范围内\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    '''\n",
        "    前馈运算 即计算A~ X W(0)\n",
        "    input X与权重W相乘，然后adj矩阵与他们的积稀疏乘\n",
        "    直接输入与权重之间进行torch.mm操作，得到support，即XW\n",
        "    support与adj进行torch.spmm操作，得到output，即AXW选择是否加bias\n",
        "    '''\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # torch.mm(a, b)是矩阵a和b矩阵相乘，torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "#通过设置断点，可以看出output的形式是0.01，0.01，0.01，0.01，0.01，#0.01，0.94]，里面的值代表该x对应标签不同的概率，故此值可转换为#[0,0,0,0,0,0,1]，对应我们之前把标签onthot后的第七种标签\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NWtCnVPB4nb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "    # 底层节点的参数，feature的个数；隐层节点个数；最终的分类数\n",
        "    super(GCN, self).__init__()  #  super()._init_()在利用父类里的对象构造函数\n",
        "    self.gc1 = GraphConvolution(nfeat, nhid)   # gc1输入尺寸nfeat，输出尺寸nhid\n",
        "    self.gc2 = GraphConvolution(nhid, nclass)  # gc2输入尺寸nhid，输出尺寸ncalss\n",
        "    self.dropout = dropout\n",
        "    # 输入分别是特征和邻接矩阵。最后输出为输出层做log_softmax变换的结果\n",
        "  def forward(self, x, adj):\n",
        "    x = F.relu(self.gc1(x, adj))    # adj即公式Z=softmax(A~Relu(A~XW(0))W(1))中的A~\n",
        "    x1 = F.dropout(x, self.dropout, training = self.training)  # x要dropout\n",
        "    x2 = self.gc2(x1, adj)\n",
        "    return F.log_softmax(x2, dim = 1), x#, x  # 参数dim=1表示对每一行求softmax，那么每一行的值加起来都等于1。\n",
        "    #return x, x2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ryUWoS5GX-",
        "outputId": "1b627c70-19fb-4feb-b011-b4fc2f03fa32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "data=autograd.Variable(torch.FloatTensor([-0.3544, -1.2094]))\n",
        "log_softmax=F.log_softmax(data,dim=0)\n",
        "print(log_softmax)\n",
        "\n",
        "softmax=F.softmax(data,dim=0)\n",
        "print(softmax)\n",
        "\n",
        "np_softmax=softmax.data.numpy()\n",
        "log_np_softmax=np.log(np_softmax)\n",
        "print(log_np_softmax)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.3544, -1.2094])\n",
            "tensor([0.7016, 0.2984])\n",
            "[-0.3543705 -1.2093705]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ZuShsc6eZJ",
        "outputId": "9ea045c8-04a8-4fc5-cfd0-d0813c4dc0ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse  # argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(Adj,features,labels)\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=2,\n",
        "            dropout=0.02)\n",
        "output, xx = model(features, adj)\n",
        "#xx = model(features, adj)\n",
        "print(xx[:10])\n",
        "print(output[:10])\n",
        "#"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3504, 0.0368, 0.0000,\n",
            "         0.0306, 0.0000, 0.0000, 0.0000, 0.4200, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0979, 0.0000, 0.0000, 0.1827, 0.0877, 0.4214, 0.2650, 0.0000,\n",
            "         0.1621, 0.0000, 0.0000, 0.0906, 0.0000, 0.0000, 0.6383],\n",
            "        [0.0000, 0.0000, 0.0822, 0.0000, 0.0000, 0.0000, 0.3116, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0594, 0.0000, 0.0194],\n",
            "        [0.0000, 0.0000, 0.0142, 0.0000, 0.0366, 0.0000, 0.3881, 0.0693, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1441, 0.1329],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0503, 0.0540, 0.3251, 0.1410, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.2650, 0.1030, 0.1048],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0796, 0.4451, 0.0457, 0.0000,\n",
            "         0.0000, 0.0531, 0.0000, 0.0000, 0.4775, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0103, 0.5062, 0.0534, 0.0000,\n",
            "         0.0000, 0.0548, 0.0000, 0.0000, 0.4740, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0176, 0.0000, 0.0000, 0.0000, 0.2017, 0.0049, 0.0090,\n",
            "         0.0000, 0.0000, 0.0662, 0.0000, 0.4415, 0.0432, 0.0000],\n",
            "        [0.1731, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.8751, 0.0208, 0.6707, 0.5066, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0699, 0.0000, 0.0439, 0.0354, 0.4149, 0.0000, 0.0000,\n",
            "         0.0655, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3048]],\n",
            "       grad_fn=<SliceBackward>)\n",
            "tensor([[-0.4418, -1.0297],\n",
            "        [-0.2827, -1.4013],\n",
            "        [-0.3872, -1.1362],\n",
            "        [-0.3960, -1.1178],\n",
            "        [-0.4496, -1.0158],\n",
            "        [-0.4686, -0.9831],\n",
            "        [-0.4685, -0.9833],\n",
            "        [-0.4441, -1.0255],\n",
            "        [-0.3818, -1.1477],\n",
            "        [-0.3405, -1.2426]], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOP3LWgtBa3z"
      },
      "source": [
        "output[:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peLF-dRCaj-M",
        "outputId": "fd6c74df-08bf-4727-f970-dbe10e42deb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "def div_list(ls,n):\n",
        "    ls_len=len(ls)  \n",
        "    j = ls_len//n\n",
        "    ls_return = []  \n",
        "    for i in range(0,(n-1)*j,j):  \n",
        "        ls_return.append(ls[i:i+j])  \n",
        "    ls_return.append(ls[(n-1)*j:])  \n",
        "    return ls_return\n",
        "cv_num =5\n",
        "reorder = np.arange(3334)\n",
        "np.random.shuffle(reorder)\n",
        "order = div_list(reorder.tolist(),cv_num)    \n",
        "for i in range(cv_num):\n",
        "  print(\"cross_validation:\", '%01d' % (i))\n",
        "  test_arr = order[i]\n",
        "  arr = list(set(reorder).difference(set(test_arr)))\n",
        "  np.random.shuffle(arr)\n",
        "  train_arr = arr\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cross_validation: 0\n",
            "cross_validation: 1\n",
            "cross_validation: 2\n",
            "cross_validation: 3\n",
            "cross_validation: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9JVSKDhxw00",
        "outputId": "62a74c39-ed4c-42ea-bc5f-7a1e4f5abce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import argparse  # argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Training settings\n",
        "learning_rate = 0.01\n",
        "weight_decay = 5e-4\n",
        "epoch_num = 500\n",
        "dropout = 0.02\n",
        "#in_size = node_features  #设置输入层的维数\n",
        "hi_size = 16 # 16 #设置隐藏层的维数\n",
        "#out_size = node_label #设置输入层的维数\n",
        "\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(Adj,features,labels)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=hi_size,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# 数据写入cuda，便于后续加速\n",
        "\n",
        "# if args.cuda:\n",
        "#     model.cuda()   # . cuda()会分配到显存里（如果gpu可用）\n",
        "#     features = features.cuda()\n",
        "#     adj = adj.cuda()\n",
        "#     labels = labels.cuda()\n",
        "#     idx_train = idx_train.cuda()\n",
        "#     idx_val = idx_val.cuda()\n",
        "#     idx_test = idx_test.cuda()\n",
        "#global node_vec\n",
        "train_loss = []\n",
        "def train(epoch_num):\n",
        "  t = time.time()  # 返回当前时间\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  # optimizer.zero_grad()意思是把梯度置零，也就是把loss关于weight的导数变成0.\n",
        "  # pytorch中每一轮batch需要设置optimizer.zero_gra\n",
        "  global Emdebding_train, output\n",
        "  output, Emdebding_train = model(features, adj)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  train_loss.append(loss_train)\n",
        "  # 由于在算output时已经使用了log_softmax，这里使用的损失函数就是NLLloss，如果前面没有加log运算，\n",
        "  # 这里就要使用CrossEntropyLoss了\n",
        "  # 损失函数NLLLoss() 的输入是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率，\n",
        "  # 适合最后一层是log_softmax()的网络. 损失函数 CrossEntropyLoss() 与 NLLLoss() 类似,\n",
        "  # 唯一的不同是它为我们去做 softmax.可以理解为：CrossEntropyLoss()=log_softmax() + NLLLoss()\n",
        "  # https://blog.csdn.net/hao5335156/article/details/80607732\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])  #计算准确率\n",
        "  loss_train.backward()  # 反向求导  Back Propagation\n",
        "  optimizer.step()  # 更新所有的参数  Gradient Descent\n",
        "    \n",
        "  #if not args.fastmode:\n",
        "      # Evaluate validation set performance separately,\n",
        "      # deactivates dropout during validation run.\n",
        "  model.eval()  # eval() 函数用来执行一个字符串表达式，并返回表达式的值\n",
        "  global Emdebding_eval\n",
        "  output, Emdebding_eval = model(features, adj)\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])    # 验证集的损失函数\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),     \n",
        "        'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "        'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "        'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "        'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "        'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "# 定义测试函数，相当于对已有的模型在测试集上运行对应的loss与accuracy\n",
        "def test():\n",
        "  model.eval()\n",
        "  global Emdebding_test\n",
        "  output, Emdebding_test = model(features, adj)\n",
        "  loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "  acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "  print(\"Test set results:\",\n",
        "        \"loss= {:.4f}\".format(loss_test.item()),\n",
        "        \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "# Train model  逐个epoch进行train，最后test\n",
        "t_total = time.time()\n",
        "for epoch in range(epoch_num):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "test()\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "epochs = len(train_loss)\n",
        "plt.plot(range(0,epochs,1), train_loss, label='train_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "#plt.savefig(\"/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/\"+time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))+\"Unet-过拟合C0.jpg\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 0.9898 acc_train: 0.5340 loss_val: 1.5705 acc_val: 0.0187 time: 0.0236s\n",
            "Epoch: 0002 loss_train: 0.8818 acc_train: 0.5380 loss_val: 1.4289 acc_val: 0.0187 time: 0.0081s\n",
            "Epoch: 0003 loss_train: 0.8033 acc_train: 0.5320 loss_val: 1.3364 acc_val: 0.0312 time: 0.0063s\n",
            "Epoch: 0004 loss_train: 0.7935 acc_train: 0.5160 loss_val: 1.2789 acc_val: 0.0563 time: 0.0094s\n",
            "Epoch: 0005 loss_train: 0.7768 acc_train: 0.5160 loss_val: 1.2585 acc_val: 0.1062 time: 0.0074s\n",
            "Epoch: 0006 loss_train: 0.7657 acc_train: 0.5240 loss_val: 1.2748 acc_val: 0.1313 time: 0.0074s\n",
            "Epoch: 0007 loss_train: 0.7538 acc_train: 0.5360 loss_val: 1.3224 acc_val: 0.1812 time: 0.0073s\n",
            "Epoch: 0008 loss_train: 0.7430 acc_train: 0.5540 loss_val: 1.3960 acc_val: 0.1750 time: 0.0067s\n",
            "Epoch: 0009 loss_train: 0.7280 acc_train: 0.5660 loss_val: 1.4903 acc_val: 0.1375 time: 0.0068s\n",
            "Epoch: 0010 loss_train: 0.7141 acc_train: 0.5620 loss_val: 1.5967 acc_val: 0.1187 time: 0.0072s\n",
            "Epoch: 0011 loss_train: 0.7024 acc_train: 0.5560 loss_val: 1.7062 acc_val: 0.0938 time: 0.0072s\n",
            "Epoch: 0012 loss_train: 0.6904 acc_train: 0.5720 loss_val: 1.8075 acc_val: 0.0750 time: 0.0072s\n",
            "Epoch: 0013 loss_train: 0.6835 acc_train: 0.5840 loss_val: 1.9133 acc_val: 0.0625 time: 0.0073s\n",
            "Epoch: 0014 loss_train: 0.6682 acc_train: 0.5940 loss_val: 2.0001 acc_val: 0.0500 time: 0.0087s\n",
            "Epoch: 0015 loss_train: 0.6700 acc_train: 0.6020 loss_val: 2.0558 acc_val: 0.0437 time: 0.0073s\n",
            "Epoch: 0016 loss_train: 0.6683 acc_train: 0.5940 loss_val: 2.1003 acc_val: 0.0563 time: 0.0071s\n",
            "Epoch: 0017 loss_train: 0.6638 acc_train: 0.6000 loss_val: 2.1370 acc_val: 0.0563 time: 0.0072s\n",
            "Epoch: 0018 loss_train: 0.6593 acc_train: 0.5980 loss_val: 2.1607 acc_val: 0.0750 time: 0.0082s\n",
            "Epoch: 0019 loss_train: 0.6576 acc_train: 0.6040 loss_val: 2.1742 acc_val: 0.0875 time: 0.0069s\n",
            "Epoch: 0020 loss_train: 0.6515 acc_train: 0.6120 loss_val: 2.1859 acc_val: 0.1688 time: 0.0068s\n",
            "Epoch: 0021 loss_train: 0.6480 acc_train: 0.6460 loss_val: 2.1812 acc_val: 0.2188 time: 0.0076s\n",
            "Epoch: 0022 loss_train: 0.6536 acc_train: 0.6600 loss_val: 2.1722 acc_val: 0.2562 time: 0.0071s\n",
            "Epoch: 0023 loss_train: 0.6495 acc_train: 0.6720 loss_val: 2.1621 acc_val: 0.3187 time: 0.0068s\n",
            "Epoch: 0024 loss_train: 0.6442 acc_train: 0.6420 loss_val: 2.1707 acc_val: 0.3563 time: 0.0071s\n",
            "Epoch: 0025 loss_train: 0.6433 acc_train: 0.6320 loss_val: 2.1939 acc_val: 0.3812 time: 0.0064s\n",
            "Epoch: 0026 loss_train: 0.6470 acc_train: 0.6080 loss_val: 2.2478 acc_val: 0.3875 time: 0.0069s\n",
            "Epoch: 0027 loss_train: 0.6387 acc_train: 0.6380 loss_val: 2.3151 acc_val: 0.3750 time: 0.0080s\n",
            "Epoch: 0028 loss_train: 0.6365 acc_train: 0.6620 loss_val: 2.3910 acc_val: 0.3812 time: 0.0080s\n",
            "Epoch: 0029 loss_train: 0.6391 acc_train: 0.6980 loss_val: 2.4650 acc_val: 0.3750 time: 0.0075s\n",
            "Epoch: 0030 loss_train: 0.6319 acc_train: 0.7080 loss_val: 2.5352 acc_val: 0.3625 time: 0.0072s\n",
            "Epoch: 0031 loss_train: 0.6322 acc_train: 0.7140 loss_val: 2.5966 acc_val: 0.3688 time: 0.0065s\n",
            "Epoch: 0032 loss_train: 0.6276 acc_train: 0.7220 loss_val: 2.6573 acc_val: 0.3875 time: 0.0065s\n",
            "Epoch: 0033 loss_train: 0.6292 acc_train: 0.7460 loss_val: 2.7111 acc_val: 0.4125 time: 0.0074s\n",
            "Epoch: 0034 loss_train: 0.6233 acc_train: 0.7500 loss_val: 2.7586 acc_val: 0.4250 time: 0.0073s\n",
            "Epoch: 0035 loss_train: 0.6222 acc_train: 0.7580 loss_val: 2.8008 acc_val: 0.4375 time: 0.0068s\n",
            "Epoch: 0036 loss_train: 0.6237 acc_train: 0.7540 loss_val: 2.8355 acc_val: 0.4375 time: 0.0071s\n",
            "Epoch: 0037 loss_train: 0.6214 acc_train: 0.7620 loss_val: 2.8609 acc_val: 0.4562 time: 0.0072s\n",
            "Epoch: 0038 loss_train: 0.6307 acc_train: 0.7820 loss_val: 2.8530 acc_val: 0.4750 time: 0.0070s\n",
            "Epoch: 0039 loss_train: 0.6152 acc_train: 0.7720 loss_val: 2.8521 acc_val: 0.4813 time: 0.0064s\n",
            "Epoch: 0040 loss_train: 0.6163 acc_train: 0.7320 loss_val: 2.8655 acc_val: 0.4938 time: 0.0063s\n",
            "Epoch: 0041 loss_train: 0.6160 acc_train: 0.6900 loss_val: 2.8972 acc_val: 0.5000 time: 0.0080s\n",
            "Epoch: 0042 loss_train: 0.6157 acc_train: 0.7020 loss_val: 2.9201 acc_val: 0.5000 time: 0.0076s\n",
            "Epoch: 0043 loss_train: 0.6137 acc_train: 0.6800 loss_val: 2.9682 acc_val: 0.5000 time: 0.0075s\n",
            "Epoch: 0044 loss_train: 0.6128 acc_train: 0.6920 loss_val: 3.0346 acc_val: 0.5000 time: 0.0067s\n",
            "Epoch: 0045 loss_train: 0.6105 acc_train: 0.6860 loss_val: 3.1115 acc_val: 0.4938 time: 0.0072s\n",
            "Epoch: 0046 loss_train: 0.6142 acc_train: 0.7060 loss_val: 3.1992 acc_val: 0.5062 time: 0.0073s\n",
            "Epoch: 0047 loss_train: 0.6066 acc_train: 0.7400 loss_val: 3.2863 acc_val: 0.5000 time: 0.0072s\n",
            "Epoch: 0048 loss_train: 0.6069 acc_train: 0.7500 loss_val: 3.3690 acc_val: 0.4750 time: 0.0095s\n",
            "Epoch: 0049 loss_train: 0.5986 acc_train: 0.7460 loss_val: 3.4390 acc_val: 0.4500 time: 0.0098s\n",
            "Epoch: 0050 loss_train: 0.6033 acc_train: 0.7500 loss_val: 3.4632 acc_val: 0.4688 time: 0.0079s\n",
            "Epoch: 0051 loss_train: 0.6020 acc_train: 0.7480 loss_val: 3.4553 acc_val: 0.5000 time: 0.0069s\n",
            "Epoch: 0052 loss_train: 0.6000 acc_train: 0.7280 loss_val: 3.4511 acc_val: 0.5125 time: 0.0067s\n",
            "Epoch: 0053 loss_train: 0.5978 acc_train: 0.7060 loss_val: 3.4634 acc_val: 0.5250 time: 0.0085s\n",
            "Epoch: 0054 loss_train: 0.6025 acc_train: 0.6900 loss_val: 3.4988 acc_val: 0.5250 time: 0.0079s\n",
            "Epoch: 0055 loss_train: 0.6027 acc_train: 0.7040 loss_val: 3.5586 acc_val: 0.5188 time: 0.0076s\n",
            "Epoch: 0056 loss_train: 0.5967 acc_train: 0.6880 loss_val: 3.6348 acc_val: 0.5250 time: 0.0074s\n",
            "Epoch: 0057 loss_train: 0.5949 acc_train: 0.6880 loss_val: 3.7190 acc_val: 0.5000 time: 0.0110s\n",
            "Epoch: 0058 loss_train: 0.5936 acc_train: 0.7240 loss_val: 3.8031 acc_val: 0.4813 time: 0.0077s\n",
            "Epoch: 0059 loss_train: 0.5957 acc_train: 0.7240 loss_val: 3.8449 acc_val: 0.4813 time: 0.0077s\n",
            "Epoch: 0060 loss_train: 0.5943 acc_train: 0.7300 loss_val: 3.8549 acc_val: 0.5000 time: 0.0078s\n",
            "Epoch: 0061 loss_train: 0.5834 acc_train: 0.7280 loss_val: 3.8714 acc_val: 0.5125 time: 0.0072s\n",
            "Epoch: 0062 loss_train: 0.5886 acc_train: 0.6960 loss_val: 3.9036 acc_val: 0.5250 time: 0.0081s\n",
            "Epoch: 0063 loss_train: 0.5848 acc_train: 0.6940 loss_val: 3.9525 acc_val: 0.5312 time: 0.0068s\n",
            "Epoch: 0064 loss_train: 0.5877 acc_train: 0.7020 loss_val: 4.0125 acc_val: 0.5437 time: 0.0079s\n",
            "Epoch: 0065 loss_train: 0.5869 acc_train: 0.7120 loss_val: 4.0415 acc_val: 0.5500 time: 0.0078s\n",
            "Epoch: 0066 loss_train: 0.6047 acc_train: 0.7020 loss_val: 4.0846 acc_val: 0.5500 time: 0.0075s\n",
            "Epoch: 0067 loss_train: 0.5809 acc_train: 0.7000 loss_val: 4.1403 acc_val: 0.5563 time: 0.0077s\n",
            "Epoch: 0068 loss_train: 0.5881 acc_train: 0.7140 loss_val: 4.1954 acc_val: 0.5312 time: 0.0071s\n",
            "Epoch: 0069 loss_train: 0.5837 acc_train: 0.7260 loss_val: 4.2346 acc_val: 0.5437 time: 0.0071s\n",
            "Epoch: 0070 loss_train: 0.5810 acc_train: 0.7060 loss_val: 4.2740 acc_val: 0.5437 time: 0.0070s\n",
            "Epoch: 0071 loss_train: 0.5794 acc_train: 0.7220 loss_val: 4.3111 acc_val: 0.5437 time: 0.0068s\n",
            "Epoch: 0072 loss_train: 0.5805 acc_train: 0.7240 loss_val: 4.3415 acc_val: 0.5312 time: 0.0065s\n",
            "Epoch: 0073 loss_train: 0.5830 acc_train: 0.7020 loss_val: 4.3778 acc_val: 0.5375 time: 0.0068s\n",
            "Epoch: 0074 loss_train: 0.5872 acc_train: 0.7200 loss_val: 4.3894 acc_val: 0.5500 time: 0.0066s\n",
            "Epoch: 0075 loss_train: 0.5919 acc_train: 0.7240 loss_val: 4.3551 acc_val: 0.6125 time: 0.0077s\n",
            "Epoch: 0076 loss_train: 0.5791 acc_train: 0.6940 loss_val: 4.3583 acc_val: 0.6250 time: 0.0071s\n",
            "Epoch: 0077 loss_train: 0.5758 acc_train: 0.6840 loss_val: 4.3995 acc_val: 0.6250 time: 0.0073s\n",
            "Epoch: 0078 loss_train: 0.5740 acc_train: 0.6940 loss_val: 4.4675 acc_val: 0.6125 time: 0.0062s\n",
            "Epoch: 0079 loss_train: 0.5717 acc_train: 0.7140 loss_val: 4.5516 acc_val: 0.5750 time: 0.0095s\n",
            "Epoch: 0080 loss_train: 0.5672 acc_train: 0.7240 loss_val: 4.6278 acc_val: 0.5625 time: 0.0075s\n",
            "Epoch: 0081 loss_train: 0.5752 acc_train: 0.7320 loss_val: 4.6965 acc_val: 0.5500 time: 0.0066s\n",
            "Epoch: 0082 loss_train: 0.5708 acc_train: 0.7160 loss_val: 4.6955 acc_val: 0.5625 time: 0.0065s\n",
            "Epoch: 0083 loss_train: 0.5683 acc_train: 0.7360 loss_val: 4.6736 acc_val: 0.6188 time: 0.0072s\n",
            "Epoch: 0084 loss_train: 0.5625 acc_train: 0.7180 loss_val: 4.6652 acc_val: 0.6312 time: 0.0071s\n",
            "Epoch: 0085 loss_train: 0.5657 acc_train: 0.7020 loss_val: 4.6871 acc_val: 0.6375 time: 0.0065s\n",
            "Epoch: 0086 loss_train: 0.5711 acc_train: 0.6800 loss_val: 4.6949 acc_val: 0.6562 time: 0.0071s\n",
            "Epoch: 0087 loss_train: 0.5635 acc_train: 0.6820 loss_val: 4.7478 acc_val: 0.6562 time: 0.0070s\n",
            "Epoch: 0088 loss_train: 0.5631 acc_train: 0.6880 loss_val: 4.8376 acc_val: 0.6500 time: 0.0075s\n",
            "Epoch: 0089 loss_train: 0.5671 acc_train: 0.7080 loss_val: 4.9459 acc_val: 0.6125 time: 0.0075s\n",
            "Epoch: 0090 loss_train: 0.5637 acc_train: 0.7220 loss_val: 5.0456 acc_val: 0.5938 time: 0.0077s\n",
            "Epoch: 0091 loss_train: 0.5632 acc_train: 0.7260 loss_val: 5.1261 acc_val: 0.5813 time: 0.0064s\n",
            "Epoch: 0092 loss_train: 0.5573 acc_train: 0.7400 loss_val: 5.1754 acc_val: 0.5750 time: 0.0067s\n",
            "Epoch: 0093 loss_train: 0.5522 acc_train: 0.7360 loss_val: 5.1957 acc_val: 0.5938 time: 0.0065s\n",
            "Epoch: 0094 loss_train: 0.5549 acc_train: 0.7340 loss_val: 5.1631 acc_val: 0.6250 time: 0.0072s\n",
            "Epoch: 0095 loss_train: 0.5546 acc_train: 0.7140 loss_val: 5.1496 acc_val: 0.6687 time: 0.0078s\n",
            "Epoch: 0096 loss_train: 0.5491 acc_train: 0.6940 loss_val: 5.1916 acc_val: 0.6687 time: 0.0065s\n",
            "Epoch: 0097 loss_train: 0.5800 acc_train: 0.7120 loss_val: 5.2135 acc_val: 0.6875 time: 0.0071s\n",
            "Epoch: 0098 loss_train: 0.5528 acc_train: 0.7080 loss_val: 5.2944 acc_val: 0.6750 time: 0.0078s\n",
            "Epoch: 0099 loss_train: 0.5529 acc_train: 0.7000 loss_val: 5.4122 acc_val: 0.6375 time: 0.0070s\n",
            "Epoch: 0100 loss_train: 0.5452 acc_train: 0.7100 loss_val: 5.5492 acc_val: 0.6000 time: 0.0065s\n",
            "Epoch: 0101 loss_train: 0.5492 acc_train: 0.7220 loss_val: 5.6241 acc_val: 0.6062 time: 0.0065s\n",
            "Epoch: 0102 loss_train: 0.5530 acc_train: 0.7200 loss_val: 5.6924 acc_val: 0.6062 time: 0.0072s\n",
            "Epoch: 0103 loss_train: 0.5535 acc_train: 0.7220 loss_val: 5.7426 acc_val: 0.6188 time: 0.0074s\n",
            "Epoch: 0104 loss_train: 0.5515 acc_train: 0.7140 loss_val: 5.7814 acc_val: 0.6312 time: 0.0067s\n",
            "Epoch: 0105 loss_train: 0.5494 acc_train: 0.7120 loss_val: 5.8201 acc_val: 0.6438 time: 0.0072s\n",
            "Epoch: 0106 loss_train: 0.5428 acc_train: 0.7080 loss_val: 5.8932 acc_val: 0.6500 time: 0.0090s\n",
            "Epoch: 0107 loss_train: 0.5484 acc_train: 0.7060 loss_val: 5.9880 acc_val: 0.6562 time: 0.0074s\n",
            "Epoch: 0108 loss_train: 0.5418 acc_train: 0.7000 loss_val: 6.0792 acc_val: 0.6375 time: 0.0068s\n",
            "Epoch: 0109 loss_train: 0.5385 acc_train: 0.7020 loss_val: 6.1699 acc_val: 0.6062 time: 0.0071s\n",
            "Epoch: 0110 loss_train: 0.5372 acc_train: 0.7160 loss_val: 6.2484 acc_val: 0.6062 time: 0.0071s\n",
            "Epoch: 0111 loss_train: 0.5403 acc_train: 0.7080 loss_val: 6.2976 acc_val: 0.6188 time: 0.0064s\n",
            "Epoch: 0112 loss_train: 0.5412 acc_train: 0.7040 loss_val: 6.2951 acc_val: 0.6500 time: 0.0063s\n",
            "Epoch: 0113 loss_train: 0.5371 acc_train: 0.7020 loss_val: 6.3256 acc_val: 0.6687 time: 0.0070s\n",
            "Epoch: 0114 loss_train: 0.5395 acc_train: 0.7000 loss_val: 6.3874 acc_val: 0.6687 time: 0.0069s\n",
            "Epoch: 0115 loss_train: 0.5299 acc_train: 0.7060 loss_val: 6.4709 acc_val: 0.6687 time: 0.0074s\n",
            "Epoch: 0116 loss_train: 0.5272 acc_train: 0.7140 loss_val: 6.5686 acc_val: 0.6562 time: 0.0073s\n",
            "Epoch: 0117 loss_train: 0.5300 acc_train: 0.7100 loss_val: 6.6569 acc_val: 0.6500 time: 0.0068s\n",
            "Epoch: 0118 loss_train: 0.5377 acc_train: 0.7080 loss_val: 6.6451 acc_val: 0.6625 time: 0.0063s\n",
            "Epoch: 0119 loss_train: 0.5302 acc_train: 0.7040 loss_val: 6.6399 acc_val: 0.6875 time: 0.0064s\n",
            "Epoch: 0120 loss_train: 0.5309 acc_train: 0.6980 loss_val: 6.6695 acc_val: 0.6875 time: 0.0066s\n",
            "Epoch: 0121 loss_train: 0.5354 acc_train: 0.7000 loss_val: 6.7443 acc_val: 0.6813 time: 0.0065s\n",
            "Epoch: 0122 loss_train: 0.5276 acc_train: 0.7080 loss_val: 6.8565 acc_val: 0.6687 time: 0.0071s\n",
            "Epoch: 0123 loss_train: 0.5335 acc_train: 0.6980 loss_val: 6.9911 acc_val: 0.6375 time: 0.0062s\n",
            "Epoch: 0124 loss_train: 0.5257 acc_train: 0.7200 loss_val: 7.1101 acc_val: 0.6312 time: 0.0074s\n",
            "Epoch: 0125 loss_train: 0.5265 acc_train: 0.7240 loss_val: 7.2055 acc_val: 0.6125 time: 0.0070s\n",
            "Epoch: 0126 loss_train: 0.5214 acc_train: 0.7220 loss_val: 7.2465 acc_val: 0.6188 time: 0.0074s\n",
            "Epoch: 0127 loss_train: 0.5228 acc_train: 0.7200 loss_val: 7.1704 acc_val: 0.6937 time: 0.0080s\n",
            "Epoch: 0128 loss_train: 0.5225 acc_train: 0.7040 loss_val: 7.1306 acc_val: 0.7500 time: 0.0073s\n",
            "Epoch: 0129 loss_train: 0.5446 acc_train: 0.7320 loss_val: 7.0803 acc_val: 0.7937 time: 0.0066s\n",
            "Epoch: 0130 loss_train: 0.5326 acc_train: 0.7220 loss_val: 7.1404 acc_val: 0.7750 time: 0.0077s\n",
            "Epoch: 0131 loss_train: 0.5355 acc_train: 0.7140 loss_val: 7.2825 acc_val: 0.6813 time: 0.0073s\n",
            "Epoch: 0132 loss_train: 0.5393 acc_train: 0.6800 loss_val: 7.4662 acc_val: 0.6438 time: 0.0088s\n",
            "Epoch: 0133 loss_train: 0.5357 acc_train: 0.7240 loss_val: 7.6245 acc_val: 0.6250 time: 0.0087s\n",
            "Epoch: 0134 loss_train: 0.5278 acc_train: 0.7280 loss_val: 7.7090 acc_val: 0.6188 time: 0.0073s\n",
            "Epoch: 0135 loss_train: 0.5331 acc_train: 0.7440 loss_val: 7.7183 acc_val: 0.6250 time: 0.0068s\n",
            "Epoch: 0136 loss_train: 0.5220 acc_train: 0.7340 loss_val: 7.7000 acc_val: 0.6375 time: 0.0067s\n",
            "Epoch: 0137 loss_train: 0.5190 acc_train: 0.7220 loss_val: 7.6851 acc_val: 0.6937 time: 0.0074s\n",
            "Epoch: 0138 loss_train: 0.5232 acc_train: 0.7160 loss_val: 7.6905 acc_val: 0.7250 time: 0.0073s\n",
            "Epoch: 0139 loss_train: 0.5248 acc_train: 0.7140 loss_val: 7.7433 acc_val: 0.7375 time: 0.0065s\n",
            "Epoch: 0140 loss_train: 0.5284 acc_train: 0.7100 loss_val: 7.8710 acc_val: 0.7000 time: 0.0065s\n",
            "Epoch: 0141 loss_train: 0.5155 acc_train: 0.7100 loss_val: 8.0219 acc_val: 0.6625 time: 0.0070s\n",
            "Epoch: 0142 loss_train: 0.5308 acc_train: 0.7180 loss_val: 8.1445 acc_val: 0.6250 time: 0.0082s\n",
            "Epoch: 0143 loss_train: 0.5158 acc_train: 0.7320 loss_val: 8.1851 acc_val: 0.6125 time: 0.0083s\n",
            "Epoch: 0144 loss_train: 0.5184 acc_train: 0.7240 loss_val: 8.1276 acc_val: 0.6250 time: 0.0070s\n",
            "Epoch: 0145 loss_train: 0.5168 acc_train: 0.7040 loss_val: 8.0542 acc_val: 0.6750 time: 0.0090s\n",
            "Epoch: 0146 loss_train: 0.5118 acc_train: 0.7000 loss_val: 8.0062 acc_val: 0.7000 time: 0.0071s\n",
            "Epoch: 0147 loss_train: 0.5058 acc_train: 0.7100 loss_val: 8.0317 acc_val: 0.7063 time: 0.0080s\n",
            "Epoch: 0148 loss_train: 0.5137 acc_train: 0.7040 loss_val: 8.1078 acc_val: 0.6937 time: 0.0066s\n",
            "Epoch: 0149 loss_train: 0.5105 acc_train: 0.7140 loss_val: 8.2129 acc_val: 0.6750 time: 0.0067s\n",
            "Epoch: 0150 loss_train: 0.5132 acc_train: 0.7260 loss_val: 8.3287 acc_val: 0.6562 time: 0.0071s\n",
            "Epoch: 0151 loss_train: 0.5090 acc_train: 0.7120 loss_val: 8.4306 acc_val: 0.6500 time: 0.0064s\n",
            "Epoch: 0152 loss_train: 0.5092 acc_train: 0.7100 loss_val: 8.5139 acc_val: 0.6500 time: 0.0096s\n",
            "Epoch: 0153 loss_train: 0.5051 acc_train: 0.7200 loss_val: 8.5715 acc_val: 0.6500 time: 0.0069s\n",
            "Epoch: 0154 loss_train: 0.5061 acc_train: 0.7100 loss_val: 8.6167 acc_val: 0.6875 time: 0.0070s\n",
            "Epoch: 0155 loss_train: 0.5231 acc_train: 0.7120 loss_val: 8.6251 acc_val: 0.7438 time: 0.0074s\n",
            "Epoch: 0156 loss_train: 0.5030 acc_train: 0.7160 loss_val: 8.6621 acc_val: 0.7562 time: 0.0067s\n",
            "Epoch: 0157 loss_train: 0.5082 acc_train: 0.7280 loss_val: 8.7413 acc_val: 0.7562 time: 0.0068s\n",
            "Epoch: 0158 loss_train: 0.5071 acc_train: 0.7240 loss_val: 8.8436 acc_val: 0.7125 time: 0.0090s\n",
            "Epoch: 0159 loss_train: 0.5015 acc_train: 0.7120 loss_val: 8.9475 acc_val: 0.6562 time: 0.0072s\n",
            "Epoch: 0160 loss_train: 0.5029 acc_train: 0.7060 loss_val: 9.0391 acc_val: 0.6375 time: 0.0074s\n",
            "Epoch: 0161 loss_train: 0.5042 acc_train: 0.7260 loss_val: 9.0974 acc_val: 0.6312 time: 0.0073s\n",
            "Epoch: 0162 loss_train: 0.5038 acc_train: 0.7180 loss_val: 9.1040 acc_val: 0.6312 time: 0.0065s\n",
            "Epoch: 0163 loss_train: 0.5177 acc_train: 0.7300 loss_val: 9.0241 acc_val: 0.6750 time: 0.0075s\n",
            "Epoch: 0164 loss_train: 0.5073 acc_train: 0.6960 loss_val: 8.9821 acc_val: 0.7312 time: 0.0074s\n",
            "Epoch: 0165 loss_train: 0.5114 acc_train: 0.7280 loss_val: 8.9772 acc_val: 0.7500 time: 0.0078s\n",
            "Epoch: 0166 loss_train: 0.5022 acc_train: 0.7140 loss_val: 9.0234 acc_val: 0.7438 time: 0.0074s\n",
            "Epoch: 0167 loss_train: 0.4990 acc_train: 0.7240 loss_val: 9.1085 acc_val: 0.6875 time: 0.0075s\n",
            "Epoch: 0168 loss_train: 0.5117 acc_train: 0.7020 loss_val: 9.1905 acc_val: 0.6500 time: 0.0087s\n",
            "Epoch: 0169 loss_train: 0.5003 acc_train: 0.7240 loss_val: 9.2347 acc_val: 0.6500 time: 0.0076s\n",
            "Epoch: 0170 loss_train: 0.4967 acc_train: 0.7160 loss_val: 9.2473 acc_val: 0.6375 time: 0.0079s\n",
            "Epoch: 0171 loss_train: 0.4938 acc_train: 0.7240 loss_val: 9.2365 acc_val: 0.6500 time: 0.0081s\n",
            "Epoch: 0172 loss_train: 0.4957 acc_train: 0.7040 loss_val: 9.2277 acc_val: 0.7063 time: 0.0069s\n",
            "Epoch: 0173 loss_train: 0.4919 acc_train: 0.7220 loss_val: 9.2483 acc_val: 0.7188 time: 0.0073s\n",
            "Epoch: 0174 loss_train: 0.4945 acc_train: 0.7140 loss_val: 9.2201 acc_val: 0.7375 time: 0.0073s\n",
            "Epoch: 0175 loss_train: 0.5003 acc_train: 0.7100 loss_val: 9.2612 acc_val: 0.7188 time: 0.0077s\n",
            "Epoch: 0176 loss_train: 0.4959 acc_train: 0.7120 loss_val: 9.3560 acc_val: 0.6750 time: 0.0093s\n",
            "Epoch: 0177 loss_train: 0.4952 acc_train: 0.7020 loss_val: 9.4624 acc_val: 0.6687 time: 0.0093s\n",
            "Epoch: 0178 loss_train: 0.4936 acc_train: 0.6980 loss_val: 9.5553 acc_val: 0.6500 time: 0.0076s\n",
            "Epoch: 0179 loss_train: 0.4898 acc_train: 0.7200 loss_val: 9.6260 acc_val: 0.6500 time: 0.0076s\n",
            "Epoch: 0180 loss_train: 0.4906 acc_train: 0.7140 loss_val: 9.6869 acc_val: 0.6687 time: 0.0073s\n",
            "Epoch: 0181 loss_train: 0.4913 acc_train: 0.7140 loss_val: 9.7297 acc_val: 0.6813 time: 0.0074s\n",
            "Epoch: 0182 loss_train: 0.4853 acc_train: 0.7240 loss_val: 9.7701 acc_val: 0.7250 time: 0.0068s\n",
            "Epoch: 0183 loss_train: 0.4893 acc_train: 0.7220 loss_val: 9.8232 acc_val: 0.7312 time: 0.0083s\n",
            "Epoch: 0184 loss_train: 0.4878 acc_train: 0.7100 loss_val: 9.8880 acc_val: 0.7250 time: 0.0078s\n",
            "Epoch: 0185 loss_train: 0.4918 acc_train: 0.7080 loss_val: 9.9063 acc_val: 0.7438 time: 0.0070s\n",
            "Epoch: 0186 loss_train: 0.4889 acc_train: 0.7280 loss_val: 9.9643 acc_val: 0.7188 time: 0.0066s\n",
            "Epoch: 0187 loss_train: 0.4863 acc_train: 0.7240 loss_val: 10.0455 acc_val: 0.6750 time: 0.0062s\n",
            "Epoch: 0188 loss_train: 0.4851 acc_train: 0.7140 loss_val: 10.1268 acc_val: 0.6562 time: 0.0060s\n",
            "Epoch: 0189 loss_train: 0.4855 acc_train: 0.7100 loss_val: 10.1961 acc_val: 0.6625 time: 0.0066s\n",
            "Epoch: 0190 loss_train: 0.4896 acc_train: 0.7180 loss_val: 10.2472 acc_val: 0.6625 time: 0.0070s\n",
            "Epoch: 0191 loss_train: 0.4898 acc_train: 0.7000 loss_val: 10.2117 acc_val: 0.7312 time: 0.0074s\n",
            "Epoch: 0192 loss_train: 0.4941 acc_train: 0.7300 loss_val: 10.2063 acc_val: 0.7562 time: 0.0068s\n",
            "Epoch: 0193 loss_train: 0.4897 acc_train: 0.7280 loss_val: 10.2838 acc_val: 0.7562 time: 0.0065s\n",
            "Epoch: 0194 loss_train: 0.4903 acc_train: 0.7100 loss_val: 10.3967 acc_val: 0.7188 time: 0.0064s\n",
            "Epoch: 0195 loss_train: 0.4904 acc_train: 0.7180 loss_val: 10.5222 acc_val: 0.6687 time: 0.0064s\n",
            "Epoch: 0196 loss_train: 0.5008 acc_train: 0.7120 loss_val: 10.5314 acc_val: 0.6750 time: 0.0065s\n",
            "Epoch: 0197 loss_train: 0.4857 acc_train: 0.7180 loss_val: 10.5379 acc_val: 0.6875 time: 0.0065s\n",
            "Epoch: 0198 loss_train: 0.4851 acc_train: 0.7200 loss_val: 10.5479 acc_val: 0.7125 time: 0.0067s\n",
            "Epoch: 0199 loss_train: 0.4870 acc_train: 0.7060 loss_val: 10.5736 acc_val: 0.7125 time: 0.0070s\n",
            "Epoch: 0200 loss_train: 0.4856 acc_train: 0.7280 loss_val: 10.6379 acc_val: 0.6813 time: 0.0061s\n",
            "Epoch: 0201 loss_train: 0.4854 acc_train: 0.7200 loss_val: 10.7110 acc_val: 0.6687 time: 0.0066s\n",
            "Epoch: 0202 loss_train: 0.4773 acc_train: 0.7200 loss_val: 10.7811 acc_val: 0.6500 time: 0.0064s\n",
            "Epoch: 0203 loss_train: 0.4954 acc_train: 0.7280 loss_val: 10.7059 acc_val: 0.6937 time: 0.0074s\n",
            "Epoch: 0204 loss_train: 0.4841 acc_train: 0.7260 loss_val: 10.6451 acc_val: 0.7250 time: 0.0070s\n",
            "Epoch: 0205 loss_train: 0.4801 acc_train: 0.7180 loss_val: 10.6565 acc_val: 0.7312 time: 0.0073s\n",
            "Epoch: 0206 loss_train: 0.4812 acc_train: 0.7160 loss_val: 10.7462 acc_val: 0.7000 time: 0.0066s\n",
            "Epoch: 0207 loss_train: 0.4893 acc_train: 0.7180 loss_val: 10.7662 acc_val: 0.7000 time: 0.0065s\n",
            "Epoch: 0208 loss_train: 0.4891 acc_train: 0.7180 loss_val: 10.8547 acc_val: 0.6750 time: 0.0075s\n",
            "Epoch: 0209 loss_train: 0.4805 acc_train: 0.7120 loss_val: 10.9389 acc_val: 0.6562 time: 0.0064s\n",
            "Epoch: 0210 loss_train: 0.4872 acc_train: 0.7060 loss_val: 11.0136 acc_val: 0.6562 time: 0.0088s\n",
            "Epoch: 0211 loss_train: 0.4851 acc_train: 0.7120 loss_val: 11.0600 acc_val: 0.6750 time: 0.0079s\n",
            "Epoch: 0212 loss_train: 0.4819 acc_train: 0.7180 loss_val: 11.0997 acc_val: 0.7063 time: 0.0074s\n",
            "Epoch: 0213 loss_train: 0.5056 acc_train: 0.7200 loss_val: 11.1232 acc_val: 0.7250 time: 0.0070s\n",
            "Epoch: 0214 loss_train: 0.4841 acc_train: 0.7180 loss_val: 11.1719 acc_val: 0.7125 time: 0.0065s\n",
            "Epoch: 0215 loss_train: 0.4795 acc_train: 0.7240 loss_val: 11.2299 acc_val: 0.6937 time: 0.0070s\n",
            "Epoch: 0216 loss_train: 0.4788 acc_train: 0.7100 loss_val: 11.2859 acc_val: 0.6562 time: 0.0067s\n",
            "Epoch: 0217 loss_train: 0.4835 acc_train: 0.7180 loss_val: 11.3141 acc_val: 0.6250 time: 0.0075s\n",
            "Epoch: 0218 loss_train: 0.4794 acc_train: 0.7440 loss_val: 11.2912 acc_val: 0.6562 time: 0.0072s\n",
            "Epoch: 0219 loss_train: 0.4747 acc_train: 0.7200 loss_val: 11.2587 acc_val: 0.6813 time: 0.0071s\n",
            "Epoch: 0220 loss_train: 0.4869 acc_train: 0.7140 loss_val: 11.1366 acc_val: 0.7438 time: 0.0066s\n",
            "Epoch: 0221 loss_train: 0.4782 acc_train: 0.7240 loss_val: 11.1286 acc_val: 0.7562 time: 0.0065s\n",
            "Epoch: 0222 loss_train: 0.4750 acc_train: 0.7280 loss_val: 11.2136 acc_val: 0.7063 time: 0.0067s\n",
            "Epoch: 0223 loss_train: 0.4754 acc_train: 0.7280 loss_val: 11.3304 acc_val: 0.6500 time: 0.0064s\n",
            "Epoch: 0224 loss_train: 0.4799 acc_train: 0.7180 loss_val: 11.4417 acc_val: 0.6312 time: 0.0061s\n",
            "Epoch: 0225 loss_train: 0.4819 acc_train: 0.7260 loss_val: 11.4974 acc_val: 0.6312 time: 0.0064s\n",
            "Epoch: 0226 loss_train: 0.4804 acc_train: 0.7360 loss_val: 11.5274 acc_val: 0.6687 time: 0.0071s\n",
            "Epoch: 0227 loss_train: 0.4852 acc_train: 0.7260 loss_val: 11.5599 acc_val: 0.7125 time: 0.0063s\n",
            "Epoch: 0228 loss_train: 0.4732 acc_train: 0.7240 loss_val: 11.6456 acc_val: 0.7250 time: 0.0069s\n",
            "Epoch: 0229 loss_train: 0.4720 acc_train: 0.7260 loss_val: 11.7883 acc_val: 0.6500 time: 0.0065s\n",
            "Epoch: 0230 loss_train: 0.4786 acc_train: 0.7140 loss_val: 11.9158 acc_val: 0.5875 time: 0.0064s\n",
            "Epoch: 0231 loss_train: 0.4799 acc_train: 0.7160 loss_val: 11.8773 acc_val: 0.6438 time: 0.0071s\n",
            "Epoch: 0232 loss_train: 0.4740 acc_train: 0.7080 loss_val: 11.8390 acc_val: 0.6687 time: 0.0067s\n",
            "Epoch: 0233 loss_train: 0.4676 acc_train: 0.7300 loss_val: 11.8544 acc_val: 0.6937 time: 0.0086s\n",
            "Epoch: 0234 loss_train: 0.4712 acc_train: 0.7260 loss_val: 11.9169 acc_val: 0.6750 time: 0.0074s\n",
            "Epoch: 0235 loss_train: 0.4747 acc_train: 0.7280 loss_val: 11.9797 acc_val: 0.6562 time: 0.0075s\n",
            "Epoch: 0236 loss_train: 0.4701 acc_train: 0.7220 loss_val: 12.0339 acc_val: 0.6438 time: 0.0095s\n",
            "Epoch: 0237 loss_train: 0.4694 acc_train: 0.7300 loss_val: 12.0815 acc_val: 0.6438 time: 0.0092s\n",
            "Epoch: 0238 loss_train: 0.4697 acc_train: 0.7300 loss_val: 12.1088 acc_val: 0.6625 time: 0.0081s\n",
            "Epoch: 0239 loss_train: 0.4729 acc_train: 0.7380 loss_val: 12.1437 acc_val: 0.6687 time: 0.0079s\n",
            "Epoch: 0240 loss_train: 0.4677 acc_train: 0.7320 loss_val: 12.1592 acc_val: 0.6813 time: 0.0074s\n",
            "Epoch: 0241 loss_train: 0.4665 acc_train: 0.7220 loss_val: 12.1918 acc_val: 0.6750 time: 0.0078s\n",
            "Epoch: 0242 loss_train: 0.4700 acc_train: 0.7280 loss_val: 12.1560 acc_val: 0.6813 time: 0.0076s\n",
            "Epoch: 0243 loss_train: 0.4663 acc_train: 0.7260 loss_val: 12.1932 acc_val: 0.6750 time: 0.0078s\n",
            "Epoch: 0244 loss_train: 0.4633 acc_train: 0.7280 loss_val: 12.2583 acc_val: 0.6625 time: 0.0074s\n",
            "Epoch: 0245 loss_train: 0.4735 acc_train: 0.7260 loss_val: 12.3360 acc_val: 0.6500 time: 0.0080s\n",
            "Epoch: 0246 loss_train: 0.4659 acc_train: 0.7260 loss_val: 12.4103 acc_val: 0.6250 time: 0.0074s\n",
            "Epoch: 0247 loss_train: 0.4715 acc_train: 0.7220 loss_val: 12.4941 acc_val: 0.6250 time: 0.0074s\n",
            "Epoch: 0248 loss_train: 0.4619 acc_train: 0.7200 loss_val: 12.5546 acc_val: 0.6375 time: 0.0077s\n",
            "Epoch: 0249 loss_train: 0.4640 acc_train: 0.7200 loss_val: 12.5923 acc_val: 0.6750 time: 0.0074s\n",
            "Epoch: 0250 loss_train: 0.4789 acc_train: 0.7260 loss_val: 12.6135 acc_val: 0.6875 time: 0.0069s\n",
            "Epoch: 0251 loss_train: 0.4642 acc_train: 0.7260 loss_val: 12.6514 acc_val: 0.6937 time: 0.0084s\n",
            "Epoch: 0252 loss_train: 0.4644 acc_train: 0.7240 loss_val: 12.6361 acc_val: 0.7188 time: 0.0069s\n",
            "Epoch: 0253 loss_train: 0.4669 acc_train: 0.7420 loss_val: 12.6873 acc_val: 0.7000 time: 0.0083s\n",
            "Epoch: 0254 loss_train: 0.4650 acc_train: 0.7440 loss_val: 12.7971 acc_val: 0.6250 time: 0.0087s\n",
            "Epoch: 0255 loss_train: 0.4643 acc_train: 0.7340 loss_val: 12.8817 acc_val: 0.6062 time: 0.0088s\n",
            "Epoch: 0256 loss_train: 0.4743 acc_train: 0.7220 loss_val: 12.9105 acc_val: 0.6188 time: 0.0084s\n",
            "Epoch: 0257 loss_train: 0.4612 acc_train: 0.7320 loss_val: 12.8944 acc_val: 0.6750 time: 0.0069s\n",
            "Epoch: 0258 loss_train: 0.4650 acc_train: 0.7360 loss_val: 12.9310 acc_val: 0.7063 time: 0.0066s\n",
            "Epoch: 0259 loss_train: 0.4648 acc_train: 0.7320 loss_val: 13.0396 acc_val: 0.6937 time: 0.0068s\n",
            "Epoch: 0260 loss_train: 0.5332 acc_train: 0.7400 loss_val: 13.0024 acc_val: 0.7000 time: 0.0068s\n",
            "Epoch: 0261 loss_train: 0.4615 acc_train: 0.7400 loss_val: 13.0563 acc_val: 0.6937 time: 0.0097s\n",
            "Epoch: 0262 loss_train: 0.4632 acc_train: 0.7360 loss_val: 13.1597 acc_val: 0.6312 time: 0.0074s\n",
            "Epoch: 0263 loss_train: 0.4657 acc_train: 0.7180 loss_val: 13.2493 acc_val: 0.6125 time: 0.0079s\n",
            "Epoch: 0264 loss_train: 0.4580 acc_train: 0.7240 loss_val: 13.2842 acc_val: 0.6250 time: 0.0075s\n",
            "Epoch: 0265 loss_train: 0.4672 acc_train: 0.7260 loss_val: 13.2872 acc_val: 0.6625 time: 0.0078s\n",
            "Epoch: 0266 loss_train: 0.4573 acc_train: 0.7480 loss_val: 13.2781 acc_val: 0.7000 time: 0.0073s\n",
            "Epoch: 0267 loss_train: 0.4617 acc_train: 0.7380 loss_val: 13.3293 acc_val: 0.7063 time: 0.0076s\n",
            "Epoch: 0268 loss_train: 0.4582 acc_train: 0.7320 loss_val: 13.4102 acc_val: 0.6687 time: 0.0082s\n",
            "Epoch: 0269 loss_train: 0.4602 acc_train: 0.7400 loss_val: 13.4426 acc_val: 0.6375 time: 0.0079s\n",
            "Epoch: 0270 loss_train: 0.4610 acc_train: 0.7340 loss_val: 13.4422 acc_val: 0.6500 time: 0.0081s\n",
            "Epoch: 0271 loss_train: 0.4554 acc_train: 0.7260 loss_val: 13.4375 acc_val: 0.6750 time: 0.0076s\n",
            "Epoch: 0272 loss_train: 0.4536 acc_train: 0.7320 loss_val: 13.3780 acc_val: 0.7250 time: 0.0080s\n",
            "Epoch: 0273 loss_train: 0.4584 acc_train: 0.7360 loss_val: 13.4168 acc_val: 0.7000 time: 0.0078s\n",
            "Epoch: 0274 loss_train: 0.4576 acc_train: 0.7460 loss_val: 13.5195 acc_val: 0.6687 time: 0.0062s\n",
            "Epoch: 0275 loss_train: 0.4681 acc_train: 0.7220 loss_val: 13.6354 acc_val: 0.6250 time: 0.0070s\n",
            "Epoch: 0276 loss_train: 0.4567 acc_train: 0.7240 loss_val: 13.7456 acc_val: 0.6062 time: 0.0071s\n",
            "Epoch: 0277 loss_train: 0.4560 acc_train: 0.7360 loss_val: 13.7936 acc_val: 0.6000 time: 0.0064s\n",
            "Epoch: 0278 loss_train: 0.4605 acc_train: 0.7360 loss_val: 13.7787 acc_val: 0.6813 time: 0.0065s\n",
            "Epoch: 0279 loss_train: 0.4511 acc_train: 0.7340 loss_val: 13.7794 acc_val: 0.7125 time: 0.0069s\n",
            "Epoch: 0280 loss_train: 0.4530 acc_train: 0.7400 loss_val: 13.8329 acc_val: 0.7188 time: 0.0066s\n",
            "Epoch: 0281 loss_train: 0.4501 acc_train: 0.7480 loss_val: 13.9278 acc_val: 0.7188 time: 0.0072s\n",
            "Epoch: 0282 loss_train: 0.4524 acc_train: 0.7460 loss_val: 13.9817 acc_val: 0.6813 time: 0.0071s\n",
            "Epoch: 0283 loss_train: 0.4654 acc_train: 0.7400 loss_val: 13.9136 acc_val: 0.6875 time: 0.0070s\n",
            "Epoch: 0284 loss_train: 0.4567 acc_train: 0.7300 loss_val: 13.8824 acc_val: 0.6625 time: 0.0081s\n",
            "Epoch: 0285 loss_train: 0.4599 acc_train: 0.7200 loss_val: 13.8909 acc_val: 0.6500 time: 0.0073s\n",
            "Epoch: 0286 loss_train: 0.4626 acc_train: 0.7180 loss_val: 13.9121 acc_val: 0.6562 time: 0.0112s\n",
            "Epoch: 0287 loss_train: 0.4638 acc_train: 0.7260 loss_val: 13.9886 acc_val: 0.6250 time: 0.0076s\n",
            "Epoch: 0288 loss_train: 0.4557 acc_train: 0.7260 loss_val: 14.0995 acc_val: 0.6188 time: 0.0075s\n",
            "Epoch: 0289 loss_train: 0.4671 acc_train: 0.7300 loss_val: 14.1746 acc_val: 0.6438 time: 0.0073s\n",
            "Epoch: 0290 loss_train: 0.4560 acc_train: 0.7480 loss_val: 14.2262 acc_val: 0.6875 time: 0.0069s\n",
            "Epoch: 0291 loss_train: 0.4602 acc_train: 0.7300 loss_val: 14.3019 acc_val: 0.6875 time: 0.0079s\n",
            "Epoch: 0292 loss_train: 0.4613 acc_train: 0.7060 loss_val: 14.4239 acc_val: 0.6813 time: 0.0071s\n",
            "Epoch: 0293 loss_train: 0.4618 acc_train: 0.7220 loss_val: 14.5579 acc_val: 0.6562 time: 0.0069s\n",
            "Epoch: 0294 loss_train: 0.4579 acc_train: 0.7320 loss_val: 14.6336 acc_val: 0.5813 time: 0.0072s\n",
            "Epoch: 0295 loss_train: 0.4644 acc_train: 0.7320 loss_val: 14.6184 acc_val: 0.5813 time: 0.0071s\n",
            "Epoch: 0296 loss_train: 0.4624 acc_train: 0.7180 loss_val: 14.5177 acc_val: 0.6438 time: 0.0065s\n",
            "Epoch: 0297 loss_train: 0.4509 acc_train: 0.7220 loss_val: 14.4259 acc_val: 0.7125 time: 0.0072s\n",
            "Epoch: 0298 loss_train: 0.4463 acc_train: 0.7360 loss_val: 14.4659 acc_val: 0.7188 time: 0.0066s\n",
            "Epoch: 0299 loss_train: 0.4509 acc_train: 0.7300 loss_val: 14.5354 acc_val: 0.7063 time: 0.0065s\n",
            "Epoch: 0300 loss_train: 0.5172 acc_train: 0.7340 loss_val: 14.4896 acc_val: 0.7375 time: 0.0070s\n",
            "Epoch: 0301 loss_train: 0.4581 acc_train: 0.7580 loss_val: 14.5390 acc_val: 0.7000 time: 0.0077s\n",
            "Epoch: 0302 loss_train: 0.4531 acc_train: 0.7460 loss_val: 14.6447 acc_val: 0.6562 time: 0.0097s\n",
            "Epoch: 0303 loss_train: 0.4559 acc_train: 0.7320 loss_val: 14.7395 acc_val: 0.6438 time: 0.0079s\n",
            "Epoch: 0304 loss_train: 0.4733 acc_train: 0.7380 loss_val: 14.7804 acc_val: 0.6562 time: 0.0067s\n",
            "Epoch: 0305 loss_train: 0.4591 acc_train: 0.7200 loss_val: 14.8370 acc_val: 0.6813 time: 0.0066s\n",
            "Epoch: 0306 loss_train: 0.4578 acc_train: 0.7340 loss_val: 14.9042 acc_val: 0.6875 time: 0.0069s\n",
            "Epoch: 0307 loss_train: 0.4555 acc_train: 0.7420 loss_val: 15.0127 acc_val: 0.6625 time: 0.0060s\n",
            "Epoch: 0308 loss_train: 0.4565 acc_train: 0.7380 loss_val: 15.1227 acc_val: 0.6500 time: 0.0062s\n",
            "Epoch: 0309 loss_train: 0.4502 acc_train: 0.7340 loss_val: 15.1993 acc_val: 0.6188 time: 0.0062s\n",
            "Epoch: 0310 loss_train: 0.4488 acc_train: 0.7460 loss_val: 15.1852 acc_val: 0.6500 time: 0.0070s\n",
            "Epoch: 0311 loss_train: 0.4506 acc_train: 0.7460 loss_val: 15.1055 acc_val: 0.6687 time: 0.0070s\n",
            "Epoch: 0312 loss_train: 0.4468 acc_train: 0.7340 loss_val: 14.9921 acc_val: 0.7000 time: 0.0090s\n",
            "Epoch: 0313 loss_train: 0.4454 acc_train: 0.7400 loss_val: 14.9910 acc_val: 0.6875 time: 0.0069s\n",
            "Epoch: 0314 loss_train: 0.4639 acc_train: 0.7360 loss_val: 15.0335 acc_val: 0.6937 time: 0.0065s\n",
            "Epoch: 0315 loss_train: 0.4415 acc_train: 0.7260 loss_val: 15.1443 acc_val: 0.6562 time: 0.0065s\n",
            "Epoch: 0316 loss_train: 0.4448 acc_train: 0.7340 loss_val: 15.2319 acc_val: 0.6375 time: 0.0063s\n",
            "Epoch: 0317 loss_train: 0.4550 acc_train: 0.7340 loss_val: 15.2624 acc_val: 0.6813 time: 0.0065s\n",
            "Epoch: 0318 loss_train: 0.4457 acc_train: 0.7420 loss_val: 15.3005 acc_val: 0.7188 time: 0.0071s\n",
            "Epoch: 0319 loss_train: 0.4479 acc_train: 0.7420 loss_val: 15.4026 acc_val: 0.7375 time: 0.0065s\n",
            "Epoch: 0320 loss_train: 0.4523 acc_train: 0.7500 loss_val: 15.5548 acc_val: 0.7188 time: 0.0063s\n",
            "Epoch: 0321 loss_train: 0.4486 acc_train: 0.7480 loss_val: 15.6894 acc_val: 0.6937 time: 0.0065s\n",
            "Epoch: 0322 loss_train: 0.4544 acc_train: 0.7500 loss_val: 15.6310 acc_val: 0.7063 time: 0.0064s\n",
            "Epoch: 0323 loss_train: 0.4533 acc_train: 0.7400 loss_val: 15.5561 acc_val: 0.7250 time: 0.0071s\n",
            "Epoch: 0324 loss_train: 0.4442 acc_train: 0.7500 loss_val: 15.5169 acc_val: 0.7188 time: 0.0070s\n",
            "Epoch: 0325 loss_train: 0.4468 acc_train: 0.7460 loss_val: 15.5122 acc_val: 0.6937 time: 0.0061s\n",
            "Epoch: 0326 loss_train: 0.4489 acc_train: 0.7360 loss_val: 15.5257 acc_val: 0.6625 time: 0.0073s\n",
            "Epoch: 0327 loss_train: 0.4443 acc_train: 0.7440 loss_val: 15.5274 acc_val: 0.6500 time: 0.0065s\n",
            "Epoch: 0328 loss_train: 0.5022 acc_train: 0.7380 loss_val: 15.4383 acc_val: 0.7000 time: 0.0069s\n",
            "Epoch: 0329 loss_train: 0.4490 acc_train: 0.7260 loss_val: 15.4470 acc_val: 0.7000 time: 0.0066s\n",
            "Epoch: 0330 loss_train: 0.4461 acc_train: 0.7360 loss_val: 15.5534 acc_val: 0.6750 time: 0.0068s\n",
            "Epoch: 0331 loss_train: 0.4535 acc_train: 0.7260 loss_val: 15.6570 acc_val: 0.6375 time: 0.0064s\n",
            "Epoch: 0332 loss_train: 0.4448 acc_train: 0.7340 loss_val: 15.7429 acc_val: 0.6375 time: 0.0069s\n",
            "Epoch: 0333 loss_train: 0.4435 acc_train: 0.7460 loss_val: 15.7832 acc_val: 0.6875 time: 0.0068s\n",
            "Epoch: 0334 loss_train: 0.4542 acc_train: 0.7320 loss_val: 15.8075 acc_val: 0.7063 time: 0.0072s\n",
            "Epoch: 0335 loss_train: 0.4468 acc_train: 0.7420 loss_val: 15.8208 acc_val: 0.7063 time: 0.0065s\n",
            "Epoch: 0336 loss_train: 0.4436 acc_train: 0.7440 loss_val: 15.8772 acc_val: 0.6937 time: 0.0068s\n",
            "Epoch: 0337 loss_train: 0.4552 acc_train: 0.7460 loss_val: 15.9060 acc_val: 0.6687 time: 0.0067s\n",
            "Epoch: 0338 loss_train: 0.4403 acc_train: 0.7400 loss_val: 15.9025 acc_val: 0.6562 time: 0.0064s\n",
            "Epoch: 0339 loss_train: 0.5188 acc_train: 0.7400 loss_val: 15.7732 acc_val: 0.7063 time: 0.0065s\n",
            "Epoch: 0340 loss_train: 0.4485 acc_train: 0.7500 loss_val: 15.7509 acc_val: 0.7250 time: 0.0092s\n",
            "Epoch: 0341 loss_train: 0.4656 acc_train: 0.7380 loss_val: 15.8740 acc_val: 0.6750 time: 0.0074s\n",
            "Epoch: 0342 loss_train: 0.4478 acc_train: 0.7420 loss_val: 15.9981 acc_val: 0.6312 time: 0.0066s\n",
            "Epoch: 0343 loss_train: 0.4500 acc_train: 0.7420 loss_val: 16.0658 acc_val: 0.6562 time: 0.0061s\n",
            "Epoch: 0344 loss_train: 0.4477 acc_train: 0.7440 loss_val: 16.0939 acc_val: 0.7000 time: 0.0070s\n",
            "Epoch: 0345 loss_train: 0.4451 acc_train: 0.7600 loss_val: 16.1421 acc_val: 0.7312 time: 0.0066s\n",
            "Epoch: 0346 loss_train: 0.4402 acc_train: 0.7540 loss_val: 16.2396 acc_val: 0.7312 time: 0.0065s\n",
            "Epoch: 0347 loss_train: 0.4375 acc_train: 0.7560 loss_val: 16.3995 acc_val: 0.7000 time: 0.0066s\n",
            "Epoch: 0348 loss_train: 0.4418 acc_train: 0.7480 loss_val: 16.4821 acc_val: 0.6438 time: 0.0063s\n",
            "Epoch: 0349 loss_train: 0.4690 acc_train: 0.7500 loss_val: 16.3708 acc_val: 0.7000 time: 0.0066s\n",
            "Epoch: 0350 loss_train: 0.4398 acc_train: 0.7360 loss_val: 16.2709 acc_val: 0.7188 time: 0.0073s\n",
            "Epoch: 0351 loss_train: 0.4447 acc_train: 0.7500 loss_val: 16.2334 acc_val: 0.7125 time: 0.0072s\n",
            "Epoch: 0352 loss_train: 0.4399 acc_train: 0.7580 loss_val: 16.3154 acc_val: 0.6750 time: 0.0066s\n",
            "Epoch: 0353 loss_train: 0.4376 acc_train: 0.7380 loss_val: 16.4216 acc_val: 0.6438 time: 0.0079s\n",
            "Epoch: 0354 loss_train: 0.4372 acc_train: 0.7460 loss_val: 16.4956 acc_val: 0.6500 time: 0.0072s\n",
            "Epoch: 0355 loss_train: 0.4383 acc_train: 0.7600 loss_val: 16.5451 acc_val: 0.7000 time: 0.0072s\n",
            "Epoch: 0356 loss_train: 0.4366 acc_train: 0.7500 loss_val: 16.5975 acc_val: 0.7188 time: 0.0079s\n",
            "Epoch: 0357 loss_train: 0.4415 acc_train: 0.7480 loss_val: 16.6995 acc_val: 0.7063 time: 0.0074s\n",
            "Epoch: 0358 loss_train: 0.4383 acc_train: 0.7460 loss_val: 16.8663 acc_val: 0.6937 time: 0.0075s\n",
            "Epoch: 0359 loss_train: 0.4384 acc_train: 0.7440 loss_val: 16.9803 acc_val: 0.6562 time: 0.0077s\n",
            "Epoch: 0360 loss_train: 0.4346 acc_train: 0.7540 loss_val: 17.0230 acc_val: 0.6500 time: 0.0081s\n",
            "Epoch: 0361 loss_train: 0.4499 acc_train: 0.7400 loss_val: 16.8590 acc_val: 0.7250 time: 0.0074s\n",
            "Epoch: 0362 loss_train: 0.4300 acc_train: 0.7640 loss_val: 16.7993 acc_val: 0.7500 time: 0.0078s\n",
            "Epoch: 0363 loss_train: 0.4296 acc_train: 0.7640 loss_val: 16.8523 acc_val: 0.7312 time: 0.0078s\n",
            "Epoch: 0364 loss_train: 0.4356 acc_train: 0.7560 loss_val: 16.9683 acc_val: 0.6750 time: 0.0077s\n",
            "Epoch: 0365 loss_train: 0.4315 acc_train: 0.7540 loss_val: 17.0875 acc_val: 0.6312 time: 0.0077s\n",
            "Epoch: 0366 loss_train: 0.4326 acc_train: 0.7660 loss_val: 17.1251 acc_val: 0.6500 time: 0.0096s\n",
            "Epoch: 0367 loss_train: 0.4441 acc_train: 0.7640 loss_val: 17.1765 acc_val: 0.7125 time: 0.0077s\n",
            "Epoch: 0368 loss_train: 0.4325 acc_train: 0.7520 loss_val: 17.2626 acc_val: 0.7250 time: 0.0075s\n",
            "Epoch: 0369 loss_train: 0.4373 acc_train: 0.7540 loss_val: 17.4248 acc_val: 0.7000 time: 0.0080s\n",
            "Epoch: 0370 loss_train: 0.4316 acc_train: 0.7520 loss_val: 17.5846 acc_val: 0.6750 time: 0.0069s\n",
            "Epoch: 0371 loss_train: 0.4283 acc_train: 0.7420 loss_val: 17.7479 acc_val: 0.6250 time: 0.0070s\n",
            "Epoch: 0372 loss_train: 0.4447 acc_train: 0.7380 loss_val: 17.7947 acc_val: 0.6250 time: 0.0075s\n",
            "Epoch: 0373 loss_train: 0.4332 acc_train: 0.7400 loss_val: 17.6770 acc_val: 0.6937 time: 0.0071s\n",
            "Epoch: 0374 loss_train: 0.4389 acc_train: 0.7340 loss_val: 17.4915 acc_val: 0.7188 time: 0.0077s\n",
            "Epoch: 0375 loss_train: 0.4304 acc_train: 0.7620 loss_val: 17.4490 acc_val: 0.7250 time: 0.0078s\n",
            "Epoch: 0376 loss_train: 0.4289 acc_train: 0.7680 loss_val: 17.4715 acc_val: 0.7063 time: 0.0075s\n",
            "Epoch: 0377 loss_train: 0.4310 acc_train: 0.7620 loss_val: 17.5774 acc_val: 0.6562 time: 0.0078s\n",
            "Epoch: 0378 loss_train: 0.4318 acc_train: 0.7620 loss_val: 17.6592 acc_val: 0.6562 time: 0.0082s\n",
            "Epoch: 0379 loss_train: 0.4287 acc_train: 0.7600 loss_val: 17.7060 acc_val: 0.6813 time: 0.0083s\n",
            "Epoch: 0380 loss_train: 0.4276 acc_train: 0.7760 loss_val: 17.7555 acc_val: 0.7250 time: 0.0087s\n",
            "Epoch: 0381 loss_train: 0.4276 acc_train: 0.7660 loss_val: 17.8602 acc_val: 0.7188 time: 0.0072s\n",
            "Epoch: 0382 loss_train: 0.4278 acc_train: 0.7720 loss_val: 18.0529 acc_val: 0.7250 time: 0.0067s\n",
            "Epoch: 0383 loss_train: 0.4267 acc_train: 0.7680 loss_val: 18.2359 acc_val: 0.7125 time: 0.0073s\n",
            "Epoch: 0384 loss_train: 0.4403 acc_train: 0.7620 loss_val: 18.2782 acc_val: 0.7063 time: 0.0067s\n",
            "Epoch: 0385 loss_train: 0.4756 acc_train: 0.7560 loss_val: 18.1125 acc_val: 0.7438 time: 0.0083s\n",
            "Epoch: 0386 loss_train: 0.4270 acc_train: 0.7680 loss_val: 18.0731 acc_val: 0.7438 time: 0.0066s\n",
            "Epoch: 0387 loss_train: 0.4284 acc_train: 0.7680 loss_val: 18.1280 acc_val: 0.7188 time: 0.0065s\n",
            "Epoch: 0388 loss_train: 0.4305 acc_train: 0.7660 loss_val: 18.2407 acc_val: 0.6250 time: 0.0065s\n",
            "Epoch: 0389 loss_train: 0.4342 acc_train: 0.7560 loss_val: 18.3110 acc_val: 0.6250 time: 0.0072s\n",
            "Epoch: 0390 loss_train: 0.4307 acc_train: 0.7500 loss_val: 18.2565 acc_val: 0.6813 time: 0.0068s\n",
            "Epoch: 0391 loss_train: 0.4264 acc_train: 0.7600 loss_val: 18.2676 acc_val: 0.7250 time: 0.0095s\n",
            "Epoch: 0392 loss_train: 0.4258 acc_train: 0.7580 loss_val: 18.3761 acc_val: 0.7188 time: 0.0078s\n",
            "Epoch: 0393 loss_train: 0.4274 acc_train: 0.7600 loss_val: 18.5471 acc_val: 0.6750 time: 0.0071s\n",
            "Epoch: 0394 loss_train: 0.4266 acc_train: 0.7500 loss_val: 18.7172 acc_val: 0.6375 time: 0.0066s\n",
            "Epoch: 0395 loss_train: 0.4362 acc_train: 0.7420 loss_val: 18.7609 acc_val: 0.6312 time: 0.0080s\n",
            "Epoch: 0396 loss_train: 0.4951 acc_train: 0.7460 loss_val: 18.4584 acc_val: 0.7375 time: 0.0080s\n",
            "Epoch: 0397 loss_train: 0.4412 acc_train: 0.7660 loss_val: 18.2932 acc_val: 0.7438 time: 0.0075s\n",
            "Epoch: 0398 loss_train: 0.4256 acc_train: 0.7780 loss_val: 18.3378 acc_val: 0.7312 time: 0.0074s\n",
            "Epoch: 0399 loss_train: 0.4266 acc_train: 0.7640 loss_val: 18.4906 acc_val: 0.6500 time: 0.0070s\n",
            "Epoch: 0400 loss_train: 0.4255 acc_train: 0.7760 loss_val: 18.5803 acc_val: 0.6125 time: 0.0067s\n",
            "Epoch: 0401 loss_train: 0.4358 acc_train: 0.7580 loss_val: 18.5807 acc_val: 0.6687 time: 0.0074s\n",
            "Epoch: 0402 loss_train: 0.4301 acc_train: 0.7680 loss_val: 18.5834 acc_val: 0.7375 time: 0.0072s\n",
            "Epoch: 0403 loss_train: 0.4257 acc_train: 0.7660 loss_val: 18.6728 acc_val: 0.7312 time: 0.0067s\n",
            "Epoch: 0404 loss_train: 0.4259 acc_train: 0.7640 loss_val: 18.8651 acc_val: 0.7000 time: 0.0070s\n",
            "Epoch: 0405 loss_train: 0.4216 acc_train: 0.7640 loss_val: 19.0230 acc_val: 0.6750 time: 0.0069s\n",
            "Epoch: 0406 loss_train: 0.4277 acc_train: 0.7480 loss_val: 19.0716 acc_val: 0.6562 time: 0.0067s\n",
            "Epoch: 0407 loss_train: 0.4260 acc_train: 0.7560 loss_val: 19.0220 acc_val: 0.6937 time: 0.0068s\n",
            "Epoch: 0408 loss_train: 0.4247 acc_train: 0.7420 loss_val: 18.9282 acc_val: 0.7063 time: 0.0076s\n",
            "Epoch: 0409 loss_train: 0.4262 acc_train: 0.7600 loss_val: 18.9961 acc_val: 0.7063 time: 0.0073s\n",
            "Epoch: 0410 loss_train: 0.4248 acc_train: 0.7680 loss_val: 19.0990 acc_val: 0.7000 time: 0.0079s\n",
            "Epoch: 0411 loss_train: 0.4226 acc_train: 0.7640 loss_val: 19.1964 acc_val: 0.6750 time: 0.0077s\n",
            "Epoch: 0412 loss_train: 0.4231 acc_train: 0.7680 loss_val: 19.2337 acc_val: 0.6937 time: 0.0082s\n",
            "Epoch: 0413 loss_train: 0.4255 acc_train: 0.7680 loss_val: 19.2426 acc_val: 0.7188 time: 0.0071s\n",
            "Epoch: 0414 loss_train: 0.4215 acc_train: 0.7700 loss_val: 19.3082 acc_val: 0.7188 time: 0.0073s\n",
            "Epoch: 0415 loss_train: 0.4269 acc_train: 0.7660 loss_val: 19.4155 acc_val: 0.7000 time: 0.0073s\n",
            "Epoch: 0416 loss_train: 0.4193 acc_train: 0.7740 loss_val: 19.4962 acc_val: 0.6937 time: 0.0073s\n",
            "Epoch: 0417 loss_train: 0.4211 acc_train: 0.7760 loss_val: 19.5955 acc_val: 0.7125 time: 0.0098s\n",
            "Epoch: 0418 loss_train: 0.4172 acc_train: 0.7760 loss_val: 19.6427 acc_val: 0.7312 time: 0.0073s\n",
            "Epoch: 0419 loss_train: 0.4229 acc_train: 0.7780 loss_val: 19.6291 acc_val: 0.7375 time: 0.0074s\n",
            "Epoch: 0420 loss_train: 0.4283 acc_train: 0.7760 loss_val: 19.7010 acc_val: 0.7312 time: 0.0070s\n",
            "Epoch: 0421 loss_train: 0.4218 acc_train: 0.7760 loss_val: 19.8093 acc_val: 0.6813 time: 0.0061s\n",
            "Epoch: 0422 loss_train: 0.4279 acc_train: 0.7700 loss_val: 19.8095 acc_val: 0.6750 time: 0.0075s\n",
            "Epoch: 0423 loss_train: 0.4226 acc_train: 0.7740 loss_val: 19.7451 acc_val: 0.7188 time: 0.0079s\n",
            "Epoch: 0424 loss_train: 0.4164 acc_train: 0.7800 loss_val: 19.6869 acc_val: 0.7375 time: 0.0080s\n",
            "Epoch: 0425 loss_train: 0.4265 acc_train: 0.7780 loss_val: 19.6919 acc_val: 0.7250 time: 0.0076s\n",
            "Epoch: 0426 loss_train: 0.4133 acc_train: 0.7800 loss_val: 19.7921 acc_val: 0.7125 time: 0.0077s\n",
            "Epoch: 0427 loss_train: 0.4137 acc_train: 0.7860 loss_val: 19.9393 acc_val: 0.6750 time: 0.0078s\n",
            "Epoch: 0428 loss_train: 0.4140 acc_train: 0.7660 loss_val: 20.0127 acc_val: 0.6562 time: 0.0080s\n",
            "Epoch: 0429 loss_train: 0.4178 acc_train: 0.7700 loss_val: 19.9287 acc_val: 0.7188 time: 0.0095s\n",
            "Epoch: 0430 loss_train: 0.4114 acc_train: 0.7720 loss_val: 19.9089 acc_val: 0.7375 time: 0.0094s\n",
            "Epoch: 0431 loss_train: 0.4158 acc_train: 0.7820 loss_val: 19.9963 acc_val: 0.7375 time: 0.0087s\n",
            "Epoch: 0432 loss_train: 0.4152 acc_train: 0.7720 loss_val: 20.1712 acc_val: 0.6937 time: 0.0080s\n",
            "Epoch: 0433 loss_train: 0.4140 acc_train: 0.7780 loss_val: 20.2943 acc_val: 0.6687 time: 0.0080s\n",
            "Epoch: 0434 loss_train: 0.4169 acc_train: 0.7600 loss_val: 20.3347 acc_val: 0.6875 time: 0.0080s\n",
            "Epoch: 0435 loss_train: 0.4146 acc_train: 0.7680 loss_val: 20.3114 acc_val: 0.7312 time: 0.0075s\n",
            "Epoch: 0436 loss_train: 0.4148 acc_train: 0.7820 loss_val: 20.2023 acc_val: 0.7438 time: 0.0073s\n",
            "Epoch: 0437 loss_train: 0.4278 acc_train: 0.7700 loss_val: 20.2613 acc_val: 0.7375 time: 0.0070s\n",
            "Epoch: 0438 loss_train: 0.4288 acc_train: 0.7800 loss_val: 20.3341 acc_val: 0.7125 time: 0.0076s\n",
            "Epoch: 0439 loss_train: 0.4169 acc_train: 0.7800 loss_val: 20.4164 acc_val: 0.6750 time: 0.0077s\n",
            "Epoch: 0440 loss_train: 0.4145 acc_train: 0.7660 loss_val: 20.4802 acc_val: 0.6750 time: 0.0073s\n",
            "Epoch: 0441 loss_train: 0.4156 acc_train: 0.7760 loss_val: 20.5069 acc_val: 0.7000 time: 0.0096s\n",
            "Epoch: 0442 loss_train: 0.4157 acc_train: 0.7680 loss_val: 20.5207 acc_val: 0.7250 time: 0.0078s\n",
            "Epoch: 0443 loss_train: 0.4148 acc_train: 0.7760 loss_val: 20.5970 acc_val: 0.7125 time: 0.0064s\n",
            "Epoch: 0444 loss_train: 0.4143 acc_train: 0.7760 loss_val: 20.6777 acc_val: 0.6937 time: 0.0064s\n",
            "Epoch: 0445 loss_train: 0.4120 acc_train: 0.7640 loss_val: 20.7665 acc_val: 0.6875 time: 0.0070s\n",
            "Epoch: 0446 loss_train: 0.4122 acc_train: 0.7620 loss_val: 20.7864 acc_val: 0.6937 time: 0.0072s\n",
            "Epoch: 0447 loss_train: 0.4124 acc_train: 0.7700 loss_val: 20.7912 acc_val: 0.7125 time: 0.0072s\n",
            "Epoch: 0448 loss_train: 0.4143 acc_train: 0.7840 loss_val: 20.7872 acc_val: 0.7125 time: 0.0063s\n",
            "Epoch: 0449 loss_train: 0.4105 acc_train: 0.7660 loss_val: 20.8113 acc_val: 0.7188 time: 0.0066s\n",
            "Epoch: 0450 loss_train: 0.4099 acc_train: 0.7840 loss_val: 20.9563 acc_val: 0.7000 time: 0.0073s\n",
            "Epoch: 0451 loss_train: 0.4138 acc_train: 0.7640 loss_val: 21.0795 acc_val: 0.6875 time: 0.0067s\n",
            "Epoch: 0452 loss_train: 0.4177 acc_train: 0.7620 loss_val: 21.0675 acc_val: 0.7063 time: 0.0063s\n",
            "Epoch: 0453 loss_train: 0.4092 acc_train: 0.7840 loss_val: 21.0579 acc_val: 0.7312 time: 0.0063s\n",
            "Epoch: 0454 loss_train: 0.4411 acc_train: 0.7880 loss_val: 21.0027 acc_val: 0.7562 time: 0.0065s\n",
            "Epoch: 0455 loss_train: 0.4126 acc_train: 0.7800 loss_val: 21.0688 acc_val: 0.7500 time: 0.0071s\n",
            "Epoch: 0456 loss_train: 0.4157 acc_train: 0.7780 loss_val: 21.1932 acc_val: 0.7063 time: 0.0062s\n",
            "Epoch: 0457 loss_train: 0.4131 acc_train: 0.7740 loss_val: 21.3085 acc_val: 0.6687 time: 0.0071s\n",
            "Epoch: 0458 loss_train: 0.4292 acc_train: 0.7660 loss_val: 21.3419 acc_val: 0.6625 time: 0.0072s\n",
            "Epoch: 0459 loss_train: 0.4185 acc_train: 0.7720 loss_val: 21.3268 acc_val: 0.7250 time: 0.0076s\n",
            "Epoch: 0460 loss_train: 0.4117 acc_train: 0.7720 loss_val: 21.2812 acc_val: 0.7188 time: 0.0064s\n",
            "Epoch: 0461 loss_train: 0.4125 acc_train: 0.7780 loss_val: 21.3290 acc_val: 0.7063 time: 0.0069s\n",
            "Epoch: 0462 loss_train: 0.4083 acc_train: 0.7820 loss_val: 21.4083 acc_val: 0.6687 time: 0.0067s\n",
            "Epoch: 0463 loss_train: 0.4254 acc_train: 0.7620 loss_val: 21.3051 acc_val: 0.7000 time: 0.0074s\n",
            "Epoch: 0464 loss_train: 0.4121 acc_train: 0.7760 loss_val: 21.3289 acc_val: 0.6937 time: 0.0064s\n",
            "Epoch: 0465 loss_train: 0.4179 acc_train: 0.7680 loss_val: 21.3679 acc_val: 0.6813 time: 0.0072s\n",
            "Epoch: 0466 loss_train: 0.4117 acc_train: 0.7680 loss_val: 21.4090 acc_val: 0.7000 time: 0.0067s\n",
            "Epoch: 0467 loss_train: 0.4125 acc_train: 0.7660 loss_val: 21.4456 acc_val: 0.7000 time: 0.0072s\n",
            "Epoch: 0468 loss_train: 0.4099 acc_train: 0.7720 loss_val: 21.4665 acc_val: 0.6937 time: 0.0065s\n",
            "Epoch: 0469 loss_train: 0.4163 acc_train: 0.7640 loss_val: 21.4751 acc_val: 0.7000 time: 0.0094s\n",
            "Epoch: 0470 loss_train: 0.4235 acc_train: 0.7740 loss_val: 21.5433 acc_val: 0.6813 time: 0.0073s\n",
            "Epoch: 0471 loss_train: 0.4313 acc_train: 0.7640 loss_val: 21.6800 acc_val: 0.6312 time: 0.0077s\n",
            "Epoch: 0472 loss_train: 0.4128 acc_train: 0.7740 loss_val: 21.7092 acc_val: 0.6687 time: 0.0081s\n",
            "Epoch: 0473 loss_train: 0.4116 acc_train: 0.7640 loss_val: 21.6766 acc_val: 0.7063 time: 0.0075s\n",
            "Epoch: 0474 loss_train: 0.4082 acc_train: 0.7780 loss_val: 21.6283 acc_val: 0.7250 time: 0.0078s\n",
            "Epoch: 0475 loss_train: 0.4077 acc_train: 0.7820 loss_val: 21.6633 acc_val: 0.7312 time: 0.0077s\n",
            "Epoch: 0476 loss_train: 0.4092 acc_train: 0.7780 loss_val: 21.7924 acc_val: 0.6937 time: 0.0074s\n",
            "Epoch: 0477 loss_train: 0.4064 acc_train: 0.7820 loss_val: 21.8697 acc_val: 0.6750 time: 0.0079s\n",
            "Epoch: 0478 loss_train: 0.4255 acc_train: 0.7760 loss_val: 21.8352 acc_val: 0.6875 time: 0.0081s\n",
            "Epoch: 0479 loss_train: 0.4441 acc_train: 0.7680 loss_val: 21.6043 acc_val: 0.7500 time: 0.0075s\n",
            "Epoch: 0480 loss_train: 0.4092 acc_train: 0.7860 loss_val: 21.5226 acc_val: 0.7625 time: 0.0078s\n",
            "Epoch: 0481 loss_train: 0.4177 acc_train: 0.7800 loss_val: 21.6234 acc_val: 0.7500 time: 0.0073s\n",
            "Epoch: 0482 loss_train: 0.4149 acc_train: 0.7900 loss_val: 21.7914 acc_val: 0.6875 time: 0.0080s\n",
            "Epoch: 0483 loss_train: 0.4096 acc_train: 0.7760 loss_val: 21.9155 acc_val: 0.6438 time: 0.0079s\n",
            "Epoch: 0484 loss_train: 0.4173 acc_train: 0.7740 loss_val: 21.9157 acc_val: 0.6625 time: 0.0067s\n",
            "Epoch: 0485 loss_train: 0.4148 acc_train: 0.7680 loss_val: 21.8372 acc_val: 0.7063 time: 0.0072s\n",
            "Epoch: 0486 loss_train: 0.4176 acc_train: 0.7760 loss_val: 21.7713 acc_val: 0.7188 time: 0.0069s\n",
            "Epoch: 0487 loss_train: 0.4128 acc_train: 0.7880 loss_val: 21.8307 acc_val: 0.6937 time: 0.0073s\n",
            "Epoch: 0488 loss_train: 0.4067 acc_train: 0.7800 loss_val: 21.9233 acc_val: 0.6500 time: 0.0072s\n",
            "Epoch: 0489 loss_train: 0.4107 acc_train: 0.7620 loss_val: 21.9755 acc_val: 0.6500 time: 0.0067s\n",
            "Epoch: 0490 loss_train: 0.4160 acc_train: 0.7560 loss_val: 21.9370 acc_val: 0.6750 time: 0.0066s\n",
            "Epoch: 0491 loss_train: 0.4133 acc_train: 0.7560 loss_val: 21.9811 acc_val: 0.6875 time: 0.0073s\n",
            "Epoch: 0492 loss_train: 0.4080 acc_train: 0.7620 loss_val: 22.1641 acc_val: 0.6813 time: 0.0066s\n",
            "Epoch: 0493 loss_train: 0.4021 acc_train: 0.7740 loss_val: 22.3607 acc_val: 0.6625 time: 0.0073s\n",
            "Epoch: 0494 loss_train: 0.4100 acc_train: 0.7620 loss_val: 22.4968 acc_val: 0.6375 time: 0.0087s\n",
            "Epoch: 0495 loss_train: 0.4079 acc_train: 0.7660 loss_val: 22.5002 acc_val: 0.6687 time: 0.0087s\n",
            "Epoch: 0496 loss_train: 0.4061 acc_train: 0.7700 loss_val: 22.4368 acc_val: 0.6937 time: 0.0068s\n",
            "Epoch: 0497 loss_train: 0.4309 acc_train: 0.7660 loss_val: 22.2825 acc_val: 0.7188 time: 0.0079s\n",
            "Epoch: 0498 loss_train: 0.4091 acc_train: 0.7760 loss_val: 22.2756 acc_val: 0.7188 time: 0.0090s\n",
            "Epoch: 0499 loss_train: 0.4065 acc_train: 0.7800 loss_val: 22.3764 acc_val: 0.7063 time: 0.0068s\n",
            "Epoch: 0500 loss_train: 0.4038 acc_train: 0.7840 loss_val: 22.5214 acc_val: 0.6687 time: 0.0085s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.0148s\n",
            "Test set results: loss= 2.0263 accuracy= 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU1f3H8fd3ZrKQBUIW1gBhl0XWsAmilUoRcd/QoqJW68+lVqstqMWlarWLtlqXulVRi1UUQYsiIC5lD/sOAQJJQAiBhCSQ/fz+mDuTSTKEAJlMkvt9PU8eZu69c+fcGOczZ7nniDEGpZRS9uUIdgGUUkoFlwaBUkrZnAaBUkrZnAaBUkrZnAaBUkrZnCvYBThV8fHxJikpKdjFUEqpRmXVqlWHjDEJ/vY1uiBISkoiJSUl2MVQSqlGRUT2nGifNg0ppZTNaRAopZTNBSwIRORtETkoIhtPsF9E5EURSRWR9SIyKFBlUUopdWKB7CN4B/gHMP0E+y8Culs/w4BXrX+VUjZTUlJCRkYGhYWFwS5KoxceHk5iYiIhISG1fk3AgsAY872IJNVwyGXAdOOe7GiZiMSISFtjzP5AlUkp1TBlZGQQHR1NUlISIhLs4jRaxhiys7PJyMigc+fOtX5dMPsI2gPpPs8zrG3ViMgdIpIiIilZWVn1UjilVP0pLCwkLi5OQ+AMiQhxcXGnXLNqFJ3FxpjXjTHJxpjkhAS/w2CVUo2chkDdOJ3fYzCDIBPo4PM80doWECvTDvPXr7dRUlYeqLdQSqlGKZhBMAe4yRo9NBzIDWT/wJq9R3jpm1SKSzUIlFLKVyCHj84AlgI9RSRDRG4TkTtF5E7rkLnALiAVeAO4K1BlAXA63JdaWq4L8SilKsvJyeGVV1455deNHz+enJycU37d5MmTmTlz5im/LlACOWro+pPsN8DdgXr/qlwOd7tZqTYNKaWq8ATBXXdV/j5aWlqKy3Xij8m5c+cGumj1otHNNXS6nFYQlGmNQKkG7YnPN7F539E6PWfvds157JI+J9w/ZcoUdu7cyYABAwgJCSE8PJyWLVuydetWtm/fzuWXX056ejqFhYXcd9993HHHHUDF3Gf5+flcdNFFjBo1iiVLltC+fXtmz55Ns2bNTlq2hQsX8uCDD1JaWsqQIUN49dVXCQsLY8qUKcyZMweXy8XYsWP5y1/+wscff8wTTzyB0+mkRYsWfP/993Xy+7FNEIQ4rRqBBoFSqopnn32WjRs3snbtWr799lsuvvhiNm7c6B2L//bbbxMbG8vx48cZMmQIV111FXFxcZXOsWPHDmbMmMEbb7zBtddeyyeffMKkSZNqfN/CwkImT57MwoUL6dGjBzfddBOvvvoqN954I7NmzWLr1q2IiLf56cknn2TevHm0b9/+tJqkTsQ2QeDpI9AagVINW03f3OvL0KFDK92Q9eKLLzJr1iwA0tPT2bFjR7Ug6Ny5MwMGDABg8ODBpKWlnfR9tm3bRufOnenRowcAN998My+//DL33HMP4eHh3HbbbUyYMIEJEyYAMHLkSCZPnsy1117LlVdeWReXCjSS+wjqgqePQIePKqVOJjIy0vv422+/ZcGCBSxdupR169YxcOBAvzdshYWFeR87nU5KS0tP+/1dLhcrVqzg6quv5osvvmDcuHEAvPbaazz11FOkp6czePBgsrOzT/s9Kr1fnZylEXA5tY9AKeVfdHQ0eXl5fvfl5ubSsmVLIiIi2Lp1K8uWLauz9+3ZsydpaWmkpqbSrVs33nvvPc477zzy8/M5duwY48ePZ+TIkXTp0gWAnTt3MmzYMIYNG8aXX35Jenp6tZrJ6bBPEDi0j0Ap5V9cXBwjR46kb9++NGvWjNatW3v3jRs3jtdee41evXrRs2dPhg8fXmfvGx4ezr/+9S+uueYab2fxnXfeyeHDh7nssssoLCzEGMPzzz8PwEMPPcSOHTswxjBmzBj69+9fJ+UQ9yjOxiM5Odmczgpl8zcf4PbpKXx+zyjOTmwRgJIppU7Xli1b6NWrV7CL0WT4+32KyCpjTLK/423XR1Barn0ESinlyz5NQ9pHoJSqZ3fffTeLFy+utO2+++7jlltuCVKJ/LNNEDi1j0CpBs0Y0+RmIH355Zfr/T1Pp7nfRk1D1lxDZRoESjU04eHhZGdnn9aHmKrgWZgmPDz8lF5nmxqBy6l9BEo1VImJiWRkZKALT505z1KVp8I+QaBzDSnVYIWEhJzS0oqqbtmmacjpvbNYg0AppXzZJghcOteQUkr5ZZ8g0D4CpZTyyz5BoH0ESinll22CwHsfgfYRKKVUJbYJghCnrlmslFL+2CYIKpaq1D4CpZTyZZsgcOnwUaWU8ss+QeDU4aNKKeWPfYJAJ51TSim/bBME2keglFL+BTQIRGSciGwTkVQRmeJnfycRWSgi60XkWxE5tZmSToH2ESillH8BCwIRcQIvAxcBvYHrRaR3lcP+Akw3xvQDngT+GMDy4HSI9hEopVQVgawRDAVSjTG7jDHFwIfAZVWO6Q18Yz1e5Gd/nXI6RPsIlFKqikAGQXsg3ed5hrXN1zrgSuvxFUC0iMRVPZGI3CEiKSKScibzlbscQmmZ9hEopZSvYHcWPwicJyJrgPOATKCs6kHGmNeNMcnGmOSEhITTfjOX1giUUqqaQC5Mkwl08HmeaG3zMsbsw6oRiEgUcJUxJidQBXI5HdpHoJRSVQSyRrAS6C4inUUkFJgIzPE9QETiRcRThqnA2wEsj9VHoE1DSinlK2BBYIwpBe4B5gFbgI+MMZtE5EkRudQ67Hxgm4hsB1oDTweqPODpI9AagVJK+QromsXGmLnA3Crbpvk8ngnMDGQZfLmcOnxUKaWqCnZncb1yORyUaBAopVQltgqCEKcOH1VKqapsFgQOSjQIlFKqEtsFQbF2FiulVCW2CoJQp4OSUq0RKKWUL1sFgcsp2jSklFJV2CoItI9AKaWqs10QaB+BUkpVZqsgCHVp05BSSlVlqyAIcTr0PgKllKrCdkGgS1UqpVRltguCYq0RKKVUJbYKglAdPqqUUtXYKghC9IYypZSqxl5B4NI+AqWUqspeQeAQisvKMUbDQCmlPOwVBE735eoC9kopVcFeQeByX652GCulVAV7BYFVIygp1RqBUkp52CoIQp0CoPcSKKWUD1sFgbdGoEGglFJetgyCUh1CqpRSXvYKAquzWJuGlFKqgq2CwNNHoE1DSilVwVZBoH0ESilVXUCDQETGicg2EUkVkSl+9ncUkUUiskZE1ovI+ECWR4NAKaWqC1gQiIgTeBm4COgNXC8ivasc9ijwkTFmIDAReCVQ5QH34vUARTrxnFJKeQWyRjAUSDXG7DLGFAMfApdVOcYAza3HLYB9ASwPraLDAdiXUxjIt1FKqUYlkEHQHkj3eZ5hbfP1ODBJRDKAucC9/k4kIneISIqIpGRlZZ12gTrHRxIe4mDL/qOnfQ6llGpqgt1ZfD3wjjEmERgPvCci1cpkjHndGJNsjElOSEg47TdzOoSeraPZvE+DQCmlPAIZBJlAB5/nidY2X7cBHwEYY5YC4UB8AMtEj9bRpGblB/ItlFKqUQlkEKwEuotIZxEJxd0ZPKfKMXuBMQAi0gt3EJx+208ttItpxqH8Ih05pJRSloAFgTGmFLgHmAdswT06aJOIPCkil1qH/Qa4XUTWATOAySbAq8a0aRGOMXAwryiQb6OUUo2GK5AnN8bMxd0J7Lttms/jzcDIQJahqjYt3COHfsw9TvuYZvX51kop1SAFu7O43rW1gmB/rg4hVUopsGMQNHfXAvbrvQRKKQXYMAiaN3PRPNxFWnZBsIuilFINgu2CQETo1iqK1IM6hFQppcCGQQDQvVW0BoFSSllsGQTdWkWRXVDMkYLiYBdFKaWCzrZBAOgdxkophd2DQJuHlFLKnkHQPqYZ4SEODQKllMKmQeBwCF3ideSQUkqBTYMAoHtrDQKllAIbB0G3hCgyc45zrLg02EVRSqmgsm8QWB3GOw/qHcZKKXuzbRB0tYJg1yFtHlJK2ZttgyCxpXvyuYwjx4NcEqWUCi7bBkFEqIu4yFANAqWU7dk2CMBdK8g4cizYxVBKqaCyeRBEkKk1AqWUzdk8CJqRceQ45eUBXSZZKaUaNNsHQXFZOVn5upC9Usq+7B0EsREA2k+glLI1WwdBBx1CqpRS9g6C9jHuGkH6Ya0RKKXsq1ZBICL3iUhzcXtLRFaLyNhAFy7QmoU6aRUdxu5DGgRKKfuqbY3gVmPMUWAs0BK4EXg2YKWqR10SInWaCaWUrdU2CMT6dzzwnjFmk8+2E79IZJyIbBORVBGZ4mf/CyKy1vrZLiI5tS963eiSEMWurAKM0SGkSil7ctXyuFUi8jXQGZgqItFAeU0vEBEn8DJwIZABrBSROcaYzZ5jjDH3+xx/LzDwFMt/xrrER5J7vITDBcXERYXV99srpVTQ1bZGcBswBRhijDkGhAC3nOQ1Q4FUY8wuY0wx8CFwWQ3HXw/MqGV56kzFLKQ6HbVSyp5qGwQjgG3GmBwRmQQ8CuSe5DXtgXSf5xnWtmpEpBPu2sY3J9h/h4ikiEhKVlZWLYtcO13jrSDI0n4CpZQ91TYIXgWOiUh/4DfATmB6HZZjIjDTGFPmb6cx5nVjTLIxJjkhIaEO3xbat2xGqMvBriytESil7Km2QVBq3L2plwH/MMa8DESf5DWZQAef54nWNn8mEoRmIQCnQ0iKi2Cn1giUUjZV2yDIE5GpuIeN/ldEHLj7CWqyEuguIp1FJBT3h/2cqgeJyFm4h6QurX2x61bf9i1YvTdHJ59TStlSbYPgOqAI9/0EP+L+dv/nml5gjCkF7gHmAVuAj4wxm0TkSRG51OfQicCHJojjN0d1i+dwQTFbfjwarCIopVTQ1Gr4qDHmRxH5ABgiIhOAFcaYk/YRGGPmAnOrbJtW5fnjtS9uYAxJigVgfUYufdq1CHJplFKqftV2iolrgRXANcC1wHIRuTqQBatPbVuE43QI+3J08jmllP3U9oayR3DfQ3AQQEQSgAXAzEAVrD65nA7aNA/X1cqUUrZU2z4ChycELNmn8NpGoV1MOJlaI1BK2VBtP8y/EpF5IjJZRCYD/6VK239jFxXmYvnuwzw+Z5OOHlJK2UqtgsAY8xDwOtDP+nndGPO7QBasviVbHcbvLElj4daDJzlaKaWaDmlss24mJyeblJSUOj9vebkh93gJ4/7+Pb3bNudftwyt8/dQSqlgEZFVxphkf/tqrBGISJ6IHPXzkyciTWrQvcMhtIwM5brkDny7PUtXLVNK2UaNQWCMiTbGNPfzE22MaV5fhaxP1w3tiMsh3DtjjfYVKKVsoUmN/KkL7WOaMe2SPqxNz+Hz9ftYs/dIsIuklFIBpUHgx4Sz2+IQuO/DtVzxyhKOFpYEu0hKKRUwGgR+tIwMpV9ijPf5J6syyDhyTJezVEo1SRoEJ/D0FX3pEh9JsxAnT3y+mVHPLeIbHVaqlGqCNAhOoE+7Fnzz4Pncf2F377bluw8HsURKKRUYGgQncevIzvz56n4ALNxygLf+t1tHEymlmhQNgpNwOR1ck9yBX57XhZ1ZBfzhi82sSc8JdrGUUqrOaBDU0n1jujOqWzwA/1qstQKlVNOhQVBLEaEu3v/FMHq0juKL9ft57fudFJWWAfDqtzv5bntWkEuolFKnR4PgFL02aTAAf/pqG899uY2i0jKe+2orN7+9IsglU0qp06NBcIq6JETx1s3ueZv+vWIP/1qcFtwCKaXUGdIgOA1jerXm7xMHUFhSzrNfbg12cZRS6ozUdqlKVcUl/dqRe7yE/6xMZ9M+90SsRaVlhLmcQS6ZUkqdGq0RnCaHQ7hpRBL//dW5/OWa/gD0e/xrHp61gePFZeQcK6ZMRxYppRoBrRHUgUEdY2gf04x2MeHMWLGXeRt/JLugmPvGdOf+C3sEu3hKKVUjrRHUgS4JUSyecgEf33kOr/58MNkFxQB8uiYjyCVTSqmTC2gQiMg4EdkmIqkiMuUEx1wrIptFZJOI/DuQ5akP4/q2Yc49I2kVHUb64ePM0jBQSjVwAQsCEXECLwMXAb2B60Wkd5VjugNTgZHGmD7ArwNVnvrULzGGV34+CIDffbKB2WszKSs3bMzMZfM+/yt83vDGMqZ+ur4+i6mUUkBgawRDgVRjzC5jTDHwIXBZlWNuB142xhwBMMY0mXmek5NieeDCHhSXlnPfh2u56e3lTHjpf1z80g9+F7pZsjObGSvSg1BSpZTdBTII2gO+n2wZ1jZfPYAeIrJYRJaJyDh/JxKRO0QkRURSsrIaz1QOfdtXLOu8ODUbAGPg+yrTUejoIqVUMAW7s9gFdAfOB64H3hCRmKoHGWNeN8YkG2OSExIS6rmIp2909wSmXnQW66aN5a7zu/LAhT1IiA7j3hlr6DPtK/7xzQ4AsguKglxSpZSdBTIIMoEOPs8TrW2+MoA5xpgSY8xuYDvuYGgSXE4HvzyvKy0iQvjtuLP41ZjuvDZpEC2ahVBQXMZfvt7O4YJiDh6tCALf5TAPHC3ksdkbKSwpC0bxlVI2EcggWAl0F5HOIhIKTATmVDnmM9y1AUQkHndT0a4AlinoBneKZdFvzvcudvPQx+tYuKWia2Tpzmzv49e+28m7S/ewVtc/UEoFUMCCwBhTCtwDzAO2AB8ZYzaJyJMicql12DwgW0Q2A4uAh4wx2f7P2HS0jAxlpLW2wcKtB3lhwXbvvhveXE7OsWKOF5fxySr30NM92QVBKadSyh4CemexMWYuMLfKtmk+jw3wgPVjK21bhFd6PrpHgrcTefrSPbSKDuNoYSkAuw8dq/fyKaXsQ6eYCBIR4Y9Xnk3r5mEMSYpl1Z4j3iB4fr67htC/Qwx5x0tOuUbw/rI99GobzeBOsXVebqVU06NBEETXD+3ofTy6ewJPXd6XMb1aced7qzheUsaLEwfw3FdbWbbrMHmFJUSHh2CMQURqPO+jn20EIO3ZiwNafqVU06BB0EA4HMKk4Z0AmHXXSETctYY7z+vKlxt/5NcfrqVtTDjLdh1m/v2jTxgGvqOOlFKqNjQIGiCHo+JDvl9iDE9e2offz97k3bZs12GKy8rZcSCPX5zbpdJrC4p1qKlS6tRoEDQCN45I4uNVGazPyAVg2uyN7DiYD7g7maPCXLSLaQZAnp/pK5RSqibBvrNY1dK9F7jvs7v93M7eEAAY+8L3nPPsNxy3agJ51kgjpZSqLQ2CRuLC3q3Z9MTP+PVP/S90c8UriykvN1ojUEqdMm0aakQiw9z/uR67pDfxUWHERYWyN/sYmTnHeembVFbvPUJeUUWNQNdQVkrVhgZBI3TLyM7ex+d0hfyiUv75/S7eW7aHTnGR3n25x0toFa1BoJSqmQZBExAV5mJY51hmr91XafsnqzJp0yKML9bt5+GLe9E1ISpIJVRKNWQaBE3EDUM78sOOQ5W2vbhwByVl5ZSWG7b+mMesu86hVfPwE5xBKWVX2lncRIzr24a10y7kp71aAfDMFWdz3Jq++rVJgzmYV8jQZxbywEdrg1lMpVQDpEHQRIgIMRGhvDZpMKse/SlDO7cE4NIB7RjXtw0dWkYA8OnqTErLymt93t2HCvjtzHUUl9b+NUqpxkWbhpoYl9NBXFQYsZGh/OmqfvzkLHcN4byeCew65J68bvnuw2zMzCWxZQTj+rbB6Tjx3EWPz9nEd9uzmNCvHaN7NJ7V4ZRStSeNbW6a5ORkk5KSEuxiNDrFpeWs2nOE//tgFTnHKu41iIkIYfWjF/L15gOM6h5PVFjl7wa/fC+FeZsO0Kttc5wO+OT/ztEhqUo1QiKyyhiT7G+fNg3ZRKjLwYiucUyb0BsROKtNNAA5x0pYuPUgd76/irs+WM0Tn2+qdFNaVFgIAFv2H2Vj5lFmWovlKKWaDg0Cm7lyUCLLHx7DF/eO4vUbBwPw3rI9AHy/PYt/LU7j7Me/ZsaKvQDkF1W+U/mRWRt57qut3ufGGO0/qMFHKen0e3weZeWNq+at7EWDwIZaRYfjcjoY3SMBp0O8C+L4+uvX7sVxDhcUA/D4Jb2Z0K8tAK9+uxOAg0cLefDj9fR49EvvXEeqskc/28jRwlINS9WgaRDYWHiIk/OsDuBxfdrw23E9vfsO5Rfx1cb9ZBcUM/7sNkwe2Zne7Zp79+ceL2HoMwv5ZLW7qSgz53j9Fr6RKT6FkVpK1TcdNWRzf766HzNXZXDTiCSahToZ16cN05fu4Z0ladz5/moAzukaB8Dt53bhQG4h7y7dwz3/Xl3pPJk5x+nWqvqdy+mHj7EhM5fxZ7cN/MU0YCUaBKoB0xqBzcVFhfHL87rSLNQ9EqhLQhSPXdKbP1/dz3tMkjV/UYjT4V0Ip+pdzJlH/NcIrnhlMXd9sPqU7l1oirRpSDVkGgSqGhHhsgHteeLSPnxx7yhu9ZnkrkNsBAM7xgDw12v6e7c/PGsDs9dmVjvXoXx3H8PhY8XebQs2H+Ct/+0OVPEbFquPOJA1go9T0r19OXXtu+1ZJE35L1v2Hw3I+VXDoE1Dyq9Ql4Obz0nyu+/924aRll1A8/CQStt/98l6uiZEER3uIiYilBbNKvYfyiumVbR7nqNfTHffB3LryKQTrr3cVBgrCQJVI9iTXcBDM9czslscH/xieJ2f/+tNPwKQknaYXm2bn+Ro1VhpEKhTFhnmok+7FgC8cF1/YiPD2Hv4GL//bCMTXvofAC6HsP7xsd7XHMovqnaeqZ9u4NL+7di8/2i1tZebmkB1FnsC5sfcwoCcX9mDBoE6I1cMTATcTR+//2yjd3tpuWH4Mwu9z//69TZ6tW1OXGSod9uHK9P5cGU6AJPPScLlbLotlYHuIwjUXQqeCpveBdG0BfT/PBEZJyLbRCRVRKb42T9ZRLJEZK3184tAlkcFTojTQcdY98R2r00axA3DOnLUZ/3kdRm53PjWclKz8v2+Pv0Enc11qbSsnD99tdVv7STQSsoC81FaFuApYoSm3XSn3AIWBCLiBF4GLgJ6A9eLSG8/h/7HGDPA+nkzUOVRgffPGwdz+YB2XHBWa5654myuGNie3j7tylt/zGPsC98D8MSlfSpNdpd60H9A1KUlO7N55dudPDZnU8Dfy8PzOR2oGoH3vGeYB19u2O+3eclbI9AqQZMWyBrBUCDVGLPLGFMMfAhcFsD3U0HWq21z/jZxIKEu95/VC9cNYO595/Lg2B48d9XZhLkcDOwYw3NXnc2k4Z3oEl+xrObmfe5RKVt/PMqVryzmYF7t27z/8c0OejzyJSebQLG03P2hmedTU6kvgRo15AmCM/mcLi0r5/8+WM3E15dW2+eJ6sY2OaU6NYEMgvZAus/zDGtbVVeJyHoRmSkiHfydSETuEJEUEUnJyqo+HYJq2O65oDvXDenI4ikXMPPOc7huSEecDuHJy/oCEB3m4t2laSzblc1L36Syem8Oz3+9vdYfPn/5ejvFZeWVhlBmHDlW7bhj1jQYwfhQKwpwjeBMrslTtj2Hq//OGhJjjM7ZFCDB7p37HEgyxvQD5gPv+jvIGPO6MSbZGJOckKBz4jdW8VFhlZqDRnSNY+cz4/n37cM5XFDMxNeX8d/1+wF3R/IDH62r9AFX9fHUT9ezcMsB7zbPNBffbc9i1HOLvEMfPY5YQRGML7eBqhEUlZ15jaDIGybV93mG9zaEj99HPttI14fnBrsYTVIggyAT8P2Gn2ht8zLGZBtjPD13bwKDA1ge1QA5HcLZiS14cGwP77Z3bx3Kned1ZdaaTG54YzlPfL6JJTsPce6fFvE/647m95btYcaKdG57t2Jtin1WEGy1bn5anHqIvMISDh51NzMdsdZhqM9vlZ53CnjT0BlcUlHpyScMbAhfxP+93D0jrjZT1b1ADh9dCXQXkc64A2AicIPvASLS1hiz33p6KbAlgOVRDdg9F3Rn4tCO7DiQz4iucQzvEssX6/exfHc2S3dl86/FaQDc/e/VPHdVP6bNrt7h+932Q7RuHu7t4MwrLOXnby5nfUYut4xMIt1q+sg9XjG19rYf82gZGeK92S1QAt1ZbM7gO3tRycnL1pCmCCkqLSc8RBdHqksBCwJjTKmI3APMA5zA28aYTSLyJJBijJkD/EpELgVKgcPA5ECVRzV88VFhxEeFARDmcvL1/aNZn5HLxNeXAXDVoERmrcngzvdXERXm4tuHzqekrJw2zcPp+9g8ZqzY611HAWDnoQLWZ+QCeIMEILugiKOFJZSVGX72t++JDnex4fGfATB7bSab9h3l4fG96vTaAl0jOBM19V94QrUhzZVUWFKmQVDHAnpDmTFmLjC3yrZpPo+nAlMDWQbVeEWEuhiaFMsdo7tw1aBEeraJ5qK+bZi1NpPrh3T0hgbAU1f0Ze6GH9mZlc+uLPfazOvSc/yeNyuviOSnFng/3PIKSzHG8MYPu3hmrnvRnd+NO6vGtZxPVcA6i8sC2zRUbrUJNaRptAP1u7SzYHcWK1Ujh0N4eHwvelpLa/60d2tevmEQo7rHVzruioGJvHFTMu9MHlrtHJ45cu7/aQ9uHdmZclP9G+6bP+z2hgDAS9/sYNAf5pOSdrhW5Vy15zDvLK4+kZ6nPTtQN5TVTR/BiT9YSxpgEBSW6CJIdU2nmFBNSofYZgDERYby94kDSYqPICYilKPHS2gX497XpkUYn6/bz4bMXO/rnp67hZ6to7kmOZGn/ruFvy3YAcCsNZl0jo+kzJga+xGuetU9Bv+mEUk4HEJRaRkuh8Pbch/wG8rOQE19BCXW+euraaikrJx3Fqdx0zmdCHP5b/4prEWfhjo1WiNQTYqIMP/+0cy971xGdY8nsWUEUWEubwgA3DG6K5/fO4q7f9KVqDAX53aPRwR+NaY7lw+sfKvLoq0HOefZb/jJn78lK6+Ip77YTEGR+4a0j1amM232RrLyKqasyMovwhhDz0e/4oGP1nq/qQesj6Cs4j6C2WszeX7+9lM+R01NQ55y11cQvL9sD0/P3VKpT6eq2oxyUqdGawSqyeneOrpWx93/0x7cNCKJlhGhHC8po0WzEIwxXHBWK5bsPMRd53fzfrAWAUOeXnzGjN4AABSDSURBVAC4J9SLiQjx1hqmL93jPWfGkePeu3Fnr93n3R6ophVPs065gfs+XAvAAxf2qOklJzyHP54mLX9BtjLtMIM7tsRRh30pOdYQ32M1rIGtNYK6pzUCZVsup4PWzcMJdTm8ayeICG/dnMya349l4hD3bTAdYpsxrHMs4SHu/13eWZLmDYGqXvtuJ3/8cmu17YcLivlhRxaPzNpQ7T4Gz+IvvndDG2MqtYXPXJXBVxv3U5Xnm7pv0PgOj62Nmr5he867P7fQ23HsKfM1ry3lbT/9Imei3KpCOWtYp0JrBHVPawRKVSEiNAt10izUyWOX9GZgx5YM6OBele277Vk8/OkG713MVc3ffMDv9pmrMpi5KgOAnw/rRO92FZPxTV+SBsCqPUdIbGnN4PrdLp77aivLpo7hk9UZ/HneNgDSnr2YY8WlTP10AzcM7egNgiKf0Mg4cowWzVp4n2/Zf5Q1e3O4YVhHv2WrsY/ACoIfdhzihQXb+c3YngDste7J2HGgbicLLLXCxuU8cRBojaDuaY1AqRrcMrKzNwQAzuuRwOIpF/DRL0cQHuJgxu0Vq4KN6hbPFQPb88vzuniHnkaFVf+uNf7FH/jbgu08/d/NFJeWez/81uzN4csN7m/97y9zNzdd9eoSbwgA/PK9FHpPm8fstfu47vVlFFrfjguKfYOgckhd9PcfeHjWhkrf6H3V3DRUse+L9T41Euube10vMOcpo6OGE+uoobqnNQKlTsPQzrFsemIcTofwzi1DCHU5OKdrxZDWm0Yk8dQXm3n6irOZvTaTzCPHedNnnWZP01JBcRnfbXdPpPjOkjTeWZLGusfGEhMRQmbO8Wo1j3mbKtc49mQXVCvbB8v3MiQpllifRYAAjhaWEBMRWu34GjuLSyvCw/ej2ZMpdb3UaGkt5rLQ+wjqntYIlDpNnm/95/dsVSkEANrHNOPVSYOJjQzllpGdmXLRWfy0V2vv/msGJxIe4vDOn+Nr876j3qmy+7RrzpCklt59E/q15Zeju5DcqaX3WF+DO7Xk++1ZDPrDfBanHqq0b9aaTG+NY/ehApakHsIYU6lpKOdYMSVl5SzZeYj1GTms8L2PwuczP98aOVXXNQLPVBY1hZPWCOqe1giUqgcup4M3b07GGIMx7hvlWjcP5x+LUnn8kt6M79eWd5ek8fKinVz/hntKjSsGtuf3E3pTUlbOzFUZxESEcPmA9kSGucjOL2LwUws4cqyEdi3C2WctKvPurUOZvjSNP321jS837qdv+4q+gic+3wzAry7oxovfpALwt+sGVPqGPeDJ+fRq25wt+ysHDFS+ac3TIV3Xw0o9o4WO1/Bhr0FQ9zQIlKpHIuL9Fv2bsT0Y06sV/RNjcDiEh352Fp+syuRHa7bUG0d08jbv3P2TbpXOExcVRnxUKIfyi2nVPJxnrjybhOgwosJc3HV+N1bvyeH9ZXt5f1n1GocnBADmbtjP11U6uP2FALhrEX+cu4Wp43uRaw3zPHqCEUo3vLGMEV3iuHdM91r8VioUFLtrGlU7sH1nHNWmobqnQaBUkIgIAzu2rLRt3v2j2ZNdQO7xEgZV2VfV8C5xfLF+Pxec1Yrze7aqtO+hn/Vk075c9vtZftKjY2xEtRA4mX9+v4ve7Zp7awSZOccpKzeV5mUyxrBkZzZLdmb7DYLs/CLifOaJ8pVfZNUIqtxH4Nt3UKQ1gjqnfQRKNSAtmoXQLzGGc7uffAGm3/7sLMb2bs2k4Z2q7evZJpqlU8fw1OV9q+37w+V9SXv2Yi4f0K7S9tvP7XzC99r6h3F8ff9oBnWM4cnPN5Nd4L6betO+ozzxecWU4EWlZWT7rBQHkH74GH0fm8eW/UfZcSCPwU8t4D8rq9dUAPIL3QFTWKWPwLcW0BRqBHdMT+G9ZXtOfmA90SBQqpHqGBfB6zclVxsd5GvS8E7s/uN47/PdfxzPjVZw3Dgiieev7c+iB89nxu3DeeTi3qz5/YXcPKIT028dyuRzkryvCw9x0qN1NHf/pBvZBcWsTDvi3Td96R6KS8v5dttBej76lfd+CY+5G/aTX1TKv5fvZac1M+y7S/b4besvOEGNwLcWUFP/QX3Jzi+i+yNzWb4r+5Rfa4xh0baDrN5z5OQH1xNtGlKqiRMRPvm/EXSIjag03DMhOowrByUC0Dk+EoCWkaE8Ya0lPbpHAhGhTgZ3qmiiOq9HRU1lUMcYVu91T/Wd/NR8jlojnZ71ubO6sKTMe4/De8v2eBfQ2bz/KA/NXM9L1w+sVNasfHdNo+qHvW8tICuviLJyQ35hKS0iQk7591EXVu/NoaTM8M/vdzGsS9wpvTbnWAklZcY78qoh0BqBUjYwuFPsaa3C9ttxZzHGZ9iry+ngi3tHMfmcJGbcMZy0Zy/m9xN606aFe2W4n/VpXen1Z/3+K15cWDEdh2/n9efr9lU6Nq+whMNWs1JRSTmfrs7wrjt9tLCiU3pfznH+tmA7/Z/8+pSn06grni6R8tOY//ugNUlhfmHDCQKtESilTknf9i0qDUu9bVRnbhvV2TuyZ31GLrdPT/F+4NVk6qfrOZRfzMs3DCL9cMXNc7uzC3jgo3UArJs2lq378wAY2DGG9MPH+XS1e/nzRVsPVpsxtj545ouqTQ58tiaTiFAnY/u0AfDOVusZIXUy32w9QJf4KJKsWlsgaI1AKVUn3ENjhf4dYnjRp8ln/NlteP+2YTxxaR/vtuev7Q/AjBXpzN98gF//Zw2PfLYBcHeY+07t/fbi3bz67U4Azu0Wz6H8IhzWJ9fn6/aRX1TKlxv2V5vML+dYsfc+h+lL0/jndztZtO1gjdeQmXOcX76X4r3x7kQ8zTrlxvDIrA384t0Uv8cZY/j1f9Zyx3urvNsO5hVWOsfJ3PpOChf89dtaHXu6tEaglKpzQ5NimX7rUGIjQ721h1Hd48nOL6JlZChXDGxPuXE3sWzad5S3F+/2frse2S2OuRt+5NWfD2L60j383WpaOrt9CzrEuifl89QeFm49SN/H5nnft3+HGB67pDczlu/l41UZXDmwPY9O6M202RUjm+KjwhjRNa5a/wTAWz/sZt6mAxSXlnPR2W1PeH2eO7+NcU/p4X7s7gTen1vIz4e5O+Q9k/P5OpWmoWKfacYDSYNAKVXnHA5hdI/qQ2AfsGYvBbh6sLuj+spB7vseco6VcDCvkE6xkfxu3Fl0ioukV9vmvPW/3cRGhnLpgHZEh7u4qG8bjhWXccfoLtz/n7XeD1aHuNepvvKVJd73+HRNJj3aVF6f4lB+EZ+v2+c3CNKsuZv8fYD7yrP6LHz7CA7lF3PrO+6agScIfEdXGWMQkYqmoVrUCHKOF5/0mLqgQaCUCrrwECdtWjhp08Ldoe0ZDZQUH8kfqtwL8eqkwd7H8349moF/mM8fLuvDDcM6sSEzl398k8qGzBwOHHV/4HpGMTmk+jfrgqJSIn1miPUNgtKyclxO/63nh/LdH9C+ndXbD+R5H/+YW0ibFuEs8ZnvKa+olObhId7gKiguo6SsnBCno2LW1SqL/HgW6gk07SNQSjVaLSND2fH0RUwa3gmnQxjQIYY3b05m6ZQxzP3Vud7jLunfjl1/vJhxVoctwPPzt9PnsXm88f0uZq7K4Oa3V7Arq4D4qFBKygx/W7CDz9ZkVrvfYfbaTN6x1pDY5DPpn+8a2MP/uJDDBcX8zycIDlrBlJVXcbd38lML+MW7KXR5eC4TXvofxaXlvPnDLu8UHkcKtEaglFInFeLnW7vDIfRu15zUpy/ijR92c+Ug98iiySOT+MoakuoZ1vr03C2VXnvTiCT+8U0q/1jknpMp+jMXT17eh9lr99G9VRQLt/rvcPZdNwLg5rdXcDCviMsHtOOztfvYfaiAbq2iKo2myj1ewoIt7mk+Nu8/yjNzt/DOkjS+257Fe7cN44hPjaC4tJxQV2C+u4s5jXGwwZScnGxSUvz30Cul1MkcPFrIre+uJCEqjEXb3GtB3HtBNxJbNkMQLunfjh0H89ifW0hEqJOpn26ottiPr5iIEM7vkcBna/f53f/Z3SOZ+PpSWkaEMvvukQx9ZmG1Yyafk8S/l++ttOToP28czFcbf2TWGvdQ2eUPj6F181O/F8RDRFYZY5L97gtkEIjIOODvgBN40xjz7AmOuwqYCQwxxtT4Ka9BoJSqK4tTDxHmcpCcFHvCY3KOFbM2PYfYyFB+O3M9IsKW/UeJDHWS8uiFNAt1cqSgmHP/tIj8olKmXHQWQ5JimTZ7IxGhTj6+8xx+2JHFjW+t8J6ze6sodhzMp19iC9Zn5LLxiZ9x1StL2HYgj3O7x/PDjkPVyvHi9QO5tH+7attrKyhBICJOYDtwIZABrASuN8ZsrnJcNPBfIBS4R4NAKdXQHSkoxiFSaYqLAms+pZvPSSLU5fCOEvJ484ddvDB/OwXFZbx0/UD6J8bQIbYZx4rLiAxz8enqDB74aB2Lp1zA+L//UKkjum/75uzPKeTr+0efcObWkwlWEIwAHjfG/Mx6PhXAGPPHKsf9DZgPPAQ8qEGglGrKDuUXERsRWm2EkDGG0nJDiNPBz99cxuLUbF6bNJguCZEYAxNe+oHfjTuLX5zb5bTet6YgCGRncXsg3ed5BjCsSsEGAR2MMf8VkYcCWBallGoQ4k/wjV5ECHG6w+HPV/fnnSVpjOnVytsZ/uV959I1ISogZQraqCERcQDPA5NrcewdwB0AHTt2DGzBlFIqyNrFNOPh8b0qbevWKvoER5+5QN5HkAl08HmeaG3ziAb6At+KSBowHJgjItWqLsaY140xycaY5ISEky/YoZRSqvYCGQQrge4i0llEQoGJwBzPTmNMrjEm3hiTZIxJApYBl56sj0AppVTdClgQGGNKgXuAecAW4CNjzCYReVJELg3U+yqllDo1Ae0jMMbMBeZW2TbtBMeeH8iyKKWU8k/nGlJKKZvTIFBKKZvTIFBKKZvTIFBKKZtrdLOPikgWsOc0Xx4PVJ/NqWnTa7YHvWZ7OJNr7mSM8XsjVqMLgjMhIiknmmujqdJrtge9ZnsI1DVr05BSStmcBoFSStmc3YLg9WAXIAj0mu1Br9keAnLNtuojUEopVZ3dagRKKaWq0CBQSimbs00QiMg4EdkmIqkiMiXY5akrIvK2iBwUkY0+22JFZL6I7LD+bWltFxF50fodrLdWiGt0RKSDiCwSkc0isklE7rO2N9nrFpFwEVkhIuusa37C2t5ZRJZb1/Yfa8p3RCTMep5q7U8KZvlPl4g4RWSNiHxhPW/S1wsgImkiskFE1opIirUtoH/btggCEXECLwMXAb2B60Wkd3BLVWfeAcZV2TYFWGiM6Q4stJ6D+/q7Wz93AK/WUxnrWinwG2NMb9wLGt1t/fdsytddBFxgjOkPDADGichw4DngBWNMN+AIcJt1/G3AEWv7C9ZxjdF9uKex92jq1+vxE2PMAJ97BgL7t22MafI/wAhgns/zqcDUYJerDq8vCdjo83wb0NZ63BbYZj3+J3C9v+Ma8w8wG7jQLtcNRACrca8BfghwWdu9f+e41wEZYT12WcdJsMt+iteZaH3oXQB8AUhTvl6f604D4qtsC+jfti1qBEB7IN3neYa1ralqbYzZbz3+EWhtPW5yvwerCWAgsJwmft1WM8la4CAwH9gJ5Bj3IlBQ+bq812ztzwXi6rfEZ+xvwG+Bcut5HE37ej0M8LWIrLLWa4cA/20HbfF6VT+MMUZEmuQYYRGJAj4Bfm2MOSoi3n1N8bqNMWXAABGJAWYBZwW5SAEjIhOAg8aYVSJyfrDLU89GGWMyRaQVMF9EtvruDMTftl1qBJlAB5/nida2puqAiLQFsP49aG1vMr8HEQnBHQIfGGM+tTY3+esGMMbkAItwN43EiIjnC53vdXmv2drfAsiu56KeiZHApSKSBnyIu3no7zTd6/UyxmRa/x7EHfhDCfDftl2CYCXQ3RpxEApMBOYEuUyBNAe42Xp8M+42dM/2m6yRBsOBXJ/qZqMh7q/+bwFbjDHP++xqstctIglWTQARaYa7T2QL7kC42jqs6jV7fhdXA98YqxG5MTDGTDXGJBpjknD///qNMebnNNHr9RCRSBGJ9jwGxgIbCfTfdrA7RuqxA2Y8sB13u+ojwS5PHV7XDGA/UIK7ffA23G2jC4EdwAIg1jpWcI+e2glsAJKDXf7TvOZRuNtR1wNrrZ/xTfm6gX7AGuuaNwLTrO1dgBVAKvAxEGZtD7eep1r7uwT7Gs7g2s8HvrDD9VrXt8762eT5rAr037ZOMaGUUjZnl6YhpZRSJ6BBoJRSNqdBoJRSNqdBoJRSNqdBoJRSNqdBoFSAicj5ntkzlWqINAiUUsrmNAiUsojIJGvO/7Ui8k9rkrd8EXnBWgNgoYgkWMcOEJFl1hzws3zmh+8mIgusdQNWi0hX6/RRIjJTRLaKyAfW3dGIyLPiXldhvYj8JUiXrmxOg0ApQER6AdcBI40xA4Ay4OdAJJBijOkDfAc8Zr1kOvA7Y0w/3Hd0erZ/ALxs3OsGnIP7rm9wz5D6a9zrYXQBRopIHHAF0Mc6z1OBvUql/NMgUMptDDAYWGlN9TwG9wd2OfAf65j3gVEi0gKIMcZ8Z21/FxhtzRHT3hgzC8AYU2iMOWYds8IYk2GMKcc9JUYS7qmSC4G3RORKwHOsUvVKg0ApNwHeNe5VoQYYY3oaYx73c9zpzslS5PO4DPfiKqW4Z5acCUwAvjrNcyt1RjQIlHJbCFxtzQHvWSO2E+7/RzyzXd4A/M8YkwscEZFzre03At8ZY/KADBG53DpHmIhEnOgNrfUUWhhj5gL3A/0DcWFKnYwuTKMUYIzZLCKP4l4ZyoF7Nte7gQJgqLXvIO5+BHBPBfya9UG/C7jF2n4j8E8RedI6xzU1vG00MFtEwnHXSB6o48tSqlZ09lGlaiAi+caYqGCXQ6lA0qYhpZSyOa0RKKWUzWmNQCmlbE6DQCmlbE6DQCmlbE6DQCmlbE6DQCmlbO7/AXmaL/Hv90ofAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqrrHtd7SGT7"
      },
      "source": [
        "建立NeuMF层模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYWxFtfXrg9a"
      },
      "source": [
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import initializers\n",
        "#from keras import initializations\n",
        "#from keras.regularizers import l1, l2, l1l2\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.core import Dense, Lambda, Activation\n",
        "from keras.layers import Embedding, Input, Dense, merge, Reshape, Flatten, Dropout\n",
        "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
        "from time import time\n",
        "import sys\n",
        "import argparse\n",
        "import scipy.sparse as sp\n",
        "import numpy as np"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kHfN6ORfXap"
      },
      "source": [
        "使用数据生成正负样本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNF9ZTvvoe3N"
      },
      "source": [
        "def load_rating_file_as_matrix(filename):\n",
        "  '''\n",
        "  Read .rating file and Return dok matrix.\n",
        "  The first line of .rating file is: num_users\\t num_items\n",
        "  '''\n",
        "  # Get number of users and items\n",
        "  num_users, num_items = 0, 0\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      u, i = int(arr[0]), int(arr[1])\n",
        "      num_users = max(num_users, u)\n",
        "      num_items = max(num_items, i)\n",
        "      line = f.readline()\n",
        "  # Construct matrix\n",
        "  print(num_users)\n",
        "  print(num_items)\n",
        "  mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "  with open(filename, \"r\") as f:\n",
        "    line = f.readline()\n",
        "    while line != None and line != \"\":\n",
        "      arr = line.split(\",\")\n",
        "      #user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "      #if (rating > 0):\n",
        "      user, item = int(arr[0]), int(arr[1])\n",
        "      mat[user, item] = 1.0\n",
        "      line = f.readline()    \n",
        "  return mat"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCaosFTwqXrM",
        "outputId": "24744dda-a131-4435-917f-2e981b0feb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "filename = '/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/data/drug-diseaseNum.csv'\n",
        "train_Martrix = load_rating_file_as_matrix(filename)\n",
        "train_Martrix"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "266\n",
            "836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<267x837 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 17414 stored elements in Dictionary Of Keys format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDjZWK9vQ5R6"
      },
      "source": [
        "将vector of each nodes construct a serise of pairs of nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PS2zqyeUgjd"
      },
      "source": [
        "def get_train_instances(train, num_negatives):\n",
        "  global user_input,item_input\n",
        "  user_input, item_input, labels = [],[],[]\n",
        "  num_users, num_items = train.shape\n",
        "  for (u, i) in train.keys():\n",
        "    # positive instance\n",
        "    user_input.append(u)\n",
        "    item_input.append(i)\n",
        "    labels.append(1)\n",
        "    # negative instances\n",
        "    for t in range(num_negatives):\n",
        "      j = np.random.randint(num_users,num_items)\n",
        "      while (u, j) in train.keys():\n",
        "        j = np.random.randint(num_users,num_items)\n",
        "      user_input.append(u)\n",
        "      item_input.append(j)\n",
        "      labels.append(0)\n",
        "  # 遍历生成NeuMF需要的drug的vecter\n",
        "  drug_latent_vector, disease_latent_vector = [], []\n",
        "  for i in user_input:\n",
        "    drug_latent_vector.append(Emdebding_train[i].detach().numpy())\n",
        "  for j in item_input:\n",
        "    disease_latent_vector.append(Emdebding_train[j].detach().numpy())\n",
        "  return drug_latent_vector, disease_latent_vector, labels"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64WXnC1BNX5u",
        "outputId": "738b72f8-54c2-4684-ed3d-ae25f0ce08b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        " from keras.layers.merge import multiply, concatenate\n",
        " import numpy as np\n",
        " drug_Embedded = [[1,2,3],[4,5,6],[4,5,6]]\n",
        " disease_Embedded = [[1,2,3],[4,5,6],[4,5,6]]\n",
        " mf_vector = concatenate([np.array(drug_Embedded), np.array(disease_Embedded)], axis = 0 )\n",
        " mf_vector"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(6, 3), dtype=int64, numpy=\n",
              "array([[1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [4, 5, 6],\n",
              "       [1, 2, 3],\n",
              "       [4, 5, 6],\n",
              "       [4, 5, 6]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iShdpqRja9on"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(drug_latent_vector, labels, test_size=0.2, random_state=42)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(disease_latent_vector, labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6txb2rHpDaS",
        "outputId": "a7646280-84b8-485c-f414-148af1d1dd62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(X_train2)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27862"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdiZg9SOgety",
        "outputId": "59317c78-caf5-4576-fc49-c5915288fc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "Emdebding_train[31]"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1258, 0.0000, 0.0000, 0.0000, 0.1699, 0.1190, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.1654, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYcnjmn0ch8R",
        "outputId": "c8df544a-01d4-486d-b8c9-a8956cad1e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = X_train1[0]\n",
        "(Emdebding_train.detach().numpy() == 0.26566425 ).nonzero()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([269]), array([0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKaQRfLbmOW",
        "outputId": "e661b39f-5389-40c0-8f8c-097e59027cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.array(X_train1).shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27862, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-_TF1C2bxWR",
        "outputId": "6ebf24d0-d3a1-4e35-84a7-ca924728c1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "X_train2[3]"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.26566425, 0.07014281, 0.        , 0.        , 0.01957224,\n",
              "       0.        , 0.02155467, 0.        , 0.19214466, 0.4725169 ,\n",
              "       0.        , 0.33538467, 0.29436526, 0.2282042 , 0.25656098,\n",
              "       0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T0MSNnNb2C5",
        "outputId": "9b98130d-f545-4ce9-d0d5-fec9e025e656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train1[3]"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-pOc3KDb9lO",
        "outputId": "13c1d49a-5061-4ce8-e38c-836183524aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train2[3]"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-kJPd5ZK_Ou"
      },
      "source": [
        "from keras.layers.merge import multiply, concatenate\n",
        "from keras.layers import Input, Dense\n",
        "def NeuMF_getmodel(layers=[10], reg_layers=[0], reg_mf=0):\n",
        "  assert len(layers) == len(reg_layers)\n",
        "  num_layer = len(layers) #Number of layers in the MLP\n",
        "  # Input variables\n",
        "  drug_Embedded = Input(shape=(16,))\n",
        "  disease_Embedded = Input(shape=(16,))\n",
        "  \n",
        "  mf_vector = multiply([drug_Embedded, disease_Embedded])\n",
        "  # mf_vector = merge([drug_Embedded, disease_Embedded], mode='mul') # element-wise multiply\n",
        "\n",
        "  # MLP part \n",
        "  mlp_vector = concatenate([drug_Embedded, disease_Embedded])\n",
        "  # mlp_vector = merge([drug_Embedded, disease_Embedded], mode='concat')\n",
        "  for idx in range(1, num_layer):\n",
        "    layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
        "    mlp_vector = layer(mlp_vector)\n",
        "\n",
        "  # Concatenate MF and MLP parts\n",
        "  predict_vector = concatenate([mf_vector, mlp_vector])\n",
        "  # predict_vector = merge([mf_vector, mlp_vector], mode='concat')  \n",
        "  # Final prediction layer\n",
        "  prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector) # sigmoid\n",
        "    \n",
        "  model = Model(input=[drug_Embedded, disease_Embedded],output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McbsRnpQ2PYJ",
        "outputId": "f416006e-8d0a-4f9c-9e21-9d80e9555974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "num_negatives = 4 # 4  1\n",
        "global num_scale \n",
        "num_scale = 69650 # 69650 27862\n",
        "batch_size = 256\n",
        "\n",
        "model = NeuMF_getmodel()\n",
        "model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "# load pima indians dataset\n",
        "drug_latent_vector, disease_latent_vector, labels = get_train_instances(train_Martrix, num_negatives)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(drug_latent_vector, labels, test_size=0.2, random_state=2)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(disease_latent_vector, labels, test_size=0.2, random_state=2)\n",
        "train_loss = []\n",
        "# Training model\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  # Generate training instances\n",
        "  # Training\n",
        "  # hist = model.fit([np.array(drug_latent_vector[:num_scale]), np.array(disease_latent_vector[:num_scale])], #input \n",
        "  #                  np.array(labels[:num_scale]), # labels \n",
        "  #                  batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "  hist = model.fit([np.array(X_train1), np.array(X_train2)],  \n",
        "                   np.array(y_train1), \n",
        "                   batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "  train_loss.append(hist.history['loss'][0])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "        'loss_train: {:.4f}'.format(hist.history['loss'][0]),\n",
        "        'acc_train: {:.4f}'.format(hist.history['acc'][0]))\n",
        "  \n",
        "test_scores = model.evaluate([np.array(X_test1), np.array(X_test2)], y_test1)\n",
        "print(\"Test set results:\",\n",
        "      \"loss= {:.4f}\".format(test_scores[0]),\n",
        "      'acc_train: {:.4f}'.format(test_scores[1]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "epochs = len(train_loss)\n",
        "plt.plot(range(0,epochs,1), train_loss, label='train_loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "#plt.savefig(\"/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/\"+time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))+\"Unet-过拟合C0.jpg\")\n",
        "plt.show()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", name=\"prediction\", kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"pr...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 0.5904 acc_train: 0.7886\n",
            "Epoch: 0002 loss_train: 0.5166 acc_train: 0.7990\n",
            "Epoch: 0003 loss_train: 0.5095 acc_train: 0.7992\n",
            "Epoch: 0004 loss_train: 0.5039 acc_train: 0.7993\n",
            "Epoch: 0005 loss_train: 0.5014 acc_train: 0.7993\n",
            "Epoch: 0006 loss_train: 0.5002 acc_train: 0.7994\n",
            "Epoch: 0007 loss_train: 0.4995 acc_train: 0.7994\n",
            "Epoch: 0008 loss_train: 0.4989 acc_train: 0.7995\n",
            "Epoch: 0009 loss_train: 0.4987 acc_train: 0.7995\n",
            "Epoch: 0010 loss_train: 0.4983 acc_train: 0.7994\n",
            "Epoch: 0011 loss_train: 0.4982 acc_train: 0.7994\n",
            "Epoch: 0012 loss_train: 0.4980 acc_train: 0.7994\n",
            "Epoch: 0013 loss_train: 0.4977 acc_train: 0.7995\n",
            "Epoch: 0014 loss_train: 0.4975 acc_train: 0.7995\n",
            "Epoch: 0015 loss_train: 0.4975 acc_train: 0.7994\n",
            "Epoch: 0016 loss_train: 0.4972 acc_train: 0.7995\n",
            "Epoch: 0017 loss_train: 0.4971 acc_train: 0.7995\n",
            "Epoch: 0018 loss_train: 0.4971 acc_train: 0.7993\n",
            "Epoch: 0019 loss_train: 0.4970 acc_train: 0.7994\n",
            "Epoch: 0020 loss_train: 0.4968 acc_train: 0.7994\n",
            "Epoch: 0021 loss_train: 0.4967 acc_train: 0.7993\n",
            "Epoch: 0022 loss_train: 0.4966 acc_train: 0.7995\n",
            "Epoch: 0023 loss_train: 0.4966 acc_train: 0.7994\n",
            "Epoch: 0024 loss_train: 0.4965 acc_train: 0.7994\n",
            "Epoch: 0025 loss_train: 0.4964 acc_train: 0.7994\n",
            "Epoch: 0026 loss_train: 0.4963 acc_train: 0.7994\n",
            "Epoch: 0027 loss_train: 0.4962 acc_train: 0.7993\n",
            "Epoch: 0028 loss_train: 0.4962 acc_train: 0.7993\n",
            "Epoch: 0029 loss_train: 0.4961 acc_train: 0.7994\n",
            "Epoch: 0030 loss_train: 0.4959 acc_train: 0.7994\n",
            "Epoch: 0031 loss_train: 0.4959 acc_train: 0.7994\n",
            "Epoch: 0032 loss_train: 0.4959 acc_train: 0.7994\n",
            "Epoch: 0033 loss_train: 0.4958 acc_train: 0.7994\n",
            "Epoch: 0034 loss_train: 0.4958 acc_train: 0.7995\n",
            "Epoch: 0035 loss_train: 0.4958 acc_train: 0.7993\n",
            "Epoch: 0036 loss_train: 0.4956 acc_train: 0.7994\n",
            "Epoch: 0037 loss_train: 0.4956 acc_train: 0.7993\n",
            "Epoch: 0038 loss_train: 0.4956 acc_train: 0.7994\n",
            "Epoch: 0039 loss_train: 0.4955 acc_train: 0.7994\n",
            "Epoch: 0040 loss_train: 0.4955 acc_train: 0.7995\n",
            "Epoch: 0041 loss_train: 0.4954 acc_train: 0.7993\n",
            "Epoch: 0042 loss_train: 0.4954 acc_train: 0.7993\n",
            "Epoch: 0043 loss_train: 0.4953 acc_train: 0.7994\n",
            "Epoch: 0044 loss_train: 0.4954 acc_train: 0.7993\n",
            "Epoch: 0045 loss_train: 0.4953 acc_train: 0.7993\n",
            "Epoch: 0046 loss_train: 0.4952 acc_train: 0.7994\n",
            "Epoch: 0047 loss_train: 0.4952 acc_train: 0.7994\n",
            "Epoch: 0048 loss_train: 0.4951 acc_train: 0.7993\n",
            "Epoch: 0049 loss_train: 0.4952 acc_train: 0.7994\n",
            "Epoch: 0050 loss_train: 0.4952 acc_train: 0.7993\n",
            "Epoch: 0051 loss_train: 0.4951 acc_train: 0.7994\n",
            "Epoch: 0052 loss_train: 0.4951 acc_train: 0.7993\n",
            "Epoch: 0053 loss_train: 0.4951 acc_train: 0.7994\n",
            "Epoch: 0054 loss_train: 0.4950 acc_train: 0.7993\n",
            "Epoch: 0055 loss_train: 0.4950 acc_train: 0.7993\n",
            "Epoch: 0056 loss_train: 0.4950 acc_train: 0.7994\n",
            "Epoch: 0057 loss_train: 0.4949 acc_train: 0.7993\n",
            "Epoch: 0058 loss_train: 0.4950 acc_train: 0.7993\n",
            "Epoch: 0059 loss_train: 0.4949 acc_train: 0.7993\n",
            "Epoch: 0060 loss_train: 0.4949 acc_train: 0.7993\n",
            "Epoch: 0061 loss_train: 0.4948 acc_train: 0.7992\n",
            "Epoch: 0062 loss_train: 0.4948 acc_train: 0.7993\n",
            "Epoch: 0063 loss_train: 0.4948 acc_train: 0.7994\n",
            "Epoch: 0064 loss_train: 0.4948 acc_train: 0.7993\n",
            "Epoch: 0065 loss_train: 0.4948 acc_train: 0.7994\n",
            "Epoch: 0066 loss_train: 0.4947 acc_train: 0.7992\n",
            "Epoch: 0067 loss_train: 0.4947 acc_train: 0.7992\n",
            "Epoch: 0068 loss_train: 0.4947 acc_train: 0.7992\n",
            "Epoch: 0069 loss_train: 0.4947 acc_train: 0.7993\n",
            "Epoch: 0070 loss_train: 0.4947 acc_train: 0.7993\n",
            "Epoch: 0071 loss_train: 0.4946 acc_train: 0.7993\n",
            "Epoch: 0072 loss_train: 0.4947 acc_train: 0.7993\n",
            "Epoch: 0073 loss_train: 0.4946 acc_train: 0.7992\n",
            "Epoch: 0074 loss_train: 0.4946 acc_train: 0.7992\n",
            "Epoch: 0075 loss_train: 0.4946 acc_train: 0.7993\n",
            "Epoch: 0076 loss_train: 0.4945 acc_train: 0.7993\n",
            "Epoch: 0077 loss_train: 0.4946 acc_train: 0.7992\n",
            "Epoch: 0078 loss_train: 0.4947 acc_train: 0.7993\n",
            "Epoch: 0079 loss_train: 0.4945 acc_train: 0.7992\n",
            "Epoch: 0080 loss_train: 0.4945 acc_train: 0.7993\n",
            "Epoch: 0081 loss_train: 0.4945 acc_train: 0.7993\n",
            "Epoch: 0082 loss_train: 0.4946 acc_train: 0.7992\n",
            "Epoch: 0083 loss_train: 0.4945 acc_train: 0.7992\n",
            "Epoch: 0084 loss_train: 0.4945 acc_train: 0.7993\n",
            "Epoch: 0085 loss_train: 0.4945 acc_train: 0.7992\n",
            "Epoch: 0086 loss_train: 0.4944 acc_train: 0.7993\n",
            "Epoch: 0087 loss_train: 0.4944 acc_train: 0.7993\n",
            "Epoch: 0088 loss_train: 0.4944 acc_train: 0.7994\n",
            "Epoch: 0089 loss_train: 0.4944 acc_train: 0.7992\n",
            "Epoch: 0090 loss_train: 0.4946 acc_train: 0.7992\n",
            "Epoch: 0091 loss_train: 0.4943 acc_train: 0.7993\n",
            "Epoch: 0092 loss_train: 0.4944 acc_train: 0.7993\n",
            "Epoch: 0093 loss_train: 0.4943 acc_train: 0.7993\n",
            "Epoch: 0094 loss_train: 0.4943 acc_train: 0.7993\n",
            "Epoch: 0095 loss_train: 0.4943 acc_train: 0.7994\n",
            "Epoch: 0096 loss_train: 0.4944 acc_train: 0.7993\n",
            "Epoch: 0097 loss_train: 0.4943 acc_train: 0.7992\n",
            "Epoch: 0098 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0099 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0100 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0101 loss_train: 0.4943 acc_train: 0.7993\n",
            "Epoch: 0102 loss_train: 0.4943 acc_train: 0.7993\n",
            "Epoch: 0103 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0104 loss_train: 0.4943 acc_train: 0.7992\n",
            "Epoch: 0105 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0106 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0107 loss_train: 0.4943 acc_train: 0.7993\n",
            "Epoch: 0108 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0109 loss_train: 0.4942 acc_train: 0.7992\n",
            "Epoch: 0110 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0111 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0112 loss_train: 0.4942 acc_train: 0.7993\n",
            "Epoch: 0113 loss_train: 0.4942 acc_train: 0.7992\n",
            "Epoch: 0114 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0115 loss_train: 0.4942 acc_train: 0.7991\n",
            "Epoch: 0116 loss_train: 0.4942 acc_train: 0.7992\n",
            "Epoch: 0117 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0118 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0119 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0120 loss_train: 0.4942 acc_train: 0.7992\n",
            "Epoch: 0121 loss_train: 0.4941 acc_train: 0.7994\n",
            "Epoch: 0122 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0123 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0124 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0125 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0126 loss_train: 0.4942 acc_train: 0.7991\n",
            "Epoch: 0127 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0128 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0129 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0130 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0131 loss_train: 0.4941 acc_train: 0.7991\n",
            "Epoch: 0132 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0133 loss_train: 0.4941 acc_train: 0.7993\n",
            "Epoch: 0134 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0135 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0136 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0137 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0138 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0139 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0140 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0141 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0142 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0143 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0144 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0145 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0146 loss_train: 0.4941 acc_train: 0.7992\n",
            "Epoch: 0147 loss_train: 0.4942 acc_train: 0.7991\n",
            "Epoch: 0148 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0149 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0150 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0151 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0152 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0153 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0154 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0155 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0156 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0157 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0158 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0159 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0160 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0161 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0162 loss_train: 0.4939 acc_train: 0.7993\n",
            "Epoch: 0163 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0164 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0165 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0166 loss_train: 0.4940 acc_train: 0.7991\n",
            "Epoch: 0167 loss_train: 0.4939 acc_train: 0.7993\n",
            "Epoch: 0168 loss_train: 0.4939 acc_train: 0.7993\n",
            "Epoch: 0169 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0170 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0171 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0172 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0173 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0174 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0175 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0176 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0177 loss_train: 0.4939 acc_train: 0.7993\n",
            "Epoch: 0178 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0179 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0180 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0181 loss_train: 0.4940 acc_train: 0.7992\n",
            "Epoch: 0182 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0183 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0184 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0185 loss_train: 0.4940 acc_train: 0.7991\n",
            "Epoch: 0186 loss_train: 0.4939 acc_train: 0.7993\n",
            "Epoch: 0187 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0188 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0189 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0190 loss_train: 0.4938 acc_train: 0.7993\n",
            "Epoch: 0191 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0192 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0193 loss_train: 0.4939 acc_train: 0.7991\n",
            "Epoch: 0194 loss_train: 0.4939 acc_train: 0.7993\n",
            "Epoch: 0195 loss_train: 0.4940 acc_train: 0.7993\n",
            "Epoch: 0196 loss_train: 0.4940 acc_train: 0.7991\n",
            "Epoch: 0197 loss_train: 0.4939 acc_train: 0.7992\n",
            "Epoch: 0198 loss_train: 0.4938 acc_train: 0.7993\n",
            "Epoch: 0199 loss_train: 0.4938 acc_train: 0.7993\n",
            "Epoch: 0200 loss_train: 0.4940 acc_train: 0.7991\n",
            "17414/17414 [==============================] - 0s 20us/step\n",
            "Test set results: loss= 0.4896 acc_train: 0.8029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxOuiXtkCtO",
        "outputId": "cbc4e05b-0525-4daf-a973-ab9d356d4d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "# 计算AUC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import time\n",
        "\n",
        "y_score1 = model.predict([np.array(drug_latent_vector[num_scale:]), np.array(disease_latent_vector[num_scale:])])\n",
        "\n",
        "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
        "#Y_train0为真实标签，Y_pred_0为预测标签，注意，这里roc_curve为一维的输入，Y_train0是一维的\n",
        "fpr, tpr, thresholds_keras = roc_curve(labels[num_scale:], y_score1)   \n",
        "auc = auc(fpr, tpr)\n",
        "print(\"AUC : \", auc)\n",
        "plt.figure()\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "#plt.savefig('/content/drive/My Drive/Colab Notebooks/GCN_Prediction link/image/'+ now + 'ROC.jpg')\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC :  0.5910786627831873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uxl8iFFwVDM"
      },
      "source": [
        "# 归一化数据\n",
        "from sklearn import preprocessing  \n",
        "min_max_scaler = preprocessing.MinMaxScaler()  #标准化训练集数据 \n",
        "data_train_nomal = min_max_scaler.fit_transform(y_score1)  #对测试集数据进行相同的归一化处理\n",
        "data_train_nomal[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz-JEXmi8tE1"
      },
      "source": [
        "print('max:', data_train_nomal.max())\n",
        "print('min:', data_train_nomal.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXfQJR1oM6HI"
      },
      "source": [
        "def auc_calculate(labels,preds,n_bins=17420):\n",
        "    postive_len = sum(labels)   #正样本数量（因为正样本都是1）\n",
        "    negative_len = len(labels) - postive_len #负样本数量\n",
        "    print('负样本数量:', negative_len)\n",
        "    total_case = postive_len * negative_len #正负样本对\n",
        "    pos_histogram = [0 for _ in range(n_bins)] \n",
        "    neg_histogram = [0 for _ in range(n_bins)]\n",
        "    bin_width = 1.0 / n_bins\n",
        "    for i in range(len(labels)):\n",
        "        nth_bin = int(preds[i]/bin_width)\n",
        "        if labels[i]==1:\n",
        "            pos_histogram[nth_bin] += 1\n",
        "        else:\n",
        "            neg_histogram[nth_bin] += 1\n",
        "    accumulated_neg = 0\n",
        "    satisfied_pair = 0\n",
        "    for i in range(n_bins):\n",
        "        satisfied_pair += (pos_histogram[i]*accumulated_neg + pos_histogram[i]*neg_histogram[i]*0.5)\n",
        "        accumulated_neg += neg_histogram[i]\n",
        "    return satisfied_pair / float(total_case)\n",
        "print(\"验证:\",auc_calculate(labels[27862:], y_score1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R95RmynP0LsH"
      },
      "source": [
        "from sklearn import metrics\n",
        "'''使用real.csv和result.csv列数据，计算PR曲线的AUC值'''\n",
        "precision, recall, _thresholds = metrics.precision_recall_curve(labels[69650:], data_train_nomal)\n",
        "area = metrics.auc(recall, precision)\n",
        "print(area) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}